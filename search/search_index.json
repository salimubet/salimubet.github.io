{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"1statistik/","text":"Pengertian Statistik Deskriptif \u00b6 adalah metode yang kaitannya dengan mengumpulkan dan menyajikan data yang tujuannya adalah menyajikan informasi yang berguna. Macam Statistik Deskriptif \u00b6 Quartile \u00b6 Suatu irisan nilai hasil pembagian data jadi 4 bagian yang sama besar itulah yg disebut Quartile. Nilai-nilai sebuah quartil biasanya disimbolkan dg Q1 (quartile bawah), Q2 (quartile tengah), dan Q3 (quartile atas). Q1 ini memiliki nilai 25% dari data. Sedangkan Q2 sama dengan median yaitu 50% dari data. dan Q3 atau quartile atas mempunyai nilai 75% x data. Rumus dibawah ini bisa kita gunakan ketika mencari quartile; \\begin{array}{l} {Q_{1}=(n+1) \\frac{1}{4}} \\\\ {Q_{2}=(n+1) \\frac{1}{2}} \\\\ {Q_{3}=(n+1) \\frac{3}{4}}\\end{array} \\begin{array}{l} {Q_{1}=(n+1) \\frac{1}{4}} \\\\ {Q_{2}=(n+1) \\frac{1}{2}} \\\\ {Q_{3}=(n+1) \\frac{3}{4}}\\end{array} ket= Q:quartile, n:banyaknya data Mean \u00b6 Mean atau bisa disebut rata-rata yaitu sebuah ukuran pemusatan data. Rata-rata sebuah data biasanya merupakan statistik karena bisa menggambarkan data tersebut berada pada kisaran rata data tersebut. Mean tidak bisa digunakan sebagai ukuran pemusatan bagi jenis data yang nominal dan ordinal. Sehingga rumus mean bisa dituliskan sebagai berikut \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i}}{N}=\\frac{x_{1}+x_{2}+x_{3}+\\ldots+x_{n}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i}}{N}=\\frac{x_{1}+x_{2}+x_{3}+\\ldots+x_{n}}{N} keterangan: x_rata-rata = x bar = nilai rata-rata sampel x = data ke-n N = banyaknya data Median \u00b6 Median yaitu suatu data atau lebih sering dikatakan nilai central/tengah sebuah data yg terurut. biasanya median disimbolkan Me . Nilai suatu median sama dengan nilai Q2 ( Quartile 2 ). Ketika mencari sebuah median, banyak data ( n ) data ganjil dan data genap mempunyai cara menghitung yang beda. Pada median digunakanlah rumus dibawah ini: {M e=Q_{2}=\\left(\\frac{\\frac{\\pi n \\pi+1}{2}}{2}\\right), \\text {jika } \\quad n \\quad \\text { genap }}\\\\ \\begin{array}{c}{M e=Q_{2}=\\left(\\frac{n+1}{2}\\right), j i k a \\quad n \\quad \\text { ganjil }}\\end{array} {M e=Q_{2}=\\left(\\frac{\\frac{\\pi n \\pi+1}{2}}{2}\\right), \\text {jika } \\quad n \\quad \\text { genap }}\\\\ \\begin{array}{c}{M e=Q_{2}=\\left(\\frac{n+1}{2}\\right), j i k a \\quad n \\quad \\text { ganjil }}\\end{array} Dengan keterangan bahwa Me yaitu Median sedangkan n adalah banyaknya data. Modus \u00b6 Modus bisa disebut sebuah angka yang paling banyak dijumpai dalam sekumpulan angka. cara untuk mendapatkan sebuah modus yaitu dengan mengumpulkan dan mengatur data kemudian dihitung tiap frekuensi dari hasil tersebut. Dan hasil penjumlahan paling tinggi itulah modus dari himpunan suatu angka. Rumus dibawah ini bisa digunakan untuk mencari suatu modus: $$ M_{o}=T b+p \\frac{b_{1}}{b_{1}+b_{2}} $$ Ket: Mo = modus kumpulan data Tb = tepi bawah (elemen modus) b1 = selisih frekuensi (elemen modus dan elemen sebelumnya) b2 = selisih frekuensi (elemen modus dengan elemen sesudahnya) p = panjang interval value b1 dan b2 \u2013> adalah mutlak (selalu positif) Varians \u00b6 Ukuran penyebaran tiap nilai suatu kelompok data dari rata-rata itulah yang disebut Varians. didlam proses untuk dapatkan varians ada beberapa langkah yang harus dikerjakan; 1 - ambil ukuran jarak dari tiap nilai dan kurangi rata-rata dari tiap nilai yang ada dalam data 2 - hasil dari ukuran jarak tadi dikuadratkan dan dibagi dg jumlah kuadrat dan jumlah nilai kelompok data. Formula dibawah dapat digunakan untuk menghitung Varians: \\sigma^{2}=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n} \\sigma^{2}=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n} Ket; xi = titik data x bar = rata-rata dari seluruh titik data n = banyaknya anggota data Standart Deviasi \u00b6 Standar deviasi yaitu ukuran dispersi himpunan data relatif pada rata-rata atau lebih. Cara menghitungnya dengan cara mengakar kuadratkan nilai dari varians. Ketika titik data > rata-rata dalam kumpulan data maka semakin tinggi standar deviasinya. Rumus dibawah ini bisa kita gunakan untuk mencari standart deviasi: \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n}} \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n}} Ket; xi = titik data x bar = rata-rata dari seluruh titik data n = banyaknya anggota data Skewness \u00b6 Skewness atau biasa disebut kemiringan yaitu ketidaksimetrisan suatu distribusi statistik dimana kurva tampak miring ke kiri atau ke kanan. Skewness diperlukan untuk menentukan perbedaan suatu distribusi dengan distribusi normal. Pada distribusi normal grafik muncul seperti kurva berbentuk lonceng. Sedangkan ketika distribusi tersebut mengalami kemiringan ke arah kanan dan ekor disisi kanan kurva lebih panjang dari ekorsisi kiri kurva maka keadaan ini disebut dengan kemiringan positif (+) dan sebaliknya dikatakan kemiringan negative (-). Rumus dibawah ini bisa digunakan untuk menghitung skewness: \\text {Skewness}=\\frac{\\sum i=1^{n}\\left(x_{i}-\\overline{x}\\right)^{i}}{(n-1) \\sigma^{3}} \\text {Skewness}=\\frac{\\sum i=1^{n}\\left(x_{i}-\\overline{x}\\right)^{i}}{(n-1) \\sigma^{3}} Ket. xi = titik data x-bar = rata-rata dari distribusi n = jumlah titik pada distribusi o = standar deviasi Penerapan Statistik Deskriptif pada Python \u00b6 Persiapan \u00b6 Pada penerapa kali inisaya ambil data random sebanyak 500 data pada excel selain itu yang diperlukan adalah library python yg dapat di unduh secara gratis. library yg diperlukan adalah; pandas, untuk manajemen dan analysis sebuah data. scipy, library fungsi matematika. Pertama \u00b6 install terlebih dahulu librarynya dengan cara ketikan import pandas as pd from scipy import stats import statistics Kedua \u00b6 selanjutnya memuat data, pada kasus ini saya menggunakan file excel df = pd . read_excel ( \"Book1.xlsx\" , usecols = [ 1 , 2 , 3 , 4 , 5 ]) Ketiga \u00b6 setelah itu ketikan rumusnya dibawah ini for i in df : print ( '--- %s ---' % i ) Q1 = df [ i ] . quantile ( 0.25 ) print ( 'Q1 =' , Q1 ) Q2 = df [ i ] . quantile ( 0.5 ) print ( 'Q2 =' , Q2 ) Q3 = df [ i ] . quantile ( 0.75 ) print ( 'Q3 =' , Q3 ) Mean = df [ i ] . mean () print ( 'Mean =' , Mean ) Stdev = statistics . stdev ( df [ i ]) print ( 'Std Dev =' , Stdev ) Skew = df [ i ] . skew () print ( 'Skewness =' , Skew , ' \\n ' ) inilah hasil runing-nya Statis X1 X2 X3 X4 X5 Q1 19 138 201,75 7 33 Q2 28,5 177,5 302 16 45 Q3 38 212 405,75 23 58 Mean 28,926 175,966 303,836 15,34 45,276 Std Dev 11,65213 42,93348 115,5791 9,014151 14,55457 Skewness 0,126823 -0,01311 -0,01151 -0,01547 -0,03891 Source \u00b6 file saya dapat diunduh disini Sumber Statistik Deskriptif \u00b6 https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"1- Statistik Deskriptif"},{"location":"1statistik/#pengertian-statistik-deskriptif","text":"adalah metode yang kaitannya dengan mengumpulkan dan menyajikan data yang tujuannya adalah menyajikan informasi yang berguna.","title":"Pengertian Statistik Deskriptif"},{"location":"1statistik/#macam-statistik-deskriptif","text":"","title":"Macam Statistik Deskriptif"},{"location":"1statistik/#quartile","text":"Suatu irisan nilai hasil pembagian data jadi 4 bagian yang sama besar itulah yg disebut Quartile. Nilai-nilai sebuah quartil biasanya disimbolkan dg Q1 (quartile bawah), Q2 (quartile tengah), dan Q3 (quartile atas). Q1 ini memiliki nilai 25% dari data. Sedangkan Q2 sama dengan median yaitu 50% dari data. dan Q3 atau quartile atas mempunyai nilai 75% x data. Rumus dibawah ini bisa kita gunakan ketika mencari quartile; \\begin{array}{l} {Q_{1}=(n+1) \\frac{1}{4}} \\\\ {Q_{2}=(n+1) \\frac{1}{2}} \\\\ {Q_{3}=(n+1) \\frac{3}{4}}\\end{array} \\begin{array}{l} {Q_{1}=(n+1) \\frac{1}{4}} \\\\ {Q_{2}=(n+1) \\frac{1}{2}} \\\\ {Q_{3}=(n+1) \\frac{3}{4}}\\end{array} ket= Q:quartile, n:banyaknya data","title":"Quartile"},{"location":"1statistik/#mean","text":"Mean atau bisa disebut rata-rata yaitu sebuah ukuran pemusatan data. Rata-rata sebuah data biasanya merupakan statistik karena bisa menggambarkan data tersebut berada pada kisaran rata data tersebut. Mean tidak bisa digunakan sebagai ukuran pemusatan bagi jenis data yang nominal dan ordinal. Sehingga rumus mean bisa dituliskan sebagai berikut \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i}}{N}=\\frac{x_{1}+x_{2}+x_{3}+\\ldots+x_{n}}{N} \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i}}{N}=\\frac{x_{1}+x_{2}+x_{3}+\\ldots+x_{n}}{N} keterangan: x_rata-rata = x bar = nilai rata-rata sampel x = data ke-n N = banyaknya data","title":"Mean"},{"location":"1statistik/#median","text":"Median yaitu suatu data atau lebih sering dikatakan nilai central/tengah sebuah data yg terurut. biasanya median disimbolkan Me . Nilai suatu median sama dengan nilai Q2 ( Quartile 2 ). Ketika mencari sebuah median, banyak data ( n ) data ganjil dan data genap mempunyai cara menghitung yang beda. Pada median digunakanlah rumus dibawah ini: {M e=Q_{2}=\\left(\\frac{\\frac{\\pi n \\pi+1}{2}}{2}\\right), \\text {jika } \\quad n \\quad \\text { genap }}\\\\ \\begin{array}{c}{M e=Q_{2}=\\left(\\frac{n+1}{2}\\right), j i k a \\quad n \\quad \\text { ganjil }}\\end{array} {M e=Q_{2}=\\left(\\frac{\\frac{\\pi n \\pi+1}{2}}{2}\\right), \\text {jika } \\quad n \\quad \\text { genap }}\\\\ \\begin{array}{c}{M e=Q_{2}=\\left(\\frac{n+1}{2}\\right), j i k a \\quad n \\quad \\text { ganjil }}\\end{array} Dengan keterangan bahwa Me yaitu Median sedangkan n adalah banyaknya data.","title":"Median"},{"location":"1statistik/#modus","text":"Modus bisa disebut sebuah angka yang paling banyak dijumpai dalam sekumpulan angka. cara untuk mendapatkan sebuah modus yaitu dengan mengumpulkan dan mengatur data kemudian dihitung tiap frekuensi dari hasil tersebut. Dan hasil penjumlahan paling tinggi itulah modus dari himpunan suatu angka. Rumus dibawah ini bisa digunakan untuk mencari suatu modus: $$ M_{o}=T b+p \\frac{b_{1}}{b_{1}+b_{2}} $$ Ket: Mo = modus kumpulan data Tb = tepi bawah (elemen modus) b1 = selisih frekuensi (elemen modus dan elemen sebelumnya) b2 = selisih frekuensi (elemen modus dengan elemen sesudahnya) p = panjang interval value b1 dan b2 \u2013> adalah mutlak (selalu positif)","title":"Modus"},{"location":"1statistik/#varians","text":"Ukuran penyebaran tiap nilai suatu kelompok data dari rata-rata itulah yang disebut Varians. didlam proses untuk dapatkan varians ada beberapa langkah yang harus dikerjakan; 1 - ambil ukuran jarak dari tiap nilai dan kurangi rata-rata dari tiap nilai yang ada dalam data 2 - hasil dari ukuran jarak tadi dikuadratkan dan dibagi dg jumlah kuadrat dan jumlah nilai kelompok data. Formula dibawah dapat digunakan untuk menghitung Varians: \\sigma^{2}=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n} \\sigma^{2}=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n} Ket; xi = titik data x bar = rata-rata dari seluruh titik data n = banyaknya anggota data","title":"Varians"},{"location":"1statistik/#standart-deviasi","text":"Standar deviasi yaitu ukuran dispersi himpunan data relatif pada rata-rata atau lebih. Cara menghitungnya dengan cara mengakar kuadratkan nilai dari varians. Ketika titik data > rata-rata dalam kumpulan data maka semakin tinggi standar deviasinya. Rumus dibawah ini bisa kita gunakan untuk mencari standart deviasi: \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n}} \\sigma=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}}{n}} Ket; xi = titik data x bar = rata-rata dari seluruh titik data n = banyaknya anggota data","title":"Standart Deviasi"},{"location":"1statistik/#skewness","text":"Skewness atau biasa disebut kemiringan yaitu ketidaksimetrisan suatu distribusi statistik dimana kurva tampak miring ke kiri atau ke kanan. Skewness diperlukan untuk menentukan perbedaan suatu distribusi dengan distribusi normal. Pada distribusi normal grafik muncul seperti kurva berbentuk lonceng. Sedangkan ketika distribusi tersebut mengalami kemiringan ke arah kanan dan ekor disisi kanan kurva lebih panjang dari ekorsisi kiri kurva maka keadaan ini disebut dengan kemiringan positif (+) dan sebaliknya dikatakan kemiringan negative (-). Rumus dibawah ini bisa digunakan untuk menghitung skewness: \\text {Skewness}=\\frac{\\sum i=1^{n}\\left(x_{i}-\\overline{x}\\right)^{i}}{(n-1) \\sigma^{3}} \\text {Skewness}=\\frac{\\sum i=1^{n}\\left(x_{i}-\\overline{x}\\right)^{i}}{(n-1) \\sigma^{3}} Ket. xi = titik data x-bar = rata-rata dari distribusi n = jumlah titik pada distribusi o = standar deviasi","title":"Skewness"},{"location":"1statistik/#penerapan-statistik-deskriptif-pada-python","text":"","title":"Penerapan Statistik Deskriptif pada Python"},{"location":"1statistik/#persiapan","text":"Pada penerapa kali inisaya ambil data random sebanyak 500 data pada excel selain itu yang diperlukan adalah library python yg dapat di unduh secara gratis. library yg diperlukan adalah; pandas, untuk manajemen dan analysis sebuah data. scipy, library fungsi matematika.","title":"Persiapan"},{"location":"1statistik/#pertama","text":"install terlebih dahulu librarynya dengan cara ketikan import pandas as pd from scipy import stats import statistics","title":"Pertama"},{"location":"1statistik/#kedua","text":"selanjutnya memuat data, pada kasus ini saya menggunakan file excel df = pd . read_excel ( \"Book1.xlsx\" , usecols = [ 1 , 2 , 3 , 4 , 5 ])","title":"Kedua"},{"location":"1statistik/#ketiga","text":"setelah itu ketikan rumusnya dibawah ini for i in df : print ( '--- %s ---' % i ) Q1 = df [ i ] . quantile ( 0.25 ) print ( 'Q1 =' , Q1 ) Q2 = df [ i ] . quantile ( 0.5 ) print ( 'Q2 =' , Q2 ) Q3 = df [ i ] . quantile ( 0.75 ) print ( 'Q3 =' , Q3 ) Mean = df [ i ] . mean () print ( 'Mean =' , Mean ) Stdev = statistics . stdev ( df [ i ]) print ( 'Std Dev =' , Stdev ) Skew = df [ i ] . skew () print ( 'Skewness =' , Skew , ' \\n ' ) inilah hasil runing-nya Statis X1 X2 X3 X4 X5 Q1 19 138 201,75 7 33 Q2 28,5 177,5 302 16 45 Q3 38 212 405,75 23 58 Mean 28,926 175,966 303,836 15,34 45,276 Std Dev 11,65213 42,93348 115,5791 9,014151 14,55457 Skewness 0,126823 -0,01311 -0,01151 -0,01547 -0,03891","title":"Ketiga"},{"location":"1statistik/#source","text":"file saya dapat diunduh disini","title":"Source"},{"location":"1statistik/#sumber-statistik-deskriptif","text":"https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"Sumber Statistik Deskriptif"},{"location":"2jarak/","text":"Mengukur Jarak Tipe Numerik \u00b6 Mengukur jarak dapat diartikan sebuah komponen utama dalam algoritma clustering yang basisnya jarak. Misal seperti K-Mean, K-medoidm dan fuzzy c-mean serta rough clustering untuk melakukan pengelompokkan pada jarak. Cara yang bisa digunakan untuk menghitung similaritas (jarak) dari sebuah data yang tipenya numerik adalah sebagai berikut: Minkowski Distance \u00b6 Euclidean distance dan Manhattan distance adalah kelompok dari Minkowski yang menjadi kasus khusus dari Minkowski distance. Minkowski distance sendiri dapat dinyatakan dengan: d_{\\min }=\\left(\\operatorname{sum}_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{m}\\right)^{\\frac{1}{m}}, m \\geq 1 d_{\\min }=\\left(\\operatorname{sum}_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{m}\\right)^{\\frac{1}{m}}, m \\geq 1 keterangan: m: bil. Real positif | xi dan yi: dua vektor dalam runang dimensi | n: implementasi ukuran jarak Manhattan Distance \u00b6 Manhattan distance yaitu sebuah kasus khusus yang jarak Minkowski distance nya pada m=1. Seperti sebelumnya, Manhattan distance sensitif pada outlier. Bila ukurannya dipakai pada algoritma clustering, maka bentuk clusternya adalah hyper-rectangular. Ukurannya ini dapat diartikan dengan $$ d_{\\operatorname{man}}=\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right| $$ Euclidian Distance \u00b6 Jarak Euiclidian ini adalah jarak yang paling terkenal dipakai pada data numerik. Pada jarak Minkowski ini adalah kasus khusus ketika m=2. Saat digunakan pada himpunan data cluster kompak atau terisolasi, jarak Euclidean ini memiliki kinerja yang baik. Namun disisi lain, ia juga memiliki kelemahan seperti: pada dua vektor data jika memiliki nilai atribut yang sama, dan pada jarak yang lebih kecil dibanding pasangan vektor data lainnya yang memiliki nilai atribut yang sama. Kelemahan yang lain terdapat pada masalah dengan jarak Euclidean untuk fitur skala terbesar yang mendominasi. Adapun cara yang bisa digunakan untuk mengatasi kelemahan ini dapat menggunakan normalisasi fitur kontinu. Berikut adalah pernyataan dari Euclidian Distance: \\left.d_{( } x, y\\right)=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}} \\left.d_{( } x, y\\right)=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}} Average Distance \u00b6 Terjadinya kelemahan pada Jarak Euclidean di atas, maka untuk memperbaiki hasil dari versi modifikasi jarak euclidean dilakukan rata-rata pada jarak. Dua titik x,y pada ruang dimensi n, dan rata-rata pada jarak dapat didefinisi kan sebagai berikut: d_{a v e}=\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}} d_{a v e}=\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}} Weighted Euclidean Distance \u00b6 Weighted Euclidean Distance bisa diartikan sebagai modifikasi lain dari jarak euclidean yang dapat digunakan dan berdasarkan pada tingkatan yang penting dari masing-masing atribut yang ditentukan. Rumus dari ukuran ini dapat dilihat di bawah ini dengan ketentuan wi adalah bobot yang diberikan pada atribut ke i. d_{w e}=\\left(\\sum_{i=1}^{n} w_{i}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}} d_{w e}=\\left(\\sum_{i=1}^{n} w_{i}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}} Chord Distance \u00b6 Salah satu ukuran jarak yang memodifikasi Euclidean Distance untuk memecahkan kekurangan darinya bisa disebut sebagai arti dari Chord Distance. Skala pengukuran yang baik juga dapat memecahkan ini. Ketika terdapat data yang jaraknya dalam keadaan tidak dinormalisasi maka dapat menggunakan perhitungan jarak ini. Chord distance bisa ditampilkan seperti di bawah ini: \\begin{aligned} d_{\\text {chord }} &=\\left(2-2 \\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}}\\right)^{\\frac{1}{2}} \\end{aligned} \\begin{aligned} d_{\\text {chord }} &=\\left(2-2 \\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}}\\right)^{\\frac{1}{2}} \\end{aligned} \\begin{aligned}\\text { dimana }|x|_{2} \\text { adalah } L^{2}-\\text { norm }|x|_{2} &=\\sqrt{\\sum_{i=1}^{n} x_{i}^{2}} \\end{aligned} \\begin{aligned}\\text { dimana }|x|_{2} \\text { adalah } L^{2}-\\text { norm }|x|_{2} &=\\sqrt{\\sum_{i=1}^{n} x_{i}^{2}} \\end{aligned} Mahalanobis Distance \u00b6 Berdasarkan pada data yg beda dengan Euclidean dan Manhattan Distance yg bebas antara data satu dan data yang lain maka itu disebut Mahalanobis Distance. Jarak Mahalanobis ini yang beraturan bisa dipakai untuk meng ektsrasi hyperellipsoidal clusters. Jarak ini juga dapat mengurangi distorsi yang disebabkan oleh korelasi linear antar fitur dg diterapkannya transformasi pemutihan pada data atau dg memakai kuadrat jarak mahalanobis. Mahalanobis ini dapat dinyatakan sebagai berikut: d_{m a h}=\\sqrt{(x-y) S^{-1}(x-y)^{T}} d_{m a h}=\\sqrt{(x-y) S^{-1}(x-y)^{T}} dengan S sbg data matrik convariance Cosine Measure \u00b6 Ukuran Cosien similarity ini lebih banyak dipakai untuk similaritas dokumen dan bisa dinyatakan: \\operatorname{cosine}(x, y)=\\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}} \\operatorname{cosine}(x, y)=\\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}} dengan ketentuan |y|2 adalah Euclidean norm dari vektor y=(y1,y2,...yn) dan di definisikan dengan \\begin{equation} |y|_{2}=\\sqrt{y_{1}^{2}+y_{2}^{2}+\\ldots+y_{n}^{2}} \\end{equation} \\begin{equation} |y|_{2}=\\sqrt{y_{1}^{2}+y_{2}^{2}+\\ldots+y_{n}^{2}} \\end{equation} Pearson Correlation \u00b6 Pearson correlation banyak dipakai pada data expresi gen. Pearson Correlation ini memiliki kelemahan sensitif terhadap outlier. Ukurannya similaritas ini dihitung dengan cara similaritas antar dua bentuk pola expresi gen. Bisa juga didefinisikan sebagai berikut: \\begin{equation} \\operatorname{Pearson}(x, y)=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{x}\\right)\\left(y_{i}-\\mu_{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}} \\end{equation} \\begin{equation} \\operatorname{Pearson}(x, y)=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{x}\\right)\\left(y_{i}-\\mu_{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}} \\end{equation} Mengukur Jarak Tipe Binary \u00b6 Atribut pada biner hanya memiliki dua status, yang pertama 0 dan kedua 1. Contoh dari atribut biner yaitu hasil tes pengguna narkoba, dimana tes urine akan mendapatkan hasil positif dipresentasikan menjadi 1 dan negatif atau 0. Dalam perhitungan pada jarak tipe biner ada cara khusus tidak boleh menyamakan dg cara perhitungan jarak tipe numerik . Lalu, bagaimana cara kita menghitung ketidaksamaan antara dua atribut biner tersebut? Yaitu dengan sebuah pendekatan yg melibatkan perhitungan matriks ketidaksamaan dari data biner yg diberikan. Jika semua atribut pada biner dianggap memiliki bobot yg sama, kita memiliki tabel kontingensi 2x2. Dimana q adalah jumlah daripada atribut yg sama dengan 1 untuk kedua objek i dan j. kemudian r adalah jumlah atribut yg sama dengan 1 untuk objek i tetapi 0 untuk j. Sedangkan s adalah jumlah atribut yang sama dengan 0 untuk objek i namun bernilai 1 untuk objek j. Dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek yaitu i dan j. Jumlah total atribut dimisalkan dengan simbol p, dimana p=q+r+s+t. Maka dapat dinyatakan sebagai atribut biner simetris seperti dibawah ini, dengan catatan jika objek i dan j dinyatakan sebagai atribut biner simetris. \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s+t} \\end{equation} \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s+t} \\end{equation} Sedangkan untuk atribut a-simetris, pada kedua kondisi dinyatakan tidak sama pentingnya, seperti hasil positif atau 1 dan negatif atau 0 seperti dari tes pengguna narkoba diatas. Ketidaksamaan berdasarkan atribut-atribut ini disebut juga sebagai asimetris biner dissimilarity, yaitu jumlah kecocokan negatif, t, dinyatakan tidak penting, dan diabaikan. Sehingga dapat diperhitungkan sebagai berikut: \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s} \\end{equation} \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s} \\end{equation} Lalu kita dapat mengukur perbedaan antara dua atribut biner berdasarkan disimilarity nya. Misal biner asimetris kesamaan antara objek i dan j kita dapat menghitungnya dengan cara: \\begin{equation} \\operatorname{sim}(i, j)=\\frac{q}{q+r+s}=1-d(i, j) \\end{equation} \\begin{equation} \\operatorname{sim}(i, j)=\\frac{q}{q+r+s}=1-d(i, j) \\end{equation} Atau bisa disebut dengan Jaccard Coefficient untuk persamaan similarity ini. Mengukur Jarak Tipe Kategorikal \u00b6 Pada sebuah data bertipe kategorikal dapat membawa dua atau lebih pernyataan. Contoh, terdapat map_color pada sebuah atribut normal yang mempunyai 5 pernyataan sebagai berikut: merah, kuning, biru, hijau, dan ungu. Status bisa disimbolkan dengan byletters, symbol, atau satu set bilangan bulat, misal 1,2,3...,m. Perhatikan bahwa pada bilangan bulat tersebut digunakan hanya untuk menangani data dan bukan mewakili pemesanan khusus apapun. Sedangkan bedanya dua objek i dan objek j dapat kita hitung dengan rasio ketidakcocokan, sebagai berikut: \\begin{equation} d_{(i, j)}=\\frac{p-m}{p} \\end{equation} \\begin{equation} d_{(i, j)}=\\frac{p-m}{p} \\end{equation} dengan catatan, m merupakan angka yg cocok atau nomor cocok untuk i dan j. Dan p adalah banyaknya fitur yg dihitung sebagai tipe nominal. Mengukur Jarak Tipe Ordinal \u00b6 Para nilai pada atribut ordinal punya urutan atau rangking, tetapi besarnya antara para nilai tidak diketahui. Misalnya tingkat terkecil, sedang, dan terbesar untuk atribut ukuran. Atribut ordinal bisa juga didapat dari diskritisasi atribut numerik dengan membuat rentang nilai pada sejumlah kategori tertentu. Kategori ini juga disusun pada rangking. Rentang pada atribut numerik bisa dipetakan ke atribut ordinal f yg memiliki Mf state. Contohnya, kisaran suhu atribut skala atau Celcius bisa diatur dalam skala: -30 hingga -10, -10 hingga 0, 0 hingga 10, yg masing-masing memiliki atau mewakili kategori untuk suhu. Dingin, sedang, dan suhu hangat. M adalah total keadaan yang bisa dilakukan oleh atribut ordinal. Untuk atribut ordinal yaitu cukup sama dengan atribut numerik ketika meghitung disimilarity antar objek. Contohnya f yaitu atribut yang ordinal atributnya dari n objek. Cara untuk menghitung disimilarity terhadap f fitur adalah: xif adalah nilai f untuk objek ke-i, dan f mempunyai Mf status urutan, mewakili rangking 1, ... , Mf Ganti tiap xif dengan rangkingnya, rif E 1...Mf Tiap atribut ordinal mempunyai jumlah state yang berbeda, diperlukan untuk memetakan rentang tiap atribut ke [0,0 ,1.0] sehingga tiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data melalui cara mengganti peringkat rif dengan: \\begin{equation} z_{i} f=\\frac{r_{i} f-1}{M_{f}-1} \\end{equation} \\begin{equation} z_{i} f=\\frac{r_{i} f-1}{M_{f}-1} \\end{equation} Kemudian pada disimilarity nya dihitung dg pakai ukuran jarak seperti atribut numerik dg data yang baru setelah ditransformasi zif. Mencari Jarak Data Tipe Campuran Menggunakan Python \u00b6 Alat dan Bahan \u00b6 Pada kasus ini saya telah mempunyai data tipe campuran yang telah saya simpan, dan dapat diunduh disini disini . Jangan lupa untuk mengunduh Library nya di internet. Library yang harus dipersiapkan sebagai berikut: \u200b 1- pandas, untuk manajemen dan analisis data. \u200b 2- scipy, library kumpulan algoritma dan fungsi matematika Pertama \u00b6 Langkah pertama adalah memasukkan library yg telah diunduh dengan cara mengetikan code di bawah ini, import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder Kedua \u00b6 Selanjutnya baca file excel tersebut data = pd . read_csv ( 'data-mhs.xlxs' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () maka akan tampil seperti berikut: Nama Jenis Kelamin IPK Penghasilan Orangtua Alamat Prestasi Ali L 3.4 3000000 Sumenep Internasional Ani P 3.2 5000000 Surabaya Regional Abi L 3.3 4000000 Bangkalan Nasional Ketiga \u00b6 Ketikan code di bawah ini untuk melakukan normalisasi pada data numerikal: def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) Fungsi dibawah ini untuk menerapkan rumus Euclidian Distance menghitung jarak bertipe numerikal: #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) Sedangkan fungsi dibawah ini untuk menghitung jarak pada data yg memiliki tipe binery symetris: #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t )) Sedangkan fungsi yang ini untuk menghitung jarak tipe kategorikal dan normalisasi pada tipe ordinal: #Menghitung Jarak tipe categorikal def distanceNom ( x , y ): p = len ( x ) or len ( y ) m = 0 for i in range ( len ( x )): if x [ i ][ 0 ] == y [ i ][ 0 ]: m += 1 return ( p - m ) / p #inisialisasi x = { 'Internasional' : 3 , 'Nasional' : 2 , 'Regional' : 1 } #Menghitung Jarak tipe ordinal def normalizedOrd ( y ): i_max = 0 for i in x : if x [ i ] > i_max : i_max = x [ i ] if y [ 0 ] == i : i_val = x [ i ] return ( i_val - 1 ) / ( i_max - 1 ) Setelah semuanya selesai maka kita akan mendapatkan hasil seperti dibawah ini: 0 1 2 3 Ali Ani Abi Ali 0 Ani 5.83 0 Abi 2.91 3.91 0 Source \u00b6 file saya dapat diunduh disini Sumber Statistik Deskriptif \u00b6 https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"2- Menghitung Jarak Data"},{"location":"2jarak/#mengukur-jarak-tipe-numerik","text":"Mengukur jarak dapat diartikan sebuah komponen utama dalam algoritma clustering yang basisnya jarak. Misal seperti K-Mean, K-medoidm dan fuzzy c-mean serta rough clustering untuk melakukan pengelompokkan pada jarak. Cara yang bisa digunakan untuk menghitung similaritas (jarak) dari sebuah data yang tipenya numerik adalah sebagai berikut:","title":"Mengukur Jarak Tipe Numerik"},{"location":"2jarak/#minkowski-distance","text":"Euclidean distance dan Manhattan distance adalah kelompok dari Minkowski yang menjadi kasus khusus dari Minkowski distance. Minkowski distance sendiri dapat dinyatakan dengan: d_{\\min }=\\left(\\operatorname{sum}_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{m}\\right)^{\\frac{1}{m}}, m \\geq 1 d_{\\min }=\\left(\\operatorname{sum}_{i=1}^{n}\\left|x_{i}-y_{i}\\right|^{m}\\right)^{\\frac{1}{m}}, m \\geq 1 keterangan: m: bil. Real positif | xi dan yi: dua vektor dalam runang dimensi | n: implementasi ukuran jarak","title":"Minkowski Distance"},{"location":"2jarak/#manhattan-distance","text":"Manhattan distance yaitu sebuah kasus khusus yang jarak Minkowski distance nya pada m=1. Seperti sebelumnya, Manhattan distance sensitif pada outlier. Bila ukurannya dipakai pada algoritma clustering, maka bentuk clusternya adalah hyper-rectangular. Ukurannya ini dapat diartikan dengan $$ d_{\\operatorname{man}}=\\sum_{i=1}^{n}\\left|x_{i}-y_{i}\\right| $$","title":"Manhattan Distance"},{"location":"2jarak/#euclidian-distance","text":"Jarak Euiclidian ini adalah jarak yang paling terkenal dipakai pada data numerik. Pada jarak Minkowski ini adalah kasus khusus ketika m=2. Saat digunakan pada himpunan data cluster kompak atau terisolasi, jarak Euclidean ini memiliki kinerja yang baik. Namun disisi lain, ia juga memiliki kelemahan seperti: pada dua vektor data jika memiliki nilai atribut yang sama, dan pada jarak yang lebih kecil dibanding pasangan vektor data lainnya yang memiliki nilai atribut yang sama. Kelemahan yang lain terdapat pada masalah dengan jarak Euclidean untuk fitur skala terbesar yang mendominasi. Adapun cara yang bisa digunakan untuk mengatasi kelemahan ini dapat menggunakan normalisasi fitur kontinu. Berikut adalah pernyataan dari Euclidian Distance: \\left.d_{( } x, y\\right)=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}} \\left.d_{( } x, y\\right)=\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}","title":"Euclidian Distance"},{"location":"2jarak/#average-distance","text":"Terjadinya kelemahan pada Jarak Euclidean di atas, maka untuk memperbaiki hasil dari versi modifikasi jarak euclidean dilakukan rata-rata pada jarak. Dua titik x,y pada ruang dimensi n, dan rata-rata pada jarak dapat didefinisi kan sebagai berikut: d_{a v e}=\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}} d_{a v e}=\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}}","title":"Average Distance"},{"location":"2jarak/#weighted-euclidean-distance","text":"Weighted Euclidean Distance bisa diartikan sebagai modifikasi lain dari jarak euclidean yang dapat digunakan dan berdasarkan pada tingkatan yang penting dari masing-masing atribut yang ditentukan. Rumus dari ukuran ini dapat dilihat di bawah ini dengan ketentuan wi adalah bobot yang diberikan pada atribut ke i. d_{w e}=\\left(\\sum_{i=1}^{n} w_{i}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}} d_{w e}=\\left(\\sum_{i=1}^{n} w_{i}\\left(x_{i}-y_{i}\\right)^{2}\\right)^{\\frac{1}{2}}","title":"Weighted Euclidean Distance"},{"location":"2jarak/#chord-distance","text":"Salah satu ukuran jarak yang memodifikasi Euclidean Distance untuk memecahkan kekurangan darinya bisa disebut sebagai arti dari Chord Distance. Skala pengukuran yang baik juga dapat memecahkan ini. Ketika terdapat data yang jaraknya dalam keadaan tidak dinormalisasi maka dapat menggunakan perhitungan jarak ini. Chord distance bisa ditampilkan seperti di bawah ini: \\begin{aligned} d_{\\text {chord }} &=\\left(2-2 \\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}}\\right)^{\\frac{1}{2}} \\end{aligned} \\begin{aligned} d_{\\text {chord }} &=\\left(2-2 \\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}}\\right)^{\\frac{1}{2}} \\end{aligned} \\begin{aligned}\\text { dimana }|x|_{2} \\text { adalah } L^{2}-\\text { norm }|x|_{2} &=\\sqrt{\\sum_{i=1}^{n} x_{i}^{2}} \\end{aligned} \\begin{aligned}\\text { dimana }|x|_{2} \\text { adalah } L^{2}-\\text { norm }|x|_{2} &=\\sqrt{\\sum_{i=1}^{n} x_{i}^{2}} \\end{aligned}","title":"Chord Distance"},{"location":"2jarak/#mahalanobis-distance","text":"Berdasarkan pada data yg beda dengan Euclidean dan Manhattan Distance yg bebas antara data satu dan data yang lain maka itu disebut Mahalanobis Distance. Jarak Mahalanobis ini yang beraturan bisa dipakai untuk meng ektsrasi hyperellipsoidal clusters. Jarak ini juga dapat mengurangi distorsi yang disebabkan oleh korelasi linear antar fitur dg diterapkannya transformasi pemutihan pada data atau dg memakai kuadrat jarak mahalanobis. Mahalanobis ini dapat dinyatakan sebagai berikut: d_{m a h}=\\sqrt{(x-y) S^{-1}(x-y)^{T}} d_{m a h}=\\sqrt{(x-y) S^{-1}(x-y)^{T}} dengan S sbg data matrik convariance","title":"Mahalanobis Distance"},{"location":"2jarak/#cosine-measure","text":"Ukuran Cosien similarity ini lebih banyak dipakai untuk similaritas dokumen dan bisa dinyatakan: \\operatorname{cosine}(x, y)=\\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}} \\operatorname{cosine}(x, y)=\\frac{\\sum_{i=1}^{n} x_{i} y_{i}}{|x|_{2}|y|_{2}} dengan ketentuan |y|2 adalah Euclidean norm dari vektor y=(y1,y2,...yn) dan di definisikan dengan \\begin{equation} |y|_{2}=\\sqrt{y_{1}^{2}+y_{2}^{2}+\\ldots+y_{n}^{2}} \\end{equation} \\begin{equation} |y|_{2}=\\sqrt{y_{1}^{2}+y_{2}^{2}+\\ldots+y_{n}^{2}} \\end{equation}","title":"Cosine Measure"},{"location":"2jarak/#pearson-correlation","text":"Pearson correlation banyak dipakai pada data expresi gen. Pearson Correlation ini memiliki kelemahan sensitif terhadap outlier. Ukurannya similaritas ini dihitung dengan cara similaritas antar dua bentuk pola expresi gen. Bisa juga didefinisikan sebagai berikut: \\begin{equation} \\operatorname{Pearson}(x, y)=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{x}\\right)\\left(y_{i}-\\mu_{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}} \\end{equation} \\begin{equation} \\operatorname{Pearson}(x, y)=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\mu_{x}\\right)\\left(y_{i}-\\mu_{y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-y_{i}\\right)^{2}}} \\end{equation}","title":"Pearson Correlation"},{"location":"2jarak/#mengukur-jarak-tipe-binary","text":"Atribut pada biner hanya memiliki dua status, yang pertama 0 dan kedua 1. Contoh dari atribut biner yaitu hasil tes pengguna narkoba, dimana tes urine akan mendapatkan hasil positif dipresentasikan menjadi 1 dan negatif atau 0. Dalam perhitungan pada jarak tipe biner ada cara khusus tidak boleh menyamakan dg cara perhitungan jarak tipe numerik . Lalu, bagaimana cara kita menghitung ketidaksamaan antara dua atribut biner tersebut? Yaitu dengan sebuah pendekatan yg melibatkan perhitungan matriks ketidaksamaan dari data biner yg diberikan. Jika semua atribut pada biner dianggap memiliki bobot yg sama, kita memiliki tabel kontingensi 2x2. Dimana q adalah jumlah daripada atribut yg sama dengan 1 untuk kedua objek i dan j. kemudian r adalah jumlah atribut yg sama dengan 1 untuk objek i tetapi 0 untuk j. Sedangkan s adalah jumlah atribut yang sama dengan 0 untuk objek i namun bernilai 1 untuk objek j. Dan t adalah jumlah atribut yang sama dengan 0 untuk kedua objek yaitu i dan j. Jumlah total atribut dimisalkan dengan simbol p, dimana p=q+r+s+t. Maka dapat dinyatakan sebagai atribut biner simetris seperti dibawah ini, dengan catatan jika objek i dan j dinyatakan sebagai atribut biner simetris. \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s+t} \\end{equation} \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s+t} \\end{equation} Sedangkan untuk atribut a-simetris, pada kedua kondisi dinyatakan tidak sama pentingnya, seperti hasil positif atau 1 dan negatif atau 0 seperti dari tes pengguna narkoba diatas. Ketidaksamaan berdasarkan atribut-atribut ini disebut juga sebagai asimetris biner dissimilarity, yaitu jumlah kecocokan negatif, t, dinyatakan tidak penting, dan diabaikan. Sehingga dapat diperhitungkan sebagai berikut: \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s} \\end{equation} \\begin{equation} d(i, j)=\\frac{r+s}{q+r+s} \\end{equation} Lalu kita dapat mengukur perbedaan antara dua atribut biner berdasarkan disimilarity nya. Misal biner asimetris kesamaan antara objek i dan j kita dapat menghitungnya dengan cara: \\begin{equation} \\operatorname{sim}(i, j)=\\frac{q}{q+r+s}=1-d(i, j) \\end{equation} \\begin{equation} \\operatorname{sim}(i, j)=\\frac{q}{q+r+s}=1-d(i, j) \\end{equation} Atau bisa disebut dengan Jaccard Coefficient untuk persamaan similarity ini.","title":"Mengukur Jarak Tipe Binary"},{"location":"2jarak/#mengukur-jarak-tipe-kategorikal","text":"Pada sebuah data bertipe kategorikal dapat membawa dua atau lebih pernyataan. Contoh, terdapat map_color pada sebuah atribut normal yang mempunyai 5 pernyataan sebagai berikut: merah, kuning, biru, hijau, dan ungu. Status bisa disimbolkan dengan byletters, symbol, atau satu set bilangan bulat, misal 1,2,3...,m. Perhatikan bahwa pada bilangan bulat tersebut digunakan hanya untuk menangani data dan bukan mewakili pemesanan khusus apapun. Sedangkan bedanya dua objek i dan objek j dapat kita hitung dengan rasio ketidakcocokan, sebagai berikut: \\begin{equation} d_{(i, j)}=\\frac{p-m}{p} \\end{equation} \\begin{equation} d_{(i, j)}=\\frac{p-m}{p} \\end{equation} dengan catatan, m merupakan angka yg cocok atau nomor cocok untuk i dan j. Dan p adalah banyaknya fitur yg dihitung sebagai tipe nominal.","title":"Mengukur Jarak Tipe Kategorikal"},{"location":"2jarak/#mengukur-jarak-tipe-ordinal","text":"Para nilai pada atribut ordinal punya urutan atau rangking, tetapi besarnya antara para nilai tidak diketahui. Misalnya tingkat terkecil, sedang, dan terbesar untuk atribut ukuran. Atribut ordinal bisa juga didapat dari diskritisasi atribut numerik dengan membuat rentang nilai pada sejumlah kategori tertentu. Kategori ini juga disusun pada rangking. Rentang pada atribut numerik bisa dipetakan ke atribut ordinal f yg memiliki Mf state. Contohnya, kisaran suhu atribut skala atau Celcius bisa diatur dalam skala: -30 hingga -10, -10 hingga 0, 0 hingga 10, yg masing-masing memiliki atau mewakili kategori untuk suhu. Dingin, sedang, dan suhu hangat. M adalah total keadaan yang bisa dilakukan oleh atribut ordinal. Untuk atribut ordinal yaitu cukup sama dengan atribut numerik ketika meghitung disimilarity antar objek. Contohnya f yaitu atribut yang ordinal atributnya dari n objek. Cara untuk menghitung disimilarity terhadap f fitur adalah: xif adalah nilai f untuk objek ke-i, dan f mempunyai Mf status urutan, mewakili rangking 1, ... , Mf Ganti tiap xif dengan rangkingnya, rif E 1...Mf Tiap atribut ordinal mempunyai jumlah state yang berbeda, diperlukan untuk memetakan rentang tiap atribut ke [0,0 ,1.0] sehingga tiap atribut memiliki bobot yang sama. Perl melakukan normalisasi data melalui cara mengganti peringkat rif dengan: \\begin{equation} z_{i} f=\\frac{r_{i} f-1}{M_{f}-1} \\end{equation} \\begin{equation} z_{i} f=\\frac{r_{i} f-1}{M_{f}-1} \\end{equation} Kemudian pada disimilarity nya dihitung dg pakai ukuran jarak seperti atribut numerik dg data yang baru setelah ditransformasi zif.","title":"Mengukur Jarak Tipe Ordinal"},{"location":"2jarak/#mencari-jarak-data-tipe-campuran-menggunakan-python","text":"","title":"Mencari Jarak Data Tipe Campuran Menggunakan Python"},{"location":"2jarak/#alat-dan-bahan","text":"Pada kasus ini saya telah mempunyai data tipe campuran yang telah saya simpan, dan dapat diunduh disini disini . Jangan lupa untuk mengunduh Library nya di internet. Library yang harus dipersiapkan sebagai berikut: \u200b 1- pandas, untuk manajemen dan analisis data. \u200b 2- scipy, library kumpulan algoritma dan fungsi matematika","title":"Alat dan Bahan"},{"location":"2jarak/#pertama","text":"Langkah pertama adalah memasukkan library yg telah diunduh dengan cara mengetikan code di bawah ini, import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder","title":"Pertama"},{"location":"2jarak/#kedua","text":"Selanjutnya baca file excel tersebut data = pd . read_csv ( 'data-mhs.xlxs' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () maka akan tampil seperti berikut: Nama Jenis Kelamin IPK Penghasilan Orangtua Alamat Prestasi Ali L 3.4 3000000 Sumenep Internasional Ani P 3.2 5000000 Surabaya Regional Abi L 3.3 4000000 Bangkalan Nasional","title":"Kedua"},{"location":"2jarak/#ketiga","text":"Ketikan code di bawah ini untuk melakukan normalisasi pada data numerikal: def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) Fungsi dibawah ini untuk menerapkan rumus Euclidian Distance menghitung jarak bertipe numerikal: #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) Sedangkan fungsi dibawah ini untuk menghitung jarak pada data yg memiliki tipe binery symetris: #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t )) Sedangkan fungsi yang ini untuk menghitung jarak tipe kategorikal dan normalisasi pada tipe ordinal: #Menghitung Jarak tipe categorikal def distanceNom ( x , y ): p = len ( x ) or len ( y ) m = 0 for i in range ( len ( x )): if x [ i ][ 0 ] == y [ i ][ 0 ]: m += 1 return ( p - m ) / p #inisialisasi x = { 'Internasional' : 3 , 'Nasional' : 2 , 'Regional' : 1 } #Menghitung Jarak tipe ordinal def normalizedOrd ( y ): i_max = 0 for i in x : if x [ i ] > i_max : i_max = x [ i ] if y [ 0 ] == i : i_val = x [ i ] return ( i_val - 1 ) / ( i_max - 1 ) Setelah semuanya selesai maka kita akan mendapatkan hasil seperti dibawah ini: 0 1 2 3 Ali Ani Abi Ali 0 Ani 5.83 0 Abi 2.91 3.91 0","title":"Ketiga"},{"location":"2jarak/#source","text":"file saya dapat diunduh disini","title":"Source"},{"location":"2jarak/#sumber-statistik-deskriptif","text":"https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"Sumber Statistik Deskriptif"},{"location":"3knn/","text":"Missing Values \u00b6 Missing Values atau bisa disebut nilai yang hilang adalah kejadian yg biasa terjadi dimana nilai yg hilang dapat menandakan total yang berbeda pada sebuah data. Dan mungkin data belum tersedia atau tidak berlaku. Missing Value biasanya disebabkan oleh seseorang ketika tidak tahu atau lupa saat memasukkan data. Metode penambangan data bervariasi dalam memperlakukan nilai yang hilang. Biasanya mereka tidak peduli terhadap nilai yang hilang, atau bisa juga menggantinya dengan nilai yang sesuai dengan cara mencari nilai tengahnya. Algoritma dalam KNN (K-Nearest Neighbors) \u00b6 Algoritma K-Nearest Naighbors atau KNN adalah sebuah algoritma klasifikasi sederhana yang bisa digunakan untuk memprediksi klasifikasi dan regresi. Tujuannya cukup sederhana yaitu untuk mengklasifikasi objek baru berdasarkan atribut dan sample data training. Penyelesaian yang bisa dilakukan oleh algoritma tersebut adalah: Tentukan jumlah tetangga terdekat yang nantinya akan dihitung. Misal tetangga yang ditentukan adalah 2 (k=2). Hitung jarak objek yg telah dipilih dengan semua tetangga yg ada. Kemudian urutkan sesuai jarak yg diperoleh dari yang terkecil hingga terbesar. Ambil 2 tetangga terdekat atau nilai jarak terkecil. Ambil rata-ratanya. Mengatasi Missing Value Menggunakan Algoritma KNN pada Python \u00b6 Alat dan Bahan \u00b6 Pada kasus kali ini saya menggunakan datasetnya internet. Pada dataset terdapat 1 fitur bertipe binary dan 5 fitur bertipe numerikal. Pada figur umur (age) dataset telah dimodifikasi dan memberikan missing value pada baris ke-257. Untuk mempermudah dalam penyelesaiannya perlu disiapkan library python untuk memudahkan pekerjaan. Library yang dibutuhkan: pandas, manajemen dan analis data scipy, kumpulan algoritma dan fungsi matematika. Kesatu \u00b6 install terlebih dahulu librarynya dengan cara ketikan import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder Kedua \u00b6 selanjutnya memuat data, pada kasus ini saya menggunakan file excel data = pd . read_csv ( 'Wong.csv' , delimiter = ';' , decimal = ',' ) df = pd . DataFrame ( data ) df . style . highlight_null ( null_color = 'red' ) . hide_index () maka akan tampil seperti di bawah: no id days duration sex age piq viq 1 3358 30 4 Male 20.6708 87 89 2 3535 16 17 Male 55.2882 95 77 3 3547 40 1 Male 55.9151 95 116 4 3592 13 10 Male 61.6646 59 73 5 3728 19 6 Male 30.1273 67 73 6 3790 13 3 Male 57.0623 76 69 7 3807 37 5 Male 24.6762 74 77 8 3808 31 7 Male 28.2683 91 110 9 4253 40 3 Male 22.6037 115 110 10 4356 31 7 Male 21.399 86 83 11 4384 35 8 Male 36.3806 76 90 12 4542 22 11 Female 21.9576 71 89 13 4705 18 1 Female 21.6838 127 109 14 4744 15 25 Male 57.566 82 85 15 4802 36 0 Male 62.475 88 97 16 4941 46 4 Female 19.0144 69 88 17 4983 33 5 Male 38.3929 102 117 18 5129 26 1 Male 25.0459 77 89 19 5154 35 5 Male 22.1903 82 95 20 5162 33 1 Male 25.0185 118 101 21 5174 38 4 Female 37.2704 87 99 22 5208 31 8 Female 21.3771 97 90 23 5253 29 1 Male 33.1335 104 105 24 5298 30 3 Male 22.9569 87 86 25 5640 34 7 Male 25.9986 93 113 26 5668 27 7 Male 40.9227 72 79 27 5680 17 1 Male 27.7563 84 90 28 5699 26 1 Female 34.2231 95 108 29 5713 36 8 Male 16.2683 89 97 30 5736 18 9 Male 16.1478 89 86 31 5754 36 1 Male 16.3368 87 86 32 5776 26 8 Male 17.128 71 88 33 6122 29 1 Male 56.2108 95 103 34 6163 21 1 Male 19.3593 112 106 35 6179 22 2 Male 38.0123 89 95 36 6671 30 7 Female 27.8056 71 82 37 6859 27 1 Male 34.2122 74 79 38 6870 22 0 Male 42.4832 84 95 39 6914 43 0 Male 61.5222 85 90 40 6937 18 0 Female 21.191 94 81 41 6977 30 1 Male 36.2108 97 94 42 7120 39 0 Male 69.7057 84 86 43 7309 31 0 Female 50.6667 85 95 44 7321 23 0 Male 26.0041 84 83 45 7548 31 0 Male 24.3669 108 106 46 2364 41 14 Male 25.8097 84 94 47 2600 3333 9 Male 43.9398 86 80 48 2761 40 3 Female 24.3696 98 112 49 3237 65 9 Male 49.8508 67 67 50 3277 51 1 Male 37.4702 104 96 51 3346 44 18 Female 57.2758 79 85 52 3359 59 9 Female 56.8953 84 91 53 3373 39 28 Female 26.308 87 91 54 3544 32 14 Male 54.5298 81 98 55 3655 57 5 Female 21.9055 90 103 56 3762 48 6 Male 20.3559 85 93 57 3919 58 1 Male 30.3655 99 95 58 4094 50 2 Male 19.7262 79 93 59 4133 34 14 Male 20 70 88 60 4183 42 3 Male 26.2341 98 116 61 4189 69 4 Female 29.462 75 86 62 4315 63 0 Male 38.141 107 130 63 4482 58 14 Female 18.2341 86 103 64 4638 20 17 Male 20.512 82 72 65 4678 63 7 Male 46.6448 96 95 66 4696 54 4 Male 46.9569 101 112 67 4755 24 18 Male 27.5127 105 102 68 4837 42 10 Male 19.6906 83 88 69 4996 51 12 Male 43.0281 77 78 70 5009 50 7 Male 24.3806 61 104 71 5014 46 7 Female 23.7618 75 90 72 5192 60 1 Male 58.6283 87 97 73 5204 71 0 Male 59.0746 97 107 74 5238 44 3 Male 45.1006 99 103 75 5280 83 1 Male 48.6434 78 88 76 5289 52 1 Male 48.5722 84 85 77 5456 48 14 Male 41.1636 80 101 78 5458 44 14 Male 34.4778 84 95 79 5474 65 2 Female 28.6598 95 86 80 5568 64 1 Female 51.9918 75 79 81 5580 56 7 Male 17.7933 86 95 82 5581 65 2 Male 26.3053 85 95 83 5628 51 3 Female 30.2642 81 85 84 6154 43 5 Female 22.6064 74 80 85 6180 59 12 Male 20.7201 67 84 86 6314 58 3 Male 16.6927 80 99 87 6340 71 0 Male 19.3238 76 72 88 6564 69 0 Male 34.4997 67 74 89 6614 57 0 Male 45.1116 80 101 90 6686 44 14 Female 38.3491 90 100 91 6795 55 0 Male 30.7159 87 104 92 7080 64 5 Female 76.6598 76 106 93 7084 54 2 Male 36.5722 87 93 94 7271 55 0 Male 41.7659 100 95 95 7371 55 1 Male 56.7858 80 88 96 2569 49 35 Male 18.7159 50 101 97 3058 56 28 Male 22.2533 65 75 98 3645 43 45 Male 27.4935 72 90 99 3844 73 9 Male 26.1164 79 94 100 4725 124 10 Male 32.9172 93 97 101 4744 65 25 Male 57.566 105 119 102 4807 64 14 Female 47.7974 74 74 103 4892 62 21 Male 22.0397 76 88 104 4962 63 1 Female 25.1964 69 67 105 5125 78 12 Male 17.5387 94 118 106 5222 63 30 Male 22.5298 77 85 107 5253 86 1 Male 33.1335 106 128 108 5386 78 21 Male 20.8761 78 93 109 5534 87 14 Male 29.2621 75 82 110 5712 88 14 Male 22.2697 70 68 111 5837 82 1 Female 33.3087 82 110 112 5879 75 21 Male 25.8453 80 105 113 5893 71 21 Male 22.8118 65 90 114 5916 84 0 Female 26.8556 93 73 115 6410 80 14 Male 32.1725 85 98 116 7173 84 4 Male 24.9801 72 75 117 7221 98 0 Male 63.5044 74 79 118 2453 120 10 Male 37.2758 63 99 119 2653 97 28 Male 30.0068 93 112 120 4218 82 28 Male 25.9904 74 92 121 4542 121 11 Female 21.9576 86 114 122 4902 102 8 Male 16.1424 87 77 123 4933 134 0 Male 18.4559 69 83 124 4941 131 4 Female 19.0144 96 96 125 5085 117 2 Male 49.0267 67 71 126 5111 107 7 Male 21.6947 71 80 127 5154 120 5 Male 22.1903 89 109 128 5222 93 30 Male 22.5298 77 91 129 5298 107 3 Male 22.9569 117 112 130 5339 119 7 Male 21.8152 87 82 131 5387 109 12 Male 21.7988 85 112 132 5414 105 10 Female 40.2765 93 104 133 5494 111 7 Male 54.6913 86 86 134 5896 126 4 Female 26.8775 50 74 135 5901 115 7 Male 22.1739 112 116 136 6135 96 18 Male 26.5626 66 105 137 6173 125 4 Male 35.3046 94 97 138 6214 112 0 Male 60.3176 65 74 139 6253 128 0 Female 46.4038 104 112 140 6433 120 4 Male 23.8604 100 103 141 6665 119 3 Female 23.0171 106 94 142 6834 123 0 Male 30.7488 72 75 143 1176 146 17 Female 19.729 65 98 144 2849 151 0 Male 20.0876 51 86 145 2882 141 18 Male 19.2334 84 85 146 3051 131 13 Male 37.2403 68 79 147 3728 151 6 Male 30.1273 96 105 148 3913 96 42 Female 23.9233 56 80 149 4133 133 14 Male 20 82 94 150 4661 135 17 Female 30.8419 84 93 151 4678 143 7 Male 46.6448 98 107 152 4696 150 4 Male 46.9569 120 120 153 4705 146 1 Female 21.6838 133 111 154 4802 142 0 Male 62.475 101 117 155 4807 139 14 Female 47.7974 80 78 156 4983 146 5 Male 38.3929 107 123 157 5014 151 7 Female 23.7618 97 110 158 5162 144 1 Male 25.0185 130 118 159 5238 150 3 Male 45.1006 117 126 160 5642 162 0 Male 65.87 89 103 161 5699 138 1 Female 34.2231 110 107 162 5713 144 8 Male 16.2683 100 99 163 5804 159 2 Female 28.8515 102 107 164 5818 125 14 Male 34.9268 72 91 165 6314 140 3 Male 16.6927 87 96 166 6664 164 2 Male 24.7337 66 73 167 1048 85 94 Male 20.115 63 82 168 1085 159 11 Male 30.7105 103 97 169 3237 189 9 Male 49.8508 79 82 170 3358 175 4 Male 20.6708 97 97 171 3808 165 7 Male 28.2683 94 111 172 4094 177 2 Male 19.7262 89 102 173 4253 175 3 Male 22.6037 114 118 174 4638 140 17 Male 20.512 89 78 175 4755 128 18 Male 27.5127 105 109 176 4865 142 35 Male 58.3354 84 103 177 4892 148 21 Male 22.0397 106 110 178 5009 174 7 Male 24.3806 77 103 179 5111 177 7 Male 21.6947 72 81 180 5125 173 12 Male 17.5387 106 119 181 5192 179 1 Male 58.6283 93 105 182 5505 171 1 Male 65.4784 95 93 183 5581 176 2 Male 26.3053 96 110 184 5599 148 21 Male 18.7488 72 81 185 5680 184 1 Male 27.7563 84 90 186 5782 108 68 Female 19.6715 69 85 187 6180 177 12 Male 20.7201 81 94 188 6671 184 7 Female 27.8056 91 92 189 2124 173 30 Male 30.7625 76 106 190 2646 187 14 Male 22.9158 97 97 191 2790 211 0 Male 48.8049 89 99 192 4189 202 4 Female 29.462 81 90 193 4775 180 28 Male 53.5551 70 86 194 4933 226 0 Male 18.4559 79 86 195 4962 210 1 Female 25.1964 71 70 196 5208 193 8 Female 21.3771 133 111 197 5456 193 14 Male 41.1636 87 110 198 5668 219 7 Male 40.9227 76 90 199 5712 192 14 Male 22.2697 87 85 200 5893 200 21 Male 22.8118 65 89 201 5916 205 0 Female 26.8556 92 76 202 6122 212 1 Male 56.2108 109 117 203 6136 216 1 Male 32.7912 92 89 204 6175 278 1 Male 51.1704 99 98 205 6228 174 3 Female 31.5537 114 108 206 7173 210 4 Male 24.9801 79 78 207 1176 216 17 Female 19.729 74 100 208 3467 186 42 Male 25.3936 53 69 209 4744 217 25 Male 57.566 108 118 210 5386 241 21 Male 20.8761 80 94 211 5837 242 1 Female 33.3087 93 105 212 6247 228 13 Male 42.3162 77 80 213 1892 276 2 Male 21.7796 87 107 214 2882 262 18 Male 19.2334 94 90 215 3058 236 28 Male 22.2533 85 88 216 4342 263 1 Male 44.063 79 91 217 4865 240 35 Male 58.3354 93 105 218 5085 269 2 Male 49.0267 65 77 219 5222 247 30 Male 22.5298 88 85 220 5339 271 7 Male 21.8152 94 89 221 5474 280 2 Female 28.6598 99 91 222 5600 232 0 Male 48.7885 75 81 223 2826 290 14 Male 23.2334 94 108 224 4725 286 10 Male 32.9172 105 94 225 5204 299 0 Male 59.0746 99 105 226 6498 270 28 Male 24.0767 82 101 227 2081 185 43 Male 17.6975 77 97 228 4678 340 7 Male 46.6448 108 119 229 5397 328 0 Female 62.7981 121 108 230 6214 318 0 Male 60.3176 78 82 231 7034 280 60 Male 23.1376 78 80 232 1493 453 60 Male 17.8042 59 81 233 1836 375 1 Male 47.0554 101 108 234 1939 295 130 Male 28.2738 67 117 235 2646 438 14 Male 22.9158 98 94 236 2653 352 28 Male 30.0068 105 126 237 3226 444 0 Male 27.4552 76 64 238 3467 333 42 Male 25.3936 68 74 239 4342 432 1 Male 44.063 92 107 240 4542 431 11 Female 21.9576 98 114 241 4661 374 17 Female 30.8419 93 95 242 4902 397 8 Male 16.1424 92 86 243 4983 398 5 Male 38.3929 121 132 244 5111 442 7 Male 21.6947 77 86 245 5125 510 12 Male 17.5387 112 125 246 5289 417 1 Male 48.5722 83 83 247 5386 436 21 Male 20.8761 90 103 248 5387 480 12 Male 21.7988 94 116 249 5505 527 1 Male 65.4784 104 87 250 5580 369 7 Male 17.7933 96 107 251 5581 378 2 Male 26.3053 95 95 252 5599 443 21 Male 18.7488 78 80 253 5668 390 7 Male 40.9227 92 92 254 5680 403 1 Male 27.7563 94 93 255 5712 365 14 Male 22.2697 98 86 256 5772 412 35 Male 26.2587 102 104 257 5804 354 2 Female nan 122 105 258 5811 431 25 Male 80.0328 78 80 259 5841 415 8 Male 27.2279 82 83 260 6226 438 0 Male 36.8022 84 92 261 6247 389 13 Male 42.3162 82 80 262 6468 513 60 Male 43.4798 99 94 263 6614 362 0 Male 45.1116 88 106 264 6665 368 3 Female 23.0171 100 92 265 781 714 15 Male 29.8699 85 85 266 1048 576 94 Male 20.115 91 96 267 1157 810 23 Male 17.3881 97 84 268 1493 684 60 Male 17.8042 66 75 269 1611 511 60 Male 23.2799 69 107 270 1624 604 1 Male 19.5619 97 85 271 1939 562 130 Male 28.2738 85 111 272 2498 615 0 Female 17.4292 86 113 273 2826 636 14 Male 23.2334 111 101 274 2849 642 0 Male 20.0876 76 98 275 3032 525 20 Male 16.9391 79 87 276 3226 683 0 Male 27.4552 89 78 277 4218 814 28 Male 25.9904 99 96 278 4807 532 14 Female 47.7974 84 82 279 5014 637 7 Female 23.7618 101 114 280 5222 690 30 Male 22.5298 81 90 281 5253 591 1 Male 33.1335 114 124 282 5628 609 3 Female 30.2642 89 78 283 6059 794 1 Female 16.9801 71 76 284 6228 662 3 Female 31.5537 128 111 285 6247 616 13 Male 42.3162 85 82 286 405 986 0 Male 21.4702 66 116 287 626 870 55 Male 19.7536 80 85 288 1075 907 42 Female 27.2772 63 64 289 2849 1040 0 Male 20.0876 91 103 290 3032 884 20 Male 16.9391 87 93 291 3226 1123 0 Male 27.4552 88 81 292 4864 936 0 Female 53.9767 119 131 293 5474 1100 2 Female 28.6598 94 88 294 5568 1114 1 Female 51.9918 81 82 295 5580 1087 7 Male 17.7933 106 98 296 5581 1113 2 Male 26.3053 99 96 297 5617 1113 17 Male 19.7864 78 87 298 5642 1143 0 Male 65.87 104 109 299 5713 1016 8 Male 16.2683 126 106 300 5837 962 1 Female 33.3087 109 110 301 6140 1077 44 Female 21.4209 65 88 302 7061 923 0 Male 36.8816 74 81 303 651 1491 21 Male 22.0068 71 94 304 2527 1294 0 Male 16.9172 93 104 305 2638 1093 255 Male 16.5613 78 84 306 4865 1363 35 Male 58.3354 88 104 307 5009 1537 7 Male 24.3806 76 112 308 5014 1523 7 Female 23.7618 105 114 309 5085 1512 2 Male 49.0267 75 75 310 1939 1926 130 Male 28.2738 95 108 311 2662 1569 180 Male 28.0821 90 101 312 2826 1809 14 Male 23.2334 104 108 313 2882 1716 18 Male 19.2334 100 103 314 3768 1916 0 Male 19.1102 69 80 315 4356 2000 7 Male 21.399 104 91 316 4638 1779 17 Male 20.512 92 76 317 4696 1769 4 Male 46.9569 105 124 318 4744 1743 25 Male 57.566 97 118 319 6140 1742 44 Female 21.4209 67 87 320 1075 2259 42 Female 27.2772 78 79 321 1939 3111 130 Male 28.2738 88 111 322 2653 2191 28 Male 30.0068 117 129 323 3592 2569 10 Male 61.6646 76 93 324 3808 2434 7 Male 28.2683 105 111 325 651 3412 21 Male 22.0068 68 92 326 1939 3864 130 Male 28.2738 88 105 327 2600 3337 9 Male 43.9398 101 84 328 3835 4933 14 Male 25.9932 91 88 329 2773 7631 42 Male 6.51335 88 103 330 5142 11628 57 Male 16.4326 101 95 331 5964 11038 0 Male 12.8363 71 73 Ketiga \u00b6 Ubah nilai fitur sex menjadi angka 0 atau 1 dg memakai fungsi dari sklearn, LabelEncode() #encode fitur tipe biner X = data . iloc [:,:] . values labelEncode_X = LabelEncoder () X [:, 4 ] = labelEncode_X . fit_transform ( X [:, 4 ]) Keempat \u00b6 Implementasikan rumus jarak ke dalam bentuk fungsi python. yaitu: eulidianDistance() dengan fungsi jarak tipe binary distanceSimetris (). def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t )) Kelima \u00b6 Cek baris missing value: c_j = 0 for j in df [ 'age' ] . isna (): if j == True : col_missing = c_j c_j += 1 Keenam \u00b6 Hitung jarak pada missing data dengan semua tetangganya dan tampung pada dictionary: missing_data = df . iloc [ col_missing , [ 2 , 3 , 6 , 7 ]] . values missing_normal = [ normalisasi ( missing_data [ 0 ], data . columns [ 2 ]), normalisasi ( missing_data [ 1 ], data . columns [ 3 ]), normalisasi ( missing_data [ 2 ], data . columns [ 6 ]), normalisasi ( missing_data [ 3 ], data . columns [ 7 ])] for i in range ( len ( data [ data . columns [ 0 ]])): if i == col_missing : continue ; select_data = df . iloc [ i , [ 2 , 3 , 6 , 7 ]] . values normal_data = [ normalisasi ( select_data [ 0 ], data . columns [ 2 ]), normalisasi ( select_data [ 1 ], data . columns [ 3 ]), normalisasi ( select_data [ 2 ], data . columns [ 6 ]), normalisasi ( select_data [ 3 ], data . columns [ 7 ])] data . loc [ i , 'jarak' ] = euclidianDistance ( missing_normal , normal_data ) + distanceSimetris ([ X [ col_missing , 4 ]],[ X [ i , 4 ]]) Ketujuh \u00b6 Urutkan data berdasarkan jarak mulai terkecil sampai terbesar. Isi data yang hilang dengan mengambil rata-rata dari 2 tetangga terdekat. df = pd . DataFrame ( data ) df . sort_values ( by = 'jarak' , axis = 0 , ascending = True , inplace = True ) df . iloc [ - 1 , [ 5 ]] = round ( df . iloc [ 0 : 2 , 5 ] . mean (), 2 ) df . style . hide_index () Berikut tampilan data yang telah diurutkan. Pada baris terakhir terlihat kolom age sudah terisi dengan angka 42.24 diperoleh dari rata-rata 2 tetangga terdekat. duration sex age piq viq jarak 229 5397 328 0 Female 62.7981 121 108 0.25 13 4705 18 1 Female 21.6838 127 109 0.53 205 6228 174 3 Female 31.5537 114 108 0.6 284 6228 662 3 Female 31.5537 128 111 0.64 161 5699 138 1 Female 34.2231 110 107 0.84 153 4705 146 1 Female 21.6838 133 111 0.86 196 5208 193 8 Female 21.3771 133 111 0.88 300 5837 962 1 Female 33.3087 109 110 1.07 139 6253 128 0 Female 46.4038 104 112 1.31 141 6665 119 3 Female 23.0171 106 94 1.33 163 5804 159 2 Female 28.8515 102 107 1.35 20 5162 33 1 Male 25.0185 118 101 1.48 279 5014 637 7 Female 23.7618 101 114 1.56 129 5298 107 3 Male 22.9569 117 112 1.64 9 4253 40 3 Male 22.6037 115 110 1.66 308 5014 1523 7 Female 23.7618 105 114 1.67 299 5713 1016 8 Male 16.2683 126 106 1.68 48 2761 40 3 Female 24.3696 98 112 1.69 157 5014 151 7 Female 23.7618 97 110 1.72 264 6665 368 3 Female 23.0171 100 92 1.73 34 6163 21 1 Male 19.3593 112 106 1.73 240 4542 431 11 Female 21.9576 98 114 1.75 221 5474 280 2 Female 28.6598 99 91 1.82 28 5699 26 1 Female 34.2231 95 108 1.83 124 4941 131 4 Female 19.0144 96 96 1.85 211 5837 242 1 Female 33.3087 93 105 1.92 292 4864 936 0 Female 53.9767 119 131 1.93 273 2826 636 14 Male 23.2334 111 101 1.94 132 5414 105 10 Female 40.2765 93 104 1.96 45 7548 31 0 Male 24.3669 108 106 1.98 22 5208 31 8 Female 21.3771 97 90 2 135 5901 115 7 Male 22.1739 112 116 2.07 173 4253 175 3 Male 22.6037 114 118 2.08 158 5162 144 1 Male 25.0185 130 118 2.09 152 4696 150 4 Male 46.9569 120 120 2.1 241 4661 374 17 Female 30.8419 93 95 2.13 55 3655 57 5 Female 21.9055 90 103 2.14 202 6122 212 1 Male 56.2108 109 117 2.22 90 6686 44 14 Female 38.3491 90 100 2.22 23 5253 29 1 Male 33.1335 104 105 2.23 79 5474 65 2 Female 28.6598 95 86 2.26 188 6671 184 7 Female 27.8056 91 92 2.26 293 5474 1100 2 Female 28.6598 94 88 2.3 67 4755 24 18 Male 27.5127 105 102 2.33 175 4755 128 18 Male 27.5127 105 109 2.33 177 4892 148 21 Male 22.0397 106 110 2.35 295 5580 1087 7 Male 17.7933 106 98 2.35 50 3277 51 1 Male 37.4702 104 96 2.38 21 5174 38 4 Female 37.2704 87 99 2.38 228 4678 340 7 Male 46.6448 108 119 2.38 298 5642 1143 0 Male 65.87 104 109 2.41 224 4725 286 10 Male 32.9172 105 94 2.41 233 1836 375 1 Male 47.0554 101 108 2.41 168 1085 159 11 Male 30.7105 103 97 2.43 63 4482 58 14 Female 18.2341 86 103 2.44 272 2498 615 0 Female 17.4292 86 113 2.46 140 6433 120 4 Male 23.8604 100 103 2.48 281 5253 591 1 Male 33.1335 114 124 2.48 121 4542 121 11 Female 21.9576 86 114 2.5 66 4696 54 4 Male 46.9569 101 112 2.5 180 5125 173 12 Male 17.5387 106 119 2.51 225 5204 299 0 Male 59.0746 99 105 2.52 40 6937 18 0 Female 21.191 94 81 2.53 162 5713 144 8 Male 16.2683 100 99 2.55 159 5238 150 3 Male 45.1006 117 126 2.55 74 5238 44 3 Male 45.1006 99 103 2.55 43 7309 31 0 Female 50.6667 85 95 2.57 209 4744 217 25 Male 57.566 108 118 2.59 204 6175 278 1 Male 51.1704 99 98 2.6 17 4983 33 5 Male 38.3929 102 117 2.61 151 4678 143 7 Male 46.6448 98 107 2.62 245 5125 510 12 Male 17.5387 112 125 2.63 156 4983 146 5 Male 38.3929 107 123 2.64 94 7271 55 0 Male 41.7659 100 95 2.65 154 4802 142 0 Male 62.475 101 117 2.65 111 5837 82 1 Female 33.3087 82 110 2.69 73 5204 71 0 Male 59.0746 97 107 2.69 57 3919 58 1 Male 30.3655 99 95 2.7 150 4661 135 17 Female 30.8419 84 93 2.72 52 3359 59 9 Female 56.8953 84 91 2.72 53 3373 39 28 Female 26.308 87 91 2.73 250 5580 369 7 Male 17.7933 96 107 2.74 147 3728 151 6 Male 30.1273 96 105 2.74 249 5505 527 1 Male 65.4784 104 87 2.75 183 5581 176 2 Male 26.3053 96 110 2.76 170 3358 175 4 Male 20.6708 97 97 2.76 101 4744 65 25 Male 57.566 105 119 2.77 296 5581 1113 2 Male 26.3053 99 96 2.78 60 4183 42 3 Male 26.2341 98 116 2.8 312 2826 1809 14 Male 23.2334 104 108 2.82 190 2646 187 14 Male 22.9158 97 97 2.82 33 6122 29 1 Male 56.2108 95 103 2.82 235 2646 438 14 Male 22.9158 98 94 2.83 256 5772 412 35 Male 26.2587 102 104 2.84 41 6977 30 1 Male 36.2108 97 94 2.86 201 5916 205 0 Female 26.8556 92 76 2.87 65 4678 63 7 Male 46.6448 96 95 2.89 282 5628 609 3 Female 30.2642 89 78 2.91 192 4189 202 4 Female 29.462 81 90 2.91 223 2826 290 14 Male 23.2334 94 108 2.92 171 3808 165 7 Male 28.2683 94 111 2.92 251 5581 378 2 Male 26.3053 95 95 2.93 243 4983 398 5 Male 38.3929 121 132 2.93 181 5192 179 1 Male 58.6283 93 105 2.93 137 6173 125 4 Male 35.3046 94 97 2.95 107 5253 86 1 Male 33.1335 106 128 2.97 277 4218 814 28 Male 25.9904 99 96 2.97 114 5916 84 0 Female 26.8556 93 73 2.98 3 3547 40 1 Male 55.9151 95 116 2.98 313 2882 1716 18 Male 19.2334 100 103 2.99 182 5505 171 1 Male 65.4784 95 93 2.99 239 4342 432 1 Male 44.063 92 107 3 278 4807 532 14 Female 47.7974 84 82 3.03 25 5640 34 7 Male 25.9986 93 113 3.03 100 4725 124 10 Male 32.9172 93 97 3.04 254 5680 403 1 Male 27.7563 94 93 3.04 248 5387 480 12 Male 21.7988 94 116 3.05 62 4315 63 0 Male 38.141 107 130 3.05 92 7080 64 5 Female 76.6598 76 106 3.05 83 5628 51 3 Female 30.2642 81 85 3.07 304 2527 1294 0 Male 16.9172 93 104 3.09 8 3808 31 7 Male 28.2683 91 110 3.11 105 5125 78 12 Male 17.5387 94 118 3.12 236 2653 352 28 Male 30.0068 105 126 3.13 315 4356 2000 7 Male 21.399 104 91 3.13 289 2849 1040 0 Male 20.0876 91 103 3.14 255 5712 365 14 Male 22.2697 98 86 3.14 317 4696 1769 4 Male 46.9569 105 124 3.16 220 5339 271 7 Male 21.8152 94 89 3.18 160 5642 162 0 Male 65.87 89 103 3.19 324 3808 2434 7 Male 28.2683 105 111 3.2 172 4094 177 2 Male 19.7262 89 102 3.2 270 1624 604 1 Male 19.5619 97 85 3.2 253 5668 390 7 Male 40.9227 92 92 3.2 127 5154 120 5 Male 22.1903 89 109 3.21 214 2882 262 18 Male 19.2334 94 90 3.22 119 2653 97 28 Male 30.0068 93 112 3.23 191 2790 211 0 Male 48.8049 89 99 3.23 294 5568 1114 1 Female 51.9918 81 82 3.23 51 3346 44 18 Female 57.2758 79 85 3.25 263 6614 362 0 Male 45.1116 88 106 3.25 207 1176 216 17 Female 19.729 74 100 3.25 247 5386 436 21 Male 20.8761 90 103 3.25 29 5713 36 8 Male 16.2683 89 97 3.28 203 6136 216 1 Male 32.7912 92 89 3.29 217 4865 240 35 Male 58.3354 93 105 3.3 71 5014 46 7 Female 23.7618 75 90 3.3 35 6179 22 2 Male 38.0123 89 95 3.31 213 1892 276 2 Male 21.7796 87 107 3.33 91 6795 55 0 Male 30.7159 87 104 3.34 15 4802 36 0 Male 62.475 88 97 3.34 72 5192 60 1 Male 58.6283 87 97 3.4 61 4189 69 4 Female 29.462 75 86 3.4 267 1157 810 23 Male 17.3881 97 84 3.4 197 5456 193 14 Male 41.1636 87 110 3.4 165 6314 140 3 Male 16.6927 87 96 3.41 242 4902 397 8 Male 16.1424 92 86 3.42 155 4807 139 14 Female 47.7974 80 78 3.42 318 4744 1743 25 Male 57.566 97 118 3.43 93 7084 54 2 Male 36.5722 87 93 3.49 81 5580 56 7 Male 17.7933 86 95 3.5 131 5387 109 12 Male 21.7988 85 112 3.54 115 6410 80 14 Male 32.1725 85 98 3.55 82 5581 65 2 Male 26.3053 85 95 3.56 322 2653 2191 28 Male 30.0068 117 129 3.57 12 4542 22 11 Female 21.9576 71 89 3.58 1 3358 30 4 Male 20.6708 87 89 3.6 30 5736 18 9 Male 16.1478 89 86 3.6 290 3032 884 20 Male 16.9391 87 93 3.61 56 3762 48 6 Male 20.3559 85 93 3.61 80 5568 64 1 Female 51.9918 75 79 3.63 38 6870 22 0 Male 42.4832 84 95 3.63 84 6154 43 5 Female 22.6064 74 80 3.65 78 5458 44 14 Male 34.4778 84 95 3.66 46 2364 41 14 Male 25.8097 84 94 3.68 260 6226 438 0 Male 36.8022 84 92 3.68 39 6914 43 0 Male 61.5222 85 90 3.69 24 5298 30 3 Male 22.9569 87 86 3.7 31 5754 36 1 Male 16.3368 87 86 3.7 16 4941 46 4 Female 19.0144 69 88 3.72 185 5680 184 1 Male 27.7563 84 90 3.73 306 4865 1363 35 Male 58.3354 88 104 3.73 27 5680 17 1 Male 27.7563 84 90 3.74 2 3535 16 17 Male 55.2882 95 77 3.75 133 5494 111 7 Male 54.6913 86 86 3.75 36 6671 30 7 Female 27.8056 71 82 3.76 19 5154 35 5 Male 22.1903 82 95 3.76 199 5712 192 14 Male 22.2697 87 85 3.76 262 6468 513 60 Male 43.4798 99 94 3.81 149 4133 133 14 Male 20 82 94 3.81 89 6614 57 0 Male 45.1116 80 101 3.81 54 3544 32 14 Male 54.5298 81 98 3.81 176 4865 142 35 Male 58.3354 84 103 3.82 86 6314 58 3 Male 16.6927 80 99 3.82 77 5456 48 14 Male 41.1636 80 101 3.84 130 5339 119 7 Male 21.8152 87 82 3.85 187 6180 177 12 Male 20.7201 81 94 3.85 226 6498 270 28 Male 24.0767 82 101 3.85 143 1176 146 17 Female 19.729 65 98 3.85 42 7120 39 0 Male 69.7057 84 86 3.86 219 5222 247 30 Male 22.5298 88 85 3.87 10 4356 31 7 Male 21.399 86 83 3.87 68 4837 42 10 Male 19.6906 83 88 3.88 112 5879 75 21 Male 25.8453 80 105 3.89 265 781 714 15 Male 29.8699 85 85 3.89 76 5289 52 1 Male 48.5722 84 85 3.9 102 4807 64 14 Female 47.7974 74 74 3.9 291 3226 1123 0 Male 27.4552 88 81 3.9 215 3058 236 28 Male 22.2533 85 88 3.91 276 3226 683 0 Male 27.4552 89 78 3.92 145 2882 141 18 Male 19.2334 84 85 3.95 44 7321 23 0 Male 26.0041 84 83 3.97 283 6059 794 1 Female 16.9801 71 76 3.97 174 4638 140 17 Male 20.512 89 78 3.97 210 5386 241 21 Male 20.8761 80 94 3.98 285 6247 616 13 Male 42.3162 85 82 3.98 99 3844 73 9 Male 26.1164 79 94 3.98 178 5009 174 7 Male 24.3806 77 103 3.99 58 4094 50 2 Male 19.7262 79 93 3.99 246 5289 417 1 Male 48.5722 83 83 4.02 216 4342 263 1 Male 44.063 79 91 4.02 95 7371 55 1 Male 56.7858 80 88 4.04 122 4902 102 8 Male 16.1424 87 77 4.07 259 5841 415 8 Male 27.2279 82 83 4.08 274 2849 642 0 Male 20.0876 76 98 4.09 280 5222 690 30 Male 22.5298 81 90 4.11 320 1075 2259 42 Female 27.2772 78 79 4.13 108 5386 78 21 Male 20.8761 78 93 4.13 14 4744 15 25 Male 57.566 82 85 4.15 75 5280 83 1 Male 48.6434 78 88 4.16 194 4933 226 0 Male 18.4559 79 86 4.16 316 4638 1779 17 Male 20.512 92 76 4.18 195 4962 210 1 Female 25.1964 71 70 4.19 18 5129 26 1 Male 25.0459 77 89 4.2 275 3032 525 20 Male 16.9391 79 87 4.2 261 6247 389 13 Male 42.3162 82 80 4.21 189 2124 173 30 Male 30.7625 76 106 4.23 198 5668 219 7 Male 40.9227 76 90 4.23 11 4384 35 8 Male 36.3806 76 90 4.24 307 5009 1537 7 Male 24.3806 76 112 4.26 244 5111 442 7 Male 21.6947 77 86 4.28 169 3237 189 9 Male 49.8508 79 82 4.3 297 5617 1113 17 Male 19.7864 78 87 4.3 301 6140 1077 44 Female 21.4209 65 88 4.32 327 2600 3337 9 Male 43.9398 101 84 4.33 128 5222 93 30 Male 22.5298 77 91 4.33 230 6214 318 0 Male 60.3176 78 82 4.34 103 4892 62 21 Male 22.0397 76 88 4.36 319 6140 1742 44 Female 21.4209 67 87 4.36 227 2081 185 43 Male 17.6975 77 97 4.42 206 7173 210 4 Male 24.9801 79 78 4.44 104 4962 63 1 Female 25.1964 69 67 4.44 120 4218 82 28 Male 25.9904 74 92 4.47 252 5599 443 21 Male 18.7488 78 80 4.48 106 5222 63 30 Male 22.5298 77 85 4.48 164 5818 125 14 Male 34.9268 72 91 4.49 212 6247 228 13 Male 42.3162 77 80 4.49 258 5811 431 25 Male 80.0328 78 80 4.52 222 5600 232 0 Male 48.7885 75 81 4.55 109 5534 87 14 Male 29.2621 75 82 4.55 186 5782 108 68 Female 19.6715 69 85 4.56 69 4996 51 12 Male 43.0281 77 78 4.58 64 4638 20 17 Male 20.512 82 72 4.59 32 5776 26 8 Male 17.128 71 88 4.6 302 7061 923 0 Male 36.8816 74 81 4.64 303 651 1491 21 Male 22.0068 71 94 4.67 59 4133 34 14 Male 20 70 88 4.69 117 7221 98 0 Male 63.5044 74 79 4.69 37 6859 27 1 Male 34.2122 74 79 4.69 323 3592 2569 10 Male 61.6646 76 93 4.72 179 5111 177 7 Male 21.6947 72 81 4.73 287 626 870 55 Male 19.7536 80 85 4.75 7 3807 37 5 Male 24.6762 74 77 4.76 136 6135 96 18 Male 26.5626 66 105 4.76 184 5599 148 21 Male 18.7488 72 81 4.8 26 5668 27 7 Male 40.9227 72 79 4.81 286 405 986 0 Male 21.4702 66 116 4.82 126 5111 107 7 Male 21.6947 71 80 4.82 193 4775 180 28 Male 53.5551 70 86 4.83 123 4933 134 0 Male 18.4559 69 83 4.85 87 6340 71 0 Male 19.3238 76 72 4.85 98 3645 43 45 Male 27.4935 72 90 4.86 309 5085 1512 2 Male 49.0267 75 75 4.91 118 2453 120 10 Male 37.2758 63 99 4.94 116 7173 84 4 Male 24.9801 72 75 4.94 142 6834 123 0 Male 30.7488 72 75 4.94 85 6180 59 12 Male 20.7201 67 84 4.96 148 3913 96 42 Female 23.9233 56 80 4.96 47 2600 3333 9 Male 43.9398 86 80 4.97 6 3790 13 3 Male 57.0623 76 69 4.99 113 5893 71 21 Male 22.8118 65 90 4.99 200 5893 200 21 Male 22.8118 65 89 5.01 70 5009 50 7 Male 24.3806 61 104 5.05 146 3051 131 13 Male 37.2403 68 79 5.05 231 7034 280 60 Male 23.1376 78 80 5.07 288 1075 907 42 Female 27.2772 63 64 5.12 266 1048 576 94 Male 20.115 91 96 5.14 269 1611 511 60 Male 23.2799 69 107 5.16 314 3768 1916 0 Male 19.1102 69 80 5.16 237 3226 444 0 Male 27.4552 76 64 5.21 134 5896 126 4 Female 26.8775 50 74 5.25 88 6564 69 0 Male 34.4997 67 74 5.26 218 5085 269 2 Male 49.0267 65 77 5.26 5 3728 19 6 Male 30.1273 67 73 5.3 166 6664 164 2 Male 24.7337 66 73 5.34 110 5712 88 14 Male 22.2697 70 68 5.36 138 6214 112 0 Male 60.3176 65 74 5.37 125 5085 117 2 Male 49.0267 67 71 5.38 97 3058 56 28 Male 22.2533 65 75 5.45 238 3467 333 42 Male 25.3936 68 74 5.46 49 3237 65 9 Male 49.8508 67 67 5.55 325 651 3412 21 Male 22.0068 68 92 5.62 328 3835 4933 14 Male 25.9932 91 88 5.69 4 3592 13 10 Male 61.6646 59 73 5.77 268 1493 684 60 Male 17.8042 66 75 5.83 144 2849 151 0 Male 20.0876 51 86 5.89 96 2569 49 35 Male 18.7159 50 101 5.94 232 1493 453 60 Male 17.8042 59 81 6.03 310 1939 1926 130 Male 28.2738 95 108 6.41 208 3467 186 42 Male 25.3936 53 69 6.45 167 1048 85 94 Male 20.115 63 82 6.51 271 1939 562 130 Male 28.2738 85 111 6.51 321 1939 3111 130 Male 28.2738 88 111 6.93 234 1939 295 130 Male 28.2738 67 117 7.17 326 1939 3864 130 Male 28.2738 88 105 7.22 329 2773 7631 42 Male 6.51335 88 103 7.96 311 2662 1569 180 Male 28.0821 90 101 8.24 331 5964 11038 0 Male 12.8363 71 73 11.23 330 5142 11628 57 Male 16.4326 101 95 11.25 305 2638 1093 255 Male 16.5613 78 84 11.27 257 5804 354 2 Female 42.24 122 105 nan Source \u00b6 seluruh file bisa diunduh disini atau disini . Sumber Statistik Deskriptif \u00b6 https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"3- Mengatasi Missing Values dengan K-NN"},{"location":"3knn/#missing-values","text":"Missing Values atau bisa disebut nilai yang hilang adalah kejadian yg biasa terjadi dimana nilai yg hilang dapat menandakan total yang berbeda pada sebuah data. Dan mungkin data belum tersedia atau tidak berlaku. Missing Value biasanya disebabkan oleh seseorang ketika tidak tahu atau lupa saat memasukkan data. Metode penambangan data bervariasi dalam memperlakukan nilai yang hilang. Biasanya mereka tidak peduli terhadap nilai yang hilang, atau bisa juga menggantinya dengan nilai yang sesuai dengan cara mencari nilai tengahnya.","title":"Missing Values"},{"location":"3knn/#algoritma-dalam-knn-k-nearest-neighbors","text":"Algoritma K-Nearest Naighbors atau KNN adalah sebuah algoritma klasifikasi sederhana yang bisa digunakan untuk memprediksi klasifikasi dan regresi. Tujuannya cukup sederhana yaitu untuk mengklasifikasi objek baru berdasarkan atribut dan sample data training. Penyelesaian yang bisa dilakukan oleh algoritma tersebut adalah: Tentukan jumlah tetangga terdekat yang nantinya akan dihitung. Misal tetangga yang ditentukan adalah 2 (k=2). Hitung jarak objek yg telah dipilih dengan semua tetangga yg ada. Kemudian urutkan sesuai jarak yg diperoleh dari yang terkecil hingga terbesar. Ambil 2 tetangga terdekat atau nilai jarak terkecil. Ambil rata-ratanya.","title":"Algoritma dalam KNN (K-Nearest Neighbors)"},{"location":"3knn/#mengatasi-missing-value-menggunakan-algoritma-knn-pada-python","text":"","title":"Mengatasi Missing Value Menggunakan Algoritma KNN pada Python"},{"location":"3knn/#alat-dan-bahan","text":"Pada kasus kali ini saya menggunakan datasetnya internet. Pada dataset terdapat 1 fitur bertipe binary dan 5 fitur bertipe numerikal. Pada figur umur (age) dataset telah dimodifikasi dan memberikan missing value pada baris ke-257. Untuk mempermudah dalam penyelesaiannya perlu disiapkan library python untuk memudahkan pekerjaan. Library yang dibutuhkan: pandas, manajemen dan analis data scipy, kumpulan algoritma dan fungsi matematika.","title":"Alat dan Bahan"},{"location":"3knn/#kesatu","text":"install terlebih dahulu librarynya dengan cara ketikan import pandas as pd import math as mt from sklearn.preprocessing import LabelEncoder","title":"Kesatu"},{"location":"3knn/#kedua","text":"selanjutnya memuat data, pada kasus ini saya menggunakan file excel data = pd . read_csv ( 'Wong.csv' , delimiter = ';' , decimal = ',' ) df = pd . DataFrame ( data ) df . style . highlight_null ( null_color = 'red' ) . hide_index () maka akan tampil seperti di bawah: no id days duration sex age piq viq 1 3358 30 4 Male 20.6708 87 89 2 3535 16 17 Male 55.2882 95 77 3 3547 40 1 Male 55.9151 95 116 4 3592 13 10 Male 61.6646 59 73 5 3728 19 6 Male 30.1273 67 73 6 3790 13 3 Male 57.0623 76 69 7 3807 37 5 Male 24.6762 74 77 8 3808 31 7 Male 28.2683 91 110 9 4253 40 3 Male 22.6037 115 110 10 4356 31 7 Male 21.399 86 83 11 4384 35 8 Male 36.3806 76 90 12 4542 22 11 Female 21.9576 71 89 13 4705 18 1 Female 21.6838 127 109 14 4744 15 25 Male 57.566 82 85 15 4802 36 0 Male 62.475 88 97 16 4941 46 4 Female 19.0144 69 88 17 4983 33 5 Male 38.3929 102 117 18 5129 26 1 Male 25.0459 77 89 19 5154 35 5 Male 22.1903 82 95 20 5162 33 1 Male 25.0185 118 101 21 5174 38 4 Female 37.2704 87 99 22 5208 31 8 Female 21.3771 97 90 23 5253 29 1 Male 33.1335 104 105 24 5298 30 3 Male 22.9569 87 86 25 5640 34 7 Male 25.9986 93 113 26 5668 27 7 Male 40.9227 72 79 27 5680 17 1 Male 27.7563 84 90 28 5699 26 1 Female 34.2231 95 108 29 5713 36 8 Male 16.2683 89 97 30 5736 18 9 Male 16.1478 89 86 31 5754 36 1 Male 16.3368 87 86 32 5776 26 8 Male 17.128 71 88 33 6122 29 1 Male 56.2108 95 103 34 6163 21 1 Male 19.3593 112 106 35 6179 22 2 Male 38.0123 89 95 36 6671 30 7 Female 27.8056 71 82 37 6859 27 1 Male 34.2122 74 79 38 6870 22 0 Male 42.4832 84 95 39 6914 43 0 Male 61.5222 85 90 40 6937 18 0 Female 21.191 94 81 41 6977 30 1 Male 36.2108 97 94 42 7120 39 0 Male 69.7057 84 86 43 7309 31 0 Female 50.6667 85 95 44 7321 23 0 Male 26.0041 84 83 45 7548 31 0 Male 24.3669 108 106 46 2364 41 14 Male 25.8097 84 94 47 2600 3333 9 Male 43.9398 86 80 48 2761 40 3 Female 24.3696 98 112 49 3237 65 9 Male 49.8508 67 67 50 3277 51 1 Male 37.4702 104 96 51 3346 44 18 Female 57.2758 79 85 52 3359 59 9 Female 56.8953 84 91 53 3373 39 28 Female 26.308 87 91 54 3544 32 14 Male 54.5298 81 98 55 3655 57 5 Female 21.9055 90 103 56 3762 48 6 Male 20.3559 85 93 57 3919 58 1 Male 30.3655 99 95 58 4094 50 2 Male 19.7262 79 93 59 4133 34 14 Male 20 70 88 60 4183 42 3 Male 26.2341 98 116 61 4189 69 4 Female 29.462 75 86 62 4315 63 0 Male 38.141 107 130 63 4482 58 14 Female 18.2341 86 103 64 4638 20 17 Male 20.512 82 72 65 4678 63 7 Male 46.6448 96 95 66 4696 54 4 Male 46.9569 101 112 67 4755 24 18 Male 27.5127 105 102 68 4837 42 10 Male 19.6906 83 88 69 4996 51 12 Male 43.0281 77 78 70 5009 50 7 Male 24.3806 61 104 71 5014 46 7 Female 23.7618 75 90 72 5192 60 1 Male 58.6283 87 97 73 5204 71 0 Male 59.0746 97 107 74 5238 44 3 Male 45.1006 99 103 75 5280 83 1 Male 48.6434 78 88 76 5289 52 1 Male 48.5722 84 85 77 5456 48 14 Male 41.1636 80 101 78 5458 44 14 Male 34.4778 84 95 79 5474 65 2 Female 28.6598 95 86 80 5568 64 1 Female 51.9918 75 79 81 5580 56 7 Male 17.7933 86 95 82 5581 65 2 Male 26.3053 85 95 83 5628 51 3 Female 30.2642 81 85 84 6154 43 5 Female 22.6064 74 80 85 6180 59 12 Male 20.7201 67 84 86 6314 58 3 Male 16.6927 80 99 87 6340 71 0 Male 19.3238 76 72 88 6564 69 0 Male 34.4997 67 74 89 6614 57 0 Male 45.1116 80 101 90 6686 44 14 Female 38.3491 90 100 91 6795 55 0 Male 30.7159 87 104 92 7080 64 5 Female 76.6598 76 106 93 7084 54 2 Male 36.5722 87 93 94 7271 55 0 Male 41.7659 100 95 95 7371 55 1 Male 56.7858 80 88 96 2569 49 35 Male 18.7159 50 101 97 3058 56 28 Male 22.2533 65 75 98 3645 43 45 Male 27.4935 72 90 99 3844 73 9 Male 26.1164 79 94 100 4725 124 10 Male 32.9172 93 97 101 4744 65 25 Male 57.566 105 119 102 4807 64 14 Female 47.7974 74 74 103 4892 62 21 Male 22.0397 76 88 104 4962 63 1 Female 25.1964 69 67 105 5125 78 12 Male 17.5387 94 118 106 5222 63 30 Male 22.5298 77 85 107 5253 86 1 Male 33.1335 106 128 108 5386 78 21 Male 20.8761 78 93 109 5534 87 14 Male 29.2621 75 82 110 5712 88 14 Male 22.2697 70 68 111 5837 82 1 Female 33.3087 82 110 112 5879 75 21 Male 25.8453 80 105 113 5893 71 21 Male 22.8118 65 90 114 5916 84 0 Female 26.8556 93 73 115 6410 80 14 Male 32.1725 85 98 116 7173 84 4 Male 24.9801 72 75 117 7221 98 0 Male 63.5044 74 79 118 2453 120 10 Male 37.2758 63 99 119 2653 97 28 Male 30.0068 93 112 120 4218 82 28 Male 25.9904 74 92 121 4542 121 11 Female 21.9576 86 114 122 4902 102 8 Male 16.1424 87 77 123 4933 134 0 Male 18.4559 69 83 124 4941 131 4 Female 19.0144 96 96 125 5085 117 2 Male 49.0267 67 71 126 5111 107 7 Male 21.6947 71 80 127 5154 120 5 Male 22.1903 89 109 128 5222 93 30 Male 22.5298 77 91 129 5298 107 3 Male 22.9569 117 112 130 5339 119 7 Male 21.8152 87 82 131 5387 109 12 Male 21.7988 85 112 132 5414 105 10 Female 40.2765 93 104 133 5494 111 7 Male 54.6913 86 86 134 5896 126 4 Female 26.8775 50 74 135 5901 115 7 Male 22.1739 112 116 136 6135 96 18 Male 26.5626 66 105 137 6173 125 4 Male 35.3046 94 97 138 6214 112 0 Male 60.3176 65 74 139 6253 128 0 Female 46.4038 104 112 140 6433 120 4 Male 23.8604 100 103 141 6665 119 3 Female 23.0171 106 94 142 6834 123 0 Male 30.7488 72 75 143 1176 146 17 Female 19.729 65 98 144 2849 151 0 Male 20.0876 51 86 145 2882 141 18 Male 19.2334 84 85 146 3051 131 13 Male 37.2403 68 79 147 3728 151 6 Male 30.1273 96 105 148 3913 96 42 Female 23.9233 56 80 149 4133 133 14 Male 20 82 94 150 4661 135 17 Female 30.8419 84 93 151 4678 143 7 Male 46.6448 98 107 152 4696 150 4 Male 46.9569 120 120 153 4705 146 1 Female 21.6838 133 111 154 4802 142 0 Male 62.475 101 117 155 4807 139 14 Female 47.7974 80 78 156 4983 146 5 Male 38.3929 107 123 157 5014 151 7 Female 23.7618 97 110 158 5162 144 1 Male 25.0185 130 118 159 5238 150 3 Male 45.1006 117 126 160 5642 162 0 Male 65.87 89 103 161 5699 138 1 Female 34.2231 110 107 162 5713 144 8 Male 16.2683 100 99 163 5804 159 2 Female 28.8515 102 107 164 5818 125 14 Male 34.9268 72 91 165 6314 140 3 Male 16.6927 87 96 166 6664 164 2 Male 24.7337 66 73 167 1048 85 94 Male 20.115 63 82 168 1085 159 11 Male 30.7105 103 97 169 3237 189 9 Male 49.8508 79 82 170 3358 175 4 Male 20.6708 97 97 171 3808 165 7 Male 28.2683 94 111 172 4094 177 2 Male 19.7262 89 102 173 4253 175 3 Male 22.6037 114 118 174 4638 140 17 Male 20.512 89 78 175 4755 128 18 Male 27.5127 105 109 176 4865 142 35 Male 58.3354 84 103 177 4892 148 21 Male 22.0397 106 110 178 5009 174 7 Male 24.3806 77 103 179 5111 177 7 Male 21.6947 72 81 180 5125 173 12 Male 17.5387 106 119 181 5192 179 1 Male 58.6283 93 105 182 5505 171 1 Male 65.4784 95 93 183 5581 176 2 Male 26.3053 96 110 184 5599 148 21 Male 18.7488 72 81 185 5680 184 1 Male 27.7563 84 90 186 5782 108 68 Female 19.6715 69 85 187 6180 177 12 Male 20.7201 81 94 188 6671 184 7 Female 27.8056 91 92 189 2124 173 30 Male 30.7625 76 106 190 2646 187 14 Male 22.9158 97 97 191 2790 211 0 Male 48.8049 89 99 192 4189 202 4 Female 29.462 81 90 193 4775 180 28 Male 53.5551 70 86 194 4933 226 0 Male 18.4559 79 86 195 4962 210 1 Female 25.1964 71 70 196 5208 193 8 Female 21.3771 133 111 197 5456 193 14 Male 41.1636 87 110 198 5668 219 7 Male 40.9227 76 90 199 5712 192 14 Male 22.2697 87 85 200 5893 200 21 Male 22.8118 65 89 201 5916 205 0 Female 26.8556 92 76 202 6122 212 1 Male 56.2108 109 117 203 6136 216 1 Male 32.7912 92 89 204 6175 278 1 Male 51.1704 99 98 205 6228 174 3 Female 31.5537 114 108 206 7173 210 4 Male 24.9801 79 78 207 1176 216 17 Female 19.729 74 100 208 3467 186 42 Male 25.3936 53 69 209 4744 217 25 Male 57.566 108 118 210 5386 241 21 Male 20.8761 80 94 211 5837 242 1 Female 33.3087 93 105 212 6247 228 13 Male 42.3162 77 80 213 1892 276 2 Male 21.7796 87 107 214 2882 262 18 Male 19.2334 94 90 215 3058 236 28 Male 22.2533 85 88 216 4342 263 1 Male 44.063 79 91 217 4865 240 35 Male 58.3354 93 105 218 5085 269 2 Male 49.0267 65 77 219 5222 247 30 Male 22.5298 88 85 220 5339 271 7 Male 21.8152 94 89 221 5474 280 2 Female 28.6598 99 91 222 5600 232 0 Male 48.7885 75 81 223 2826 290 14 Male 23.2334 94 108 224 4725 286 10 Male 32.9172 105 94 225 5204 299 0 Male 59.0746 99 105 226 6498 270 28 Male 24.0767 82 101 227 2081 185 43 Male 17.6975 77 97 228 4678 340 7 Male 46.6448 108 119 229 5397 328 0 Female 62.7981 121 108 230 6214 318 0 Male 60.3176 78 82 231 7034 280 60 Male 23.1376 78 80 232 1493 453 60 Male 17.8042 59 81 233 1836 375 1 Male 47.0554 101 108 234 1939 295 130 Male 28.2738 67 117 235 2646 438 14 Male 22.9158 98 94 236 2653 352 28 Male 30.0068 105 126 237 3226 444 0 Male 27.4552 76 64 238 3467 333 42 Male 25.3936 68 74 239 4342 432 1 Male 44.063 92 107 240 4542 431 11 Female 21.9576 98 114 241 4661 374 17 Female 30.8419 93 95 242 4902 397 8 Male 16.1424 92 86 243 4983 398 5 Male 38.3929 121 132 244 5111 442 7 Male 21.6947 77 86 245 5125 510 12 Male 17.5387 112 125 246 5289 417 1 Male 48.5722 83 83 247 5386 436 21 Male 20.8761 90 103 248 5387 480 12 Male 21.7988 94 116 249 5505 527 1 Male 65.4784 104 87 250 5580 369 7 Male 17.7933 96 107 251 5581 378 2 Male 26.3053 95 95 252 5599 443 21 Male 18.7488 78 80 253 5668 390 7 Male 40.9227 92 92 254 5680 403 1 Male 27.7563 94 93 255 5712 365 14 Male 22.2697 98 86 256 5772 412 35 Male 26.2587 102 104 257 5804 354 2 Female nan 122 105 258 5811 431 25 Male 80.0328 78 80 259 5841 415 8 Male 27.2279 82 83 260 6226 438 0 Male 36.8022 84 92 261 6247 389 13 Male 42.3162 82 80 262 6468 513 60 Male 43.4798 99 94 263 6614 362 0 Male 45.1116 88 106 264 6665 368 3 Female 23.0171 100 92 265 781 714 15 Male 29.8699 85 85 266 1048 576 94 Male 20.115 91 96 267 1157 810 23 Male 17.3881 97 84 268 1493 684 60 Male 17.8042 66 75 269 1611 511 60 Male 23.2799 69 107 270 1624 604 1 Male 19.5619 97 85 271 1939 562 130 Male 28.2738 85 111 272 2498 615 0 Female 17.4292 86 113 273 2826 636 14 Male 23.2334 111 101 274 2849 642 0 Male 20.0876 76 98 275 3032 525 20 Male 16.9391 79 87 276 3226 683 0 Male 27.4552 89 78 277 4218 814 28 Male 25.9904 99 96 278 4807 532 14 Female 47.7974 84 82 279 5014 637 7 Female 23.7618 101 114 280 5222 690 30 Male 22.5298 81 90 281 5253 591 1 Male 33.1335 114 124 282 5628 609 3 Female 30.2642 89 78 283 6059 794 1 Female 16.9801 71 76 284 6228 662 3 Female 31.5537 128 111 285 6247 616 13 Male 42.3162 85 82 286 405 986 0 Male 21.4702 66 116 287 626 870 55 Male 19.7536 80 85 288 1075 907 42 Female 27.2772 63 64 289 2849 1040 0 Male 20.0876 91 103 290 3032 884 20 Male 16.9391 87 93 291 3226 1123 0 Male 27.4552 88 81 292 4864 936 0 Female 53.9767 119 131 293 5474 1100 2 Female 28.6598 94 88 294 5568 1114 1 Female 51.9918 81 82 295 5580 1087 7 Male 17.7933 106 98 296 5581 1113 2 Male 26.3053 99 96 297 5617 1113 17 Male 19.7864 78 87 298 5642 1143 0 Male 65.87 104 109 299 5713 1016 8 Male 16.2683 126 106 300 5837 962 1 Female 33.3087 109 110 301 6140 1077 44 Female 21.4209 65 88 302 7061 923 0 Male 36.8816 74 81 303 651 1491 21 Male 22.0068 71 94 304 2527 1294 0 Male 16.9172 93 104 305 2638 1093 255 Male 16.5613 78 84 306 4865 1363 35 Male 58.3354 88 104 307 5009 1537 7 Male 24.3806 76 112 308 5014 1523 7 Female 23.7618 105 114 309 5085 1512 2 Male 49.0267 75 75 310 1939 1926 130 Male 28.2738 95 108 311 2662 1569 180 Male 28.0821 90 101 312 2826 1809 14 Male 23.2334 104 108 313 2882 1716 18 Male 19.2334 100 103 314 3768 1916 0 Male 19.1102 69 80 315 4356 2000 7 Male 21.399 104 91 316 4638 1779 17 Male 20.512 92 76 317 4696 1769 4 Male 46.9569 105 124 318 4744 1743 25 Male 57.566 97 118 319 6140 1742 44 Female 21.4209 67 87 320 1075 2259 42 Female 27.2772 78 79 321 1939 3111 130 Male 28.2738 88 111 322 2653 2191 28 Male 30.0068 117 129 323 3592 2569 10 Male 61.6646 76 93 324 3808 2434 7 Male 28.2683 105 111 325 651 3412 21 Male 22.0068 68 92 326 1939 3864 130 Male 28.2738 88 105 327 2600 3337 9 Male 43.9398 101 84 328 3835 4933 14 Male 25.9932 91 88 329 2773 7631 42 Male 6.51335 88 103 330 5142 11628 57 Male 16.4326 101 95 331 5964 11038 0 Male 12.8363 71 73","title":"Kedua"},{"location":"3knn/#ketiga","text":"Ubah nilai fitur sex menjadi angka 0 atau 1 dg memakai fungsi dari sklearn, LabelEncode() #encode fitur tipe biner X = data . iloc [:,:] . values labelEncode_X = LabelEncoder () X [:, 4 ] = labelEncode_X . fit_transform ( X [:, 4 ])","title":"Ketiga"},{"location":"3knn/#keempat","text":"Implementasikan rumus jarak ke dalam bentuk fungsi python. yaitu: eulidianDistance() dengan fungsi jarak tipe binary distanceSimetris (). def Zscore ( x , mean , std ): top = x - mean if top == 0 : return top else : return round ( top / std , 2 ) def normalisasi ( num , col_x ): return Zscore ( num , pd . Series ( data [ col_x ] . values ) . mean (), pd . Series ( data [ col_x ] . values ) . std ()) #menghitung jarak tipe numerikal def euclidianDistance ( x , y ): dis = 0 for i in range ( len ( x )): dis += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dis ), 2 ) #Menghitung jarak tipe binary def distanceSimetris ( x , y ): q = r = s = t = 0 for i in range ( len ( x )): if x [ i ] == 1 and y [ i ] == 1 : q += 1 elif x [ i ] == 1 and y [ i ] == 0 : r += 1 elif x [ i ] == 0 and y [ i ] == 1 : s += 1 elif x [ i ] == 0 and y [ i ] == 0 : t += 1 return (( r + s ) / ( q + r + s + t ))","title":"Keempat"},{"location":"3knn/#kelima","text":"Cek baris missing value: c_j = 0 for j in df [ 'age' ] . isna (): if j == True : col_missing = c_j c_j += 1","title":"Kelima"},{"location":"3knn/#keenam","text":"Hitung jarak pada missing data dengan semua tetangganya dan tampung pada dictionary: missing_data = df . iloc [ col_missing , [ 2 , 3 , 6 , 7 ]] . values missing_normal = [ normalisasi ( missing_data [ 0 ], data . columns [ 2 ]), normalisasi ( missing_data [ 1 ], data . columns [ 3 ]), normalisasi ( missing_data [ 2 ], data . columns [ 6 ]), normalisasi ( missing_data [ 3 ], data . columns [ 7 ])] for i in range ( len ( data [ data . columns [ 0 ]])): if i == col_missing : continue ; select_data = df . iloc [ i , [ 2 , 3 , 6 , 7 ]] . values normal_data = [ normalisasi ( select_data [ 0 ], data . columns [ 2 ]), normalisasi ( select_data [ 1 ], data . columns [ 3 ]), normalisasi ( select_data [ 2 ], data . columns [ 6 ]), normalisasi ( select_data [ 3 ], data . columns [ 7 ])] data . loc [ i , 'jarak' ] = euclidianDistance ( missing_normal , normal_data ) + distanceSimetris ([ X [ col_missing , 4 ]],[ X [ i , 4 ]])","title":"Keenam"},{"location":"3knn/#ketujuh","text":"Urutkan data berdasarkan jarak mulai terkecil sampai terbesar. Isi data yang hilang dengan mengambil rata-rata dari 2 tetangga terdekat. df = pd . DataFrame ( data ) df . sort_values ( by = 'jarak' , axis = 0 , ascending = True , inplace = True ) df . iloc [ - 1 , [ 5 ]] = round ( df . iloc [ 0 : 2 , 5 ] . mean (), 2 ) df . style . hide_index () Berikut tampilan data yang telah diurutkan. Pada baris terakhir terlihat kolom age sudah terisi dengan angka 42.24 diperoleh dari rata-rata 2 tetangga terdekat. duration sex age piq viq jarak 229 5397 328 0 Female 62.7981 121 108 0.25 13 4705 18 1 Female 21.6838 127 109 0.53 205 6228 174 3 Female 31.5537 114 108 0.6 284 6228 662 3 Female 31.5537 128 111 0.64 161 5699 138 1 Female 34.2231 110 107 0.84 153 4705 146 1 Female 21.6838 133 111 0.86 196 5208 193 8 Female 21.3771 133 111 0.88 300 5837 962 1 Female 33.3087 109 110 1.07 139 6253 128 0 Female 46.4038 104 112 1.31 141 6665 119 3 Female 23.0171 106 94 1.33 163 5804 159 2 Female 28.8515 102 107 1.35 20 5162 33 1 Male 25.0185 118 101 1.48 279 5014 637 7 Female 23.7618 101 114 1.56 129 5298 107 3 Male 22.9569 117 112 1.64 9 4253 40 3 Male 22.6037 115 110 1.66 308 5014 1523 7 Female 23.7618 105 114 1.67 299 5713 1016 8 Male 16.2683 126 106 1.68 48 2761 40 3 Female 24.3696 98 112 1.69 157 5014 151 7 Female 23.7618 97 110 1.72 264 6665 368 3 Female 23.0171 100 92 1.73 34 6163 21 1 Male 19.3593 112 106 1.73 240 4542 431 11 Female 21.9576 98 114 1.75 221 5474 280 2 Female 28.6598 99 91 1.82 28 5699 26 1 Female 34.2231 95 108 1.83 124 4941 131 4 Female 19.0144 96 96 1.85 211 5837 242 1 Female 33.3087 93 105 1.92 292 4864 936 0 Female 53.9767 119 131 1.93 273 2826 636 14 Male 23.2334 111 101 1.94 132 5414 105 10 Female 40.2765 93 104 1.96 45 7548 31 0 Male 24.3669 108 106 1.98 22 5208 31 8 Female 21.3771 97 90 2 135 5901 115 7 Male 22.1739 112 116 2.07 173 4253 175 3 Male 22.6037 114 118 2.08 158 5162 144 1 Male 25.0185 130 118 2.09 152 4696 150 4 Male 46.9569 120 120 2.1 241 4661 374 17 Female 30.8419 93 95 2.13 55 3655 57 5 Female 21.9055 90 103 2.14 202 6122 212 1 Male 56.2108 109 117 2.22 90 6686 44 14 Female 38.3491 90 100 2.22 23 5253 29 1 Male 33.1335 104 105 2.23 79 5474 65 2 Female 28.6598 95 86 2.26 188 6671 184 7 Female 27.8056 91 92 2.26 293 5474 1100 2 Female 28.6598 94 88 2.3 67 4755 24 18 Male 27.5127 105 102 2.33 175 4755 128 18 Male 27.5127 105 109 2.33 177 4892 148 21 Male 22.0397 106 110 2.35 295 5580 1087 7 Male 17.7933 106 98 2.35 50 3277 51 1 Male 37.4702 104 96 2.38 21 5174 38 4 Female 37.2704 87 99 2.38 228 4678 340 7 Male 46.6448 108 119 2.38 298 5642 1143 0 Male 65.87 104 109 2.41 224 4725 286 10 Male 32.9172 105 94 2.41 233 1836 375 1 Male 47.0554 101 108 2.41 168 1085 159 11 Male 30.7105 103 97 2.43 63 4482 58 14 Female 18.2341 86 103 2.44 272 2498 615 0 Female 17.4292 86 113 2.46 140 6433 120 4 Male 23.8604 100 103 2.48 281 5253 591 1 Male 33.1335 114 124 2.48 121 4542 121 11 Female 21.9576 86 114 2.5 66 4696 54 4 Male 46.9569 101 112 2.5 180 5125 173 12 Male 17.5387 106 119 2.51 225 5204 299 0 Male 59.0746 99 105 2.52 40 6937 18 0 Female 21.191 94 81 2.53 162 5713 144 8 Male 16.2683 100 99 2.55 159 5238 150 3 Male 45.1006 117 126 2.55 74 5238 44 3 Male 45.1006 99 103 2.55 43 7309 31 0 Female 50.6667 85 95 2.57 209 4744 217 25 Male 57.566 108 118 2.59 204 6175 278 1 Male 51.1704 99 98 2.6 17 4983 33 5 Male 38.3929 102 117 2.61 151 4678 143 7 Male 46.6448 98 107 2.62 245 5125 510 12 Male 17.5387 112 125 2.63 156 4983 146 5 Male 38.3929 107 123 2.64 94 7271 55 0 Male 41.7659 100 95 2.65 154 4802 142 0 Male 62.475 101 117 2.65 111 5837 82 1 Female 33.3087 82 110 2.69 73 5204 71 0 Male 59.0746 97 107 2.69 57 3919 58 1 Male 30.3655 99 95 2.7 150 4661 135 17 Female 30.8419 84 93 2.72 52 3359 59 9 Female 56.8953 84 91 2.72 53 3373 39 28 Female 26.308 87 91 2.73 250 5580 369 7 Male 17.7933 96 107 2.74 147 3728 151 6 Male 30.1273 96 105 2.74 249 5505 527 1 Male 65.4784 104 87 2.75 183 5581 176 2 Male 26.3053 96 110 2.76 170 3358 175 4 Male 20.6708 97 97 2.76 101 4744 65 25 Male 57.566 105 119 2.77 296 5581 1113 2 Male 26.3053 99 96 2.78 60 4183 42 3 Male 26.2341 98 116 2.8 312 2826 1809 14 Male 23.2334 104 108 2.82 190 2646 187 14 Male 22.9158 97 97 2.82 33 6122 29 1 Male 56.2108 95 103 2.82 235 2646 438 14 Male 22.9158 98 94 2.83 256 5772 412 35 Male 26.2587 102 104 2.84 41 6977 30 1 Male 36.2108 97 94 2.86 201 5916 205 0 Female 26.8556 92 76 2.87 65 4678 63 7 Male 46.6448 96 95 2.89 282 5628 609 3 Female 30.2642 89 78 2.91 192 4189 202 4 Female 29.462 81 90 2.91 223 2826 290 14 Male 23.2334 94 108 2.92 171 3808 165 7 Male 28.2683 94 111 2.92 251 5581 378 2 Male 26.3053 95 95 2.93 243 4983 398 5 Male 38.3929 121 132 2.93 181 5192 179 1 Male 58.6283 93 105 2.93 137 6173 125 4 Male 35.3046 94 97 2.95 107 5253 86 1 Male 33.1335 106 128 2.97 277 4218 814 28 Male 25.9904 99 96 2.97 114 5916 84 0 Female 26.8556 93 73 2.98 3 3547 40 1 Male 55.9151 95 116 2.98 313 2882 1716 18 Male 19.2334 100 103 2.99 182 5505 171 1 Male 65.4784 95 93 2.99 239 4342 432 1 Male 44.063 92 107 3 278 4807 532 14 Female 47.7974 84 82 3.03 25 5640 34 7 Male 25.9986 93 113 3.03 100 4725 124 10 Male 32.9172 93 97 3.04 254 5680 403 1 Male 27.7563 94 93 3.04 248 5387 480 12 Male 21.7988 94 116 3.05 62 4315 63 0 Male 38.141 107 130 3.05 92 7080 64 5 Female 76.6598 76 106 3.05 83 5628 51 3 Female 30.2642 81 85 3.07 304 2527 1294 0 Male 16.9172 93 104 3.09 8 3808 31 7 Male 28.2683 91 110 3.11 105 5125 78 12 Male 17.5387 94 118 3.12 236 2653 352 28 Male 30.0068 105 126 3.13 315 4356 2000 7 Male 21.399 104 91 3.13 289 2849 1040 0 Male 20.0876 91 103 3.14 255 5712 365 14 Male 22.2697 98 86 3.14 317 4696 1769 4 Male 46.9569 105 124 3.16 220 5339 271 7 Male 21.8152 94 89 3.18 160 5642 162 0 Male 65.87 89 103 3.19 324 3808 2434 7 Male 28.2683 105 111 3.2 172 4094 177 2 Male 19.7262 89 102 3.2 270 1624 604 1 Male 19.5619 97 85 3.2 253 5668 390 7 Male 40.9227 92 92 3.2 127 5154 120 5 Male 22.1903 89 109 3.21 214 2882 262 18 Male 19.2334 94 90 3.22 119 2653 97 28 Male 30.0068 93 112 3.23 191 2790 211 0 Male 48.8049 89 99 3.23 294 5568 1114 1 Female 51.9918 81 82 3.23 51 3346 44 18 Female 57.2758 79 85 3.25 263 6614 362 0 Male 45.1116 88 106 3.25 207 1176 216 17 Female 19.729 74 100 3.25 247 5386 436 21 Male 20.8761 90 103 3.25 29 5713 36 8 Male 16.2683 89 97 3.28 203 6136 216 1 Male 32.7912 92 89 3.29 217 4865 240 35 Male 58.3354 93 105 3.3 71 5014 46 7 Female 23.7618 75 90 3.3 35 6179 22 2 Male 38.0123 89 95 3.31 213 1892 276 2 Male 21.7796 87 107 3.33 91 6795 55 0 Male 30.7159 87 104 3.34 15 4802 36 0 Male 62.475 88 97 3.34 72 5192 60 1 Male 58.6283 87 97 3.4 61 4189 69 4 Female 29.462 75 86 3.4 267 1157 810 23 Male 17.3881 97 84 3.4 197 5456 193 14 Male 41.1636 87 110 3.4 165 6314 140 3 Male 16.6927 87 96 3.41 242 4902 397 8 Male 16.1424 92 86 3.42 155 4807 139 14 Female 47.7974 80 78 3.42 318 4744 1743 25 Male 57.566 97 118 3.43 93 7084 54 2 Male 36.5722 87 93 3.49 81 5580 56 7 Male 17.7933 86 95 3.5 131 5387 109 12 Male 21.7988 85 112 3.54 115 6410 80 14 Male 32.1725 85 98 3.55 82 5581 65 2 Male 26.3053 85 95 3.56 322 2653 2191 28 Male 30.0068 117 129 3.57 12 4542 22 11 Female 21.9576 71 89 3.58 1 3358 30 4 Male 20.6708 87 89 3.6 30 5736 18 9 Male 16.1478 89 86 3.6 290 3032 884 20 Male 16.9391 87 93 3.61 56 3762 48 6 Male 20.3559 85 93 3.61 80 5568 64 1 Female 51.9918 75 79 3.63 38 6870 22 0 Male 42.4832 84 95 3.63 84 6154 43 5 Female 22.6064 74 80 3.65 78 5458 44 14 Male 34.4778 84 95 3.66 46 2364 41 14 Male 25.8097 84 94 3.68 260 6226 438 0 Male 36.8022 84 92 3.68 39 6914 43 0 Male 61.5222 85 90 3.69 24 5298 30 3 Male 22.9569 87 86 3.7 31 5754 36 1 Male 16.3368 87 86 3.7 16 4941 46 4 Female 19.0144 69 88 3.72 185 5680 184 1 Male 27.7563 84 90 3.73 306 4865 1363 35 Male 58.3354 88 104 3.73 27 5680 17 1 Male 27.7563 84 90 3.74 2 3535 16 17 Male 55.2882 95 77 3.75 133 5494 111 7 Male 54.6913 86 86 3.75 36 6671 30 7 Female 27.8056 71 82 3.76 19 5154 35 5 Male 22.1903 82 95 3.76 199 5712 192 14 Male 22.2697 87 85 3.76 262 6468 513 60 Male 43.4798 99 94 3.81 149 4133 133 14 Male 20 82 94 3.81 89 6614 57 0 Male 45.1116 80 101 3.81 54 3544 32 14 Male 54.5298 81 98 3.81 176 4865 142 35 Male 58.3354 84 103 3.82 86 6314 58 3 Male 16.6927 80 99 3.82 77 5456 48 14 Male 41.1636 80 101 3.84 130 5339 119 7 Male 21.8152 87 82 3.85 187 6180 177 12 Male 20.7201 81 94 3.85 226 6498 270 28 Male 24.0767 82 101 3.85 143 1176 146 17 Female 19.729 65 98 3.85 42 7120 39 0 Male 69.7057 84 86 3.86 219 5222 247 30 Male 22.5298 88 85 3.87 10 4356 31 7 Male 21.399 86 83 3.87 68 4837 42 10 Male 19.6906 83 88 3.88 112 5879 75 21 Male 25.8453 80 105 3.89 265 781 714 15 Male 29.8699 85 85 3.89 76 5289 52 1 Male 48.5722 84 85 3.9 102 4807 64 14 Female 47.7974 74 74 3.9 291 3226 1123 0 Male 27.4552 88 81 3.9 215 3058 236 28 Male 22.2533 85 88 3.91 276 3226 683 0 Male 27.4552 89 78 3.92 145 2882 141 18 Male 19.2334 84 85 3.95 44 7321 23 0 Male 26.0041 84 83 3.97 283 6059 794 1 Female 16.9801 71 76 3.97 174 4638 140 17 Male 20.512 89 78 3.97 210 5386 241 21 Male 20.8761 80 94 3.98 285 6247 616 13 Male 42.3162 85 82 3.98 99 3844 73 9 Male 26.1164 79 94 3.98 178 5009 174 7 Male 24.3806 77 103 3.99 58 4094 50 2 Male 19.7262 79 93 3.99 246 5289 417 1 Male 48.5722 83 83 4.02 216 4342 263 1 Male 44.063 79 91 4.02 95 7371 55 1 Male 56.7858 80 88 4.04 122 4902 102 8 Male 16.1424 87 77 4.07 259 5841 415 8 Male 27.2279 82 83 4.08 274 2849 642 0 Male 20.0876 76 98 4.09 280 5222 690 30 Male 22.5298 81 90 4.11 320 1075 2259 42 Female 27.2772 78 79 4.13 108 5386 78 21 Male 20.8761 78 93 4.13 14 4744 15 25 Male 57.566 82 85 4.15 75 5280 83 1 Male 48.6434 78 88 4.16 194 4933 226 0 Male 18.4559 79 86 4.16 316 4638 1779 17 Male 20.512 92 76 4.18 195 4962 210 1 Female 25.1964 71 70 4.19 18 5129 26 1 Male 25.0459 77 89 4.2 275 3032 525 20 Male 16.9391 79 87 4.2 261 6247 389 13 Male 42.3162 82 80 4.21 189 2124 173 30 Male 30.7625 76 106 4.23 198 5668 219 7 Male 40.9227 76 90 4.23 11 4384 35 8 Male 36.3806 76 90 4.24 307 5009 1537 7 Male 24.3806 76 112 4.26 244 5111 442 7 Male 21.6947 77 86 4.28 169 3237 189 9 Male 49.8508 79 82 4.3 297 5617 1113 17 Male 19.7864 78 87 4.3 301 6140 1077 44 Female 21.4209 65 88 4.32 327 2600 3337 9 Male 43.9398 101 84 4.33 128 5222 93 30 Male 22.5298 77 91 4.33 230 6214 318 0 Male 60.3176 78 82 4.34 103 4892 62 21 Male 22.0397 76 88 4.36 319 6140 1742 44 Female 21.4209 67 87 4.36 227 2081 185 43 Male 17.6975 77 97 4.42 206 7173 210 4 Male 24.9801 79 78 4.44 104 4962 63 1 Female 25.1964 69 67 4.44 120 4218 82 28 Male 25.9904 74 92 4.47 252 5599 443 21 Male 18.7488 78 80 4.48 106 5222 63 30 Male 22.5298 77 85 4.48 164 5818 125 14 Male 34.9268 72 91 4.49 212 6247 228 13 Male 42.3162 77 80 4.49 258 5811 431 25 Male 80.0328 78 80 4.52 222 5600 232 0 Male 48.7885 75 81 4.55 109 5534 87 14 Male 29.2621 75 82 4.55 186 5782 108 68 Female 19.6715 69 85 4.56 69 4996 51 12 Male 43.0281 77 78 4.58 64 4638 20 17 Male 20.512 82 72 4.59 32 5776 26 8 Male 17.128 71 88 4.6 302 7061 923 0 Male 36.8816 74 81 4.64 303 651 1491 21 Male 22.0068 71 94 4.67 59 4133 34 14 Male 20 70 88 4.69 117 7221 98 0 Male 63.5044 74 79 4.69 37 6859 27 1 Male 34.2122 74 79 4.69 323 3592 2569 10 Male 61.6646 76 93 4.72 179 5111 177 7 Male 21.6947 72 81 4.73 287 626 870 55 Male 19.7536 80 85 4.75 7 3807 37 5 Male 24.6762 74 77 4.76 136 6135 96 18 Male 26.5626 66 105 4.76 184 5599 148 21 Male 18.7488 72 81 4.8 26 5668 27 7 Male 40.9227 72 79 4.81 286 405 986 0 Male 21.4702 66 116 4.82 126 5111 107 7 Male 21.6947 71 80 4.82 193 4775 180 28 Male 53.5551 70 86 4.83 123 4933 134 0 Male 18.4559 69 83 4.85 87 6340 71 0 Male 19.3238 76 72 4.85 98 3645 43 45 Male 27.4935 72 90 4.86 309 5085 1512 2 Male 49.0267 75 75 4.91 118 2453 120 10 Male 37.2758 63 99 4.94 116 7173 84 4 Male 24.9801 72 75 4.94 142 6834 123 0 Male 30.7488 72 75 4.94 85 6180 59 12 Male 20.7201 67 84 4.96 148 3913 96 42 Female 23.9233 56 80 4.96 47 2600 3333 9 Male 43.9398 86 80 4.97 6 3790 13 3 Male 57.0623 76 69 4.99 113 5893 71 21 Male 22.8118 65 90 4.99 200 5893 200 21 Male 22.8118 65 89 5.01 70 5009 50 7 Male 24.3806 61 104 5.05 146 3051 131 13 Male 37.2403 68 79 5.05 231 7034 280 60 Male 23.1376 78 80 5.07 288 1075 907 42 Female 27.2772 63 64 5.12 266 1048 576 94 Male 20.115 91 96 5.14 269 1611 511 60 Male 23.2799 69 107 5.16 314 3768 1916 0 Male 19.1102 69 80 5.16 237 3226 444 0 Male 27.4552 76 64 5.21 134 5896 126 4 Female 26.8775 50 74 5.25 88 6564 69 0 Male 34.4997 67 74 5.26 218 5085 269 2 Male 49.0267 65 77 5.26 5 3728 19 6 Male 30.1273 67 73 5.3 166 6664 164 2 Male 24.7337 66 73 5.34 110 5712 88 14 Male 22.2697 70 68 5.36 138 6214 112 0 Male 60.3176 65 74 5.37 125 5085 117 2 Male 49.0267 67 71 5.38 97 3058 56 28 Male 22.2533 65 75 5.45 238 3467 333 42 Male 25.3936 68 74 5.46 49 3237 65 9 Male 49.8508 67 67 5.55 325 651 3412 21 Male 22.0068 68 92 5.62 328 3835 4933 14 Male 25.9932 91 88 5.69 4 3592 13 10 Male 61.6646 59 73 5.77 268 1493 684 60 Male 17.8042 66 75 5.83 144 2849 151 0 Male 20.0876 51 86 5.89 96 2569 49 35 Male 18.7159 50 101 5.94 232 1493 453 60 Male 17.8042 59 81 6.03 310 1939 1926 130 Male 28.2738 95 108 6.41 208 3467 186 42 Male 25.3936 53 69 6.45 167 1048 85 94 Male 20.115 63 82 6.51 271 1939 562 130 Male 28.2738 85 111 6.51 321 1939 3111 130 Male 28.2738 88 111 6.93 234 1939 295 130 Male 28.2738 67 117 7.17 326 1939 3864 130 Male 28.2738 88 105 7.22 329 2773 7631 42 Male 6.51335 88 103 7.96 311 2662 1569 180 Male 28.0821 90 101 8.24 331 5964 11038 0 Male 12.8363 71 73 11.23 330 5142 11628 57 Male 16.4326 101 95 11.25 305 2638 1093 255 Male 16.5613 78 84 11.27 257 5804 354 2 Female 42.24 122 105 nan","title":"Ketujuh"},{"location":"3knn/#source","text":"seluruh file bisa diunduh disini atau disini .","title":"Source"},{"location":"3knn/#sumber-statistik-deskriptif","text":"https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"Sumber Statistik Deskriptif"},{"location":"4tree/","text":"Klasifikasi dapat diartikan sebagai masalah pengidentifikasian sekumpulan himpunan kategori dan masuk pada observasi baru, berdasarkan serangkaian data pelatihan yang berisi observasi dan kategori yang diketahui keanggotannya. Berikut ada beberapa cara yang bisa dipakai untuk melakukan klasifikasi: DecisionTree based Methods (DBM) Rule based Method (RBM) Neural Networks (NN) Naive Bayes and Bayesian Belief Networks Suport Vector Machine Namun saya disini hanya membahas tentang pengklasifikasian menggunakan Cara Decision Tree based Methods (Metode basis Pohon Keputusan) Decision Tree based Model \u00b6 Decision tree merupakan model prediktif untuk menjalankan pengamatan yang diwakili oleh cabang dengan bentuk menyerupai pohon, yang tujuannya untuk mendapatkan kesimpulan ttg target nilai sebuah item. J.R. Quinlan menyebut ID3 sebagai model untuk membangun pohon keputusan. Nilai information gain tertinggi sebuah atribut digunakan untuk menentukan akar (root) pada decision tree yang akan dibuat. Entropy \u00b6 Entropy bisa disebut juga sebagai ukuran gangguan atau tidak murninya suatu sistem dalam banyak contoh. Entropy bisa dihitung menggunakan rumus dibawah ini: \\begin{equation} E(S)=\\sum_{i=1}^{n}-P_{i} \\log _{2} \\mathrm{P}_{i} \\end{equation} \\begin{equation} E(S)=\\sum_{i=1}^{n}-P_{i} \\log _{2} \\mathrm{P}_{i} \\end{equation} dengan catatan: S = Kemurnian sebuah data P = Probability Class Information Gain \u00b6 Information Gain adalah ukuran suatu informasi yang diberikan oleh fitur kepada kita mengenai kelas. Atau bisa juga dinyatakan dengan: \\begin{equation} \\operatorname{Gain}(S, A)=\\operatorname{entropy}(S)-\\sum_{i=1}^{n} \\frac{\\left|s_{i}\\right|}{|s|} \\times \\operatorname{entropy}\\left(S_{i}\\right) \\end{equation} \\begin{equation} \\operatorname{Gain}(S, A)=\\operatorname{entropy}(S)-\\sum_{i=1}^{n} \\frac{\\left|s_{i}\\right|}{|s|} \\times \\operatorname{entropy}\\left(S_{i}\\right) \\end{equation} Keterangan: |S| = Banyak data Si = Nilai Atribut A = Atribut Manual Klasifikasi Decison Tree \u00b6 Berikut adalah data yang digunakan sebagai data training dalam klasifikasi Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Untuk memudahkan dalam perhitungan entropy dan information gain, maka kita bisa membentuk dan mengumpulkannya seperti tabel di bawah ini: Node Attibut Value Jumlah Kasus (S) C0 C1 1 Total 20 10 10 Gender F 10 6 4 M 10 4 6 Car Type Family 4 8 0 Sport 8 1 7 Luxury 8 Shirt Size Small 5 3 2 Medium 7 3 4 Large 4 2 2 Extra Large 4 2 2 Dari tabel di atas, pertama yang kita lakukan adalah mencari nilai entropy totalnya dengan menggunakan rumus yang telah dinyatakan di atas. \\begin{equation} \\begin{array}{l}{\\text {Entropy}(S)=-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}+-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}} \\\\ {E(S)=1}\\end{array} \\end{equation} \\begin{equation} \\begin{array}{l}{\\text {Entropy}(S)=-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}+-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}} \\\\ {E(S)=1}\\end{array} \\end{equation} Setelah memperoleh nilai entropy total, maka kita harus mendapatkan information gain dari tiap fitur. Fitur yang memiliki nilai informasi gain tertinggilah yang akan menjadi akar (root) pada decision tree nantinya. Menghitung informasion gain dari fitur gender: \\begin{aligned} G(S, \\text { Gender }) &=1-\\frac{10}{20} \\times E(6,4)+\\frac{10}{20} \\times E(4,6) \\\\ &=1-\\frac{10}{20} \\times 0.97095+\\frac{10}{20} \\times 0.97095 \\\\ &=1-0.97095=0.02905 \\end{aligned} \\begin{aligned} G(S, \\text { Gender }) &=1-\\frac{10}{20} \\times E(6,4)+\\frac{10}{20} \\times E(4,6) \\\\ &=1-\\frac{10}{20} \\times 0.97095+\\frac{10}{20} \\times 0.97095 \\\\ &=1-0.97095=0.02905 \\end{aligned} Menghitung informmation gain dari fitur Tipe Car: \\begin{equation} \\begin{aligned} G(S, \\text { CarType }) &=1-\\frac{4}{20} \\times E(1,3)+\\frac{8}{20} \\times E(8,0)+\\frac{8}{20} \\times E(1,7) \\\\ &=1-\\frac{4}{20} \\times 0.81128+\\frac{8}{20} \\times 0+\\frac{8}{20} \\times 0.54356 \\\\ &=1-0.37968=0.62032 \\end{aligned} \\end{equation} \\begin{equation} \\begin{aligned} G(S, \\text { CarType }) &=1-\\frac{4}{20} \\times E(1,3)+\\frac{8}{20} \\times E(8,0)+\\frac{8}{20} \\times E(1,7) \\\\ &=1-\\frac{4}{20} \\times 0.81128+\\frac{8}{20} \\times 0+\\frac{8}{20} \\times 0.54356 \\\\ &=1-0.37968=0.62032 \\end{aligned} \\end{equation} Menghitung information gain dari fitur shitSize \\begin{equation} \\begin{aligned} G(S, \\text { Shirt Size }) &=1-\\frac{5}{20} \\times E(3,2)+\\frac{7}{20} \\times E(3,4)+\\frac{4}{20} \\times E(2,2)+\\frac{4}{20} \\times E(2,2) \\\\ &=1-\\frac{5}{20} \\times 0.97095+\\frac{7}{20} \\times 0.98523+\\frac{4}{20} \\times 1+\\frac{8}{20} \\times 1 \\\\ &=1-0.98756=0.12440 \\end{aligned} \\end{equation} \\begin{equation} \\begin{aligned} G(S, \\text { Shirt Size }) &=1-\\frac{5}{20} \\times E(3,2)+\\frac{7}{20} \\times E(3,4)+\\frac{4}{20} \\times E(2,2)+\\frac{4}{20} \\times E(2,2) \\\\ &=1-\\frac{5}{20} \\times 0.97095+\\frac{7}{20} \\times 0.98523+\\frac{4}{20} \\times 1+\\frac{8}{20} \\times 1 \\\\ &=1-0.98756=0.12440 \\end{aligned} \\end{equation} Berdasarkan perhitungan diatas maka diperoleh fitur Car Type merupakan fitur yang nilai information gain nya tertinggi. Maka kita bisa membuat gambaran decision tree nya seperti ini. Selanjutnya kelompokan kembali seperti tabel berikut dengan menyaring data rule seluruh nilai dari fitur car type adalah family. Node Attribut Value Jumlah Kasus (S) C0 C1 2 Car Type=Family 4 1 3 Gender M 4 1 3 Shirt Size Small 1 1 0 Medium 1 0 1 Large 1 0 1 Extra Large 1 0 1 Lalu lakukan cara yang sama seperti sebelumnya untuk mencari nilai entropy total. \\begin{array}{l}{\\text {Entropy}(\\text {Family})=-\\frac{1}{4} \\times^{2} \\log _{\\frac{1}{4}}+-\\frac{3}{4} \\times^{2} \\log _{\\frac{3}{4}}} \\\\ {E(\\text {Family})=0,811278}\\end{array} \\begin{array}{l}{\\text {Entropy}(\\text {Family})=-\\frac{1}{4} \\times^{2} \\log _{\\frac{1}{4}}+-\\frac{3}{4} \\times^{2} \\log _{\\frac{3}{4}}} \\\\ {E(\\text {Family})=0,811278}\\end{array} Setelah itu cari nilai information gain dari masing-masing fitur yg telah disaring/filter. Nilai firut yang paling tinggi lah yang akan menjadi akar. Menghitung informaton gain dari feature gender: \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{4}{4} \\times E(1,3)} \\\\ {} & {=0,811278-1 \\times 0,811278} \\\\ {} & {=0,811278-0,811278=0}\\end{array} \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{4}{4} \\times E(1,3)} \\\\ {} & {=0,811278-1 \\times 0,811278} \\\\ {} & {=0,811278-0,811278=0}\\end{array} Menghitung information gain dari fitur shirtSize: \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{1}{4} \\times E(1,0)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)} \\\\ {} & {=0,811278-\\left(\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)\\right)} \\\\ {=} & {0,811278-0=0,811278}\\end{array} \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{1}{4} \\times E(1,0)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)} \\\\ {} & {=0,811278-\\left(\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)\\right)} \\\\ {=} & {0,811278-0=0,811278}\\end{array} Setelah didapatkan nilai information gain tertingginya yaitu Shirt Size, bentuk decision tree Lakukan hal yang sama seperti diatas, dengan data yg disaring menggunakan rule, seluruh nilai dari Car Type adalah Sport. Node Attribut Value Jumlah Kasus (S) C0 C1 2 Car Type=Sport 8 8 0 Gender M 5 5 0 F 3 3 0 Shirt Size Small 3 3 0 Medium 2 2 0 Large 1 1 0 Extra Large 2 2 0 Kita bisa menghitung seperti cara sebelumnya. Karena atribut pada clas C1 = 0, maka bisa kita buat node seperti sport adalah C0. Selanjutnya filter data dengan rule, seluruh nilai Car Type adalah Luxury. Node Attribut Value Jumlah Kasus (S) C0 C1 2 Car Type=Luxury 8 1 7 Gender M 1 0 1 F 7 1 6 Shirt Size Small 2 0 1 Medium 3 0 3 Large 2 1 1 Extra Large 1 0 1 Lakukan cara yang sama seperti di atas, untuk mendapatkan nilai entropy total pada atribut car type dengan nilai luxury. \\begin{array}{l}{\\text {Entropy}(\\text {Luxury})=-\\frac{1}{8} \\times^{2} \\log _{\\frac{1}{8}}+-\\frac{7}{8} \\times^{2} \\log _{\\frac{7}{8}}} \\\\ {E(\\text {Luxury})=0,543564}\\end{array} \\begin{array}{l}{\\text {Entropy}(\\text {Luxury})=-\\frac{1}{8} \\times^{2} \\log _{\\frac{1}{8}}+-\\frac{7}{8} \\times^{2} \\log _{\\frac{7}{8}}} \\\\ {E(\\text {Luxury})=0,543564}\\end{array} Cari information gain dari masing masing fitur yg telah difilter. Nilai tertinggi akan menjadi akar Information Gain dari fitur gender \\begin{array}{l}{G(S, M)=0,543564-\\frac{1}{8} \\times E(0,1)+\\frac{7}{8} \\times E(1,6)} \\\\ {\\qquad \\begin{array}{l}{=0,543564-\\left(\\frac{1}{8} \\times 0\\right)+\\left(\\frac{7}{8} \\times 0,591673\\right)} \\\\ {=0,543564-0,517714=0,025851}\\end{array}}\\end{array} \\begin{array}{l}{G(S, M)=0,543564-\\frac{1}{8} \\times E(0,1)+\\frac{7}{8} \\times E(1,6)} \\\\ {\\qquad \\begin{array}{l}{=0,543564-\\left(\\frac{1}{8} \\times 0\\right)+\\left(\\frac{7}{8} \\times 0,591673\\right)} \\\\ {=0,543564-0,517714=0,025851}\\end{array}}\\end{array} Information gain dari fitur shirt size \\begin{array}{l}{G(S, M)=0,543564-\\frac{2}{8} \\times E(0,1)+\\frac{3}{8} \\times E(0,3)+\\frac{2}{8} \\times E(1,1)+\\frac{1}{8} \\times E(0,1)} \\\\ {\\quad=0,543564-\\left(\\left(\\frac{2}{8} \\times 0\\right)+\\left(\\frac{3}{8} \\times 0\\right)+\\left(\\frac{2}{8} \\times 1\\right)+\\left(\\frac{1}{8} \\times 0\\right)\\right)} \\\\ {=0,543564-0,25=0,293564}\\end{array} \\begin{array}{l}{G(S, M)=0,543564-\\frac{2}{8} \\times E(0,1)+\\frac{3}{8} \\times E(0,3)+\\frac{2}{8} \\times E(1,1)+\\frac{1}{8} \\times E(0,1)} \\\\ {\\quad=0,543564-\\left(\\left(\\frac{2}{8} \\times 0\\right)+\\left(\\frac{3}{8} \\times 0\\right)+\\left(\\frac{2}{8} \\times 1\\right)+\\left(\\frac{1}{8} \\times 0\\right)\\right)} \\\\ {=0,543564-0,25=0,293564}\\end{array} Akhirnya diperoleh decision tree seperti berikut: Kelompokan data berdasarkan nilai atribut car tipe dan shirt size. kita dapat membuat tabel hasil pengelompokan seperti dibawah ini: Data yang difilter rule, car type yaitu family dan shirt size small. Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Small 1 1 0 Gender M 1 1 0 Data yang difilter rule, car type yaitu family dan shirt size adalah medium. Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Medium 1 0 1 Gender M 1 0 1 Data filter rule, car type yaitu family dan shirt = large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Large 1 0 1 Gender M 1 0 1 Data filter rule car type adalah fmaily dan shirt size = extra large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Extra Large 1 0 1 Gender M 1 0 1 Data filter rule car type car type = luxury dan shirt size = small Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Small 2 0 2 Gender F 2 0 2 Data filter rule car type = luxury dan shirt size = medium. Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Medium 3 0 3 Gender F 3 0 3 data filter rule car type = luxury, shirt size = large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Large 2 1 1 Gender F 2 1 1 Data filter rule, car type = luxury dan shirt size = extra large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Extra Large 1 0 1 Gender F 1 0 1 Dari hasil pengelompokan diatas maka menghasilkan decision tree seperti ini: Penerapan Klasifikasi Decision Tree Menggunakan Python \u00b6 Alat dan Bahan \u00b6 Sebagai persiapan untuk melakukan implementasi klasifikasi decision tree, kita perlu data training dan library python yang akan memudahkan menyelesaikan kasus. Data training menggunakan data diatas. Persiapkan juga librarynya: pandas, manajemen dan analisis data scipy, kumpulan algoritma dan fungsi matemaika Proses \u00b6 Masukan library untuk melakukan klasifikasi, setelah itu panggil data yang akan dipakai data training. import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier , plot_tree data = pd . read_excel ( 'klasifikasi_data.xlsx' ) df = pd . DataFrame ( data ) df . style . hide_index () tampilah dataframe seperti berikut: Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Sklearn tidak bisa memproses data karakter, oleh karena itu rubah seluruh fitur dalam bentuk angka dengan modul sklearn yaitu LabelEncoder() la = LabelEncoder () df [ 'gender_n' ] = la . fit_transform ( df [ 'Gender' ]) df [ 'car_type_n' ] = la . fit_transform ( df [ 'Car Type' ]) df [ 'shirt_size_n' ] = la . fit_transform ( df [ 'Shirt Size' ]) df [ 'class_n' ] = la . fit_transform ( df [ 'Class' ]) df . style . hide_index () Munculah seperti berikut dalam bentuk dataframe: Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 1 1 Buang fitur string, gunakan fitur yg telah diubah dalam bentuk angka untuk data training. inputs = df . drop ([ 'Customer ID' , 'Gender' , 'Car Type' , 'Shirt Size' , 'Class' , 'class_n' ], axis = 'columns' ) target = df [ 'class_n' ] Data training yang telah diambil, lakukan klasifikasi dengan modul bawaan sklearn dengan kriteria berupa entropy seperti berikut. model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) Lalu kita dapat memvisualkan dalam decision tree dengan plot_tree dari sklearn. plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ 'Customer ID' , 'Gender' , 'Car Type' , 'Shirt Size' ,], class_names = [ 'C0' , 'C1' ], label = 'all' , filled = True , impurity = True , node_ids = False , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) maka akan tampil seperti berikut proses encode diatas diperolehlah hasil sperti ini: Gender Car Type Shirt Size Class Value Encode Value Encode Value Encode Value Encode F 0 Family 1 Small 3 C0 0 M 1 Sport 2 Medium 2 C1 1 Luxury 0 Large 1 Extra Large 0 Jadi apabila ada data baru yang belum memiliki kelas, kita dapat memprediksi dia masuk kelas apa. Misal: Customer ID Gender Car Type Shirt Size Class 21 F Family Medium ? model . predict ([[ 0 , 1 , 2 ]]) Output : array ([ 1 ]) Dari prediksi di atas menghasilkan nilai [1] itu berarti data masuk pada kelas C1. Sumber Statistik Deskriptif \u00b6 https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"4- Klasifikasi dengan Decition Tree"},{"location":"4tree/#decision-tree-based-model","text":"Decision tree merupakan model prediktif untuk menjalankan pengamatan yang diwakili oleh cabang dengan bentuk menyerupai pohon, yang tujuannya untuk mendapatkan kesimpulan ttg target nilai sebuah item. J.R. Quinlan menyebut ID3 sebagai model untuk membangun pohon keputusan. Nilai information gain tertinggi sebuah atribut digunakan untuk menentukan akar (root) pada decision tree yang akan dibuat.","title":"Decision Tree based Model"},{"location":"4tree/#entropy","text":"Entropy bisa disebut juga sebagai ukuran gangguan atau tidak murninya suatu sistem dalam banyak contoh. Entropy bisa dihitung menggunakan rumus dibawah ini: \\begin{equation} E(S)=\\sum_{i=1}^{n}-P_{i} \\log _{2} \\mathrm{P}_{i} \\end{equation} \\begin{equation} E(S)=\\sum_{i=1}^{n}-P_{i} \\log _{2} \\mathrm{P}_{i} \\end{equation} dengan catatan: S = Kemurnian sebuah data P = Probability Class","title":"Entropy"},{"location":"4tree/#information-gain","text":"Information Gain adalah ukuran suatu informasi yang diberikan oleh fitur kepada kita mengenai kelas. Atau bisa juga dinyatakan dengan: \\begin{equation} \\operatorname{Gain}(S, A)=\\operatorname{entropy}(S)-\\sum_{i=1}^{n} \\frac{\\left|s_{i}\\right|}{|s|} \\times \\operatorname{entropy}\\left(S_{i}\\right) \\end{equation} \\begin{equation} \\operatorname{Gain}(S, A)=\\operatorname{entropy}(S)-\\sum_{i=1}^{n} \\frac{\\left|s_{i}\\right|}{|s|} \\times \\operatorname{entropy}\\left(S_{i}\\right) \\end{equation} Keterangan: |S| = Banyak data Si = Nilai Atribut A = Atribut","title":"Information Gain"},{"location":"4tree/#manual-klasifikasi-decison-tree","text":"Berikut adalah data yang digunakan sebagai data training dalam klasifikasi Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Untuk memudahkan dalam perhitungan entropy dan information gain, maka kita bisa membentuk dan mengumpulkannya seperti tabel di bawah ini: Node Attibut Value Jumlah Kasus (S) C0 C1 1 Total 20 10 10 Gender F 10 6 4 M 10 4 6 Car Type Family 4 8 0 Sport 8 1 7 Luxury 8 Shirt Size Small 5 3 2 Medium 7 3 4 Large 4 2 2 Extra Large 4 2 2 Dari tabel di atas, pertama yang kita lakukan adalah mencari nilai entropy totalnya dengan menggunakan rumus yang telah dinyatakan di atas. \\begin{equation} \\begin{array}{l}{\\text {Entropy}(S)=-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}+-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}} \\\\ {E(S)=1}\\end{array} \\end{equation} \\begin{equation} \\begin{array}{l}{\\text {Entropy}(S)=-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}+-\\frac{10}{20} \\times^{2} \\log _{\\frac{10}{20}}} \\\\ {E(S)=1}\\end{array} \\end{equation} Setelah memperoleh nilai entropy total, maka kita harus mendapatkan information gain dari tiap fitur. Fitur yang memiliki nilai informasi gain tertinggilah yang akan menjadi akar (root) pada decision tree nantinya. Menghitung informasion gain dari fitur gender: \\begin{aligned} G(S, \\text { Gender }) &=1-\\frac{10}{20} \\times E(6,4)+\\frac{10}{20} \\times E(4,6) \\\\ &=1-\\frac{10}{20} \\times 0.97095+\\frac{10}{20} \\times 0.97095 \\\\ &=1-0.97095=0.02905 \\end{aligned} \\begin{aligned} G(S, \\text { Gender }) &=1-\\frac{10}{20} \\times E(6,4)+\\frac{10}{20} \\times E(4,6) \\\\ &=1-\\frac{10}{20} \\times 0.97095+\\frac{10}{20} \\times 0.97095 \\\\ &=1-0.97095=0.02905 \\end{aligned} Menghitung informmation gain dari fitur Tipe Car: \\begin{equation} \\begin{aligned} G(S, \\text { CarType }) &=1-\\frac{4}{20} \\times E(1,3)+\\frac{8}{20} \\times E(8,0)+\\frac{8}{20} \\times E(1,7) \\\\ &=1-\\frac{4}{20} \\times 0.81128+\\frac{8}{20} \\times 0+\\frac{8}{20} \\times 0.54356 \\\\ &=1-0.37968=0.62032 \\end{aligned} \\end{equation} \\begin{equation} \\begin{aligned} G(S, \\text { CarType }) &=1-\\frac{4}{20} \\times E(1,3)+\\frac{8}{20} \\times E(8,0)+\\frac{8}{20} \\times E(1,7) \\\\ &=1-\\frac{4}{20} \\times 0.81128+\\frac{8}{20} \\times 0+\\frac{8}{20} \\times 0.54356 \\\\ &=1-0.37968=0.62032 \\end{aligned} \\end{equation} Menghitung information gain dari fitur shitSize \\begin{equation} \\begin{aligned} G(S, \\text { Shirt Size }) &=1-\\frac{5}{20} \\times E(3,2)+\\frac{7}{20} \\times E(3,4)+\\frac{4}{20} \\times E(2,2)+\\frac{4}{20} \\times E(2,2) \\\\ &=1-\\frac{5}{20} \\times 0.97095+\\frac{7}{20} \\times 0.98523+\\frac{4}{20} \\times 1+\\frac{8}{20} \\times 1 \\\\ &=1-0.98756=0.12440 \\end{aligned} \\end{equation} \\begin{equation} \\begin{aligned} G(S, \\text { Shirt Size }) &=1-\\frac{5}{20} \\times E(3,2)+\\frac{7}{20} \\times E(3,4)+\\frac{4}{20} \\times E(2,2)+\\frac{4}{20} \\times E(2,2) \\\\ &=1-\\frac{5}{20} \\times 0.97095+\\frac{7}{20} \\times 0.98523+\\frac{4}{20} \\times 1+\\frac{8}{20} \\times 1 \\\\ &=1-0.98756=0.12440 \\end{aligned} \\end{equation} Berdasarkan perhitungan diatas maka diperoleh fitur Car Type merupakan fitur yang nilai information gain nya tertinggi. Maka kita bisa membuat gambaran decision tree nya seperti ini. Selanjutnya kelompokan kembali seperti tabel berikut dengan menyaring data rule seluruh nilai dari fitur car type adalah family. Node Attribut Value Jumlah Kasus (S) C0 C1 2 Car Type=Family 4 1 3 Gender M 4 1 3 Shirt Size Small 1 1 0 Medium 1 0 1 Large 1 0 1 Extra Large 1 0 1 Lalu lakukan cara yang sama seperti sebelumnya untuk mencari nilai entropy total. \\begin{array}{l}{\\text {Entropy}(\\text {Family})=-\\frac{1}{4} \\times^{2} \\log _{\\frac{1}{4}}+-\\frac{3}{4} \\times^{2} \\log _{\\frac{3}{4}}} \\\\ {E(\\text {Family})=0,811278}\\end{array} \\begin{array}{l}{\\text {Entropy}(\\text {Family})=-\\frac{1}{4} \\times^{2} \\log _{\\frac{1}{4}}+-\\frac{3}{4} \\times^{2} \\log _{\\frac{3}{4}}} \\\\ {E(\\text {Family})=0,811278}\\end{array} Setelah itu cari nilai information gain dari masing-masing fitur yg telah disaring/filter. Nilai firut yang paling tinggi lah yang akan menjadi akar. Menghitung informaton gain dari feature gender: \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{4}{4} \\times E(1,3)} \\\\ {} & {=0,811278-1 \\times 0,811278} \\\\ {} & {=0,811278-0,811278=0}\\end{array} \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{4}{4} \\times E(1,3)} \\\\ {} & {=0,811278-1 \\times 0,811278} \\\\ {} & {=0,811278-0,811278=0}\\end{array} Menghitung information gain dari fitur shirtSize: \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{1}{4} \\times E(1,0)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)} \\\\ {} & {=0,811278-\\left(\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)\\right)} \\\\ {=} & {0,811278-0=0,811278}\\end{array} \\begin{array}{rl}{G(S, M)=0,81} & {1278-\\frac{1}{4} \\times E(1,0)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)+\\frac{1}{4} \\times E(0,1)} \\\\ {} & {=0,811278-\\left(\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)+\\left(\\frac{1}{4} \\times 0\\right)\\right)} \\\\ {=} & {0,811278-0=0,811278}\\end{array} Setelah didapatkan nilai information gain tertingginya yaitu Shirt Size, bentuk decision tree Lakukan hal yang sama seperti diatas, dengan data yg disaring menggunakan rule, seluruh nilai dari Car Type adalah Sport. Node Attribut Value Jumlah Kasus (S) C0 C1 2 Car Type=Sport 8 8 0 Gender M 5 5 0 F 3 3 0 Shirt Size Small 3 3 0 Medium 2 2 0 Large 1 1 0 Extra Large 2 2 0 Kita bisa menghitung seperti cara sebelumnya. Karena atribut pada clas C1 = 0, maka bisa kita buat node seperti sport adalah C0. Selanjutnya filter data dengan rule, seluruh nilai Car Type adalah Luxury. Node Attribut Value Jumlah Kasus (S) C0 C1 2 Car Type=Luxury 8 1 7 Gender M 1 0 1 F 7 1 6 Shirt Size Small 2 0 1 Medium 3 0 3 Large 2 1 1 Extra Large 1 0 1 Lakukan cara yang sama seperti di atas, untuk mendapatkan nilai entropy total pada atribut car type dengan nilai luxury. \\begin{array}{l}{\\text {Entropy}(\\text {Luxury})=-\\frac{1}{8} \\times^{2} \\log _{\\frac{1}{8}}+-\\frac{7}{8} \\times^{2} \\log _{\\frac{7}{8}}} \\\\ {E(\\text {Luxury})=0,543564}\\end{array} \\begin{array}{l}{\\text {Entropy}(\\text {Luxury})=-\\frac{1}{8} \\times^{2} \\log _{\\frac{1}{8}}+-\\frac{7}{8} \\times^{2} \\log _{\\frac{7}{8}}} \\\\ {E(\\text {Luxury})=0,543564}\\end{array} Cari information gain dari masing masing fitur yg telah difilter. Nilai tertinggi akan menjadi akar Information Gain dari fitur gender \\begin{array}{l}{G(S, M)=0,543564-\\frac{1}{8} \\times E(0,1)+\\frac{7}{8} \\times E(1,6)} \\\\ {\\qquad \\begin{array}{l}{=0,543564-\\left(\\frac{1}{8} \\times 0\\right)+\\left(\\frac{7}{8} \\times 0,591673\\right)} \\\\ {=0,543564-0,517714=0,025851}\\end{array}}\\end{array} \\begin{array}{l}{G(S, M)=0,543564-\\frac{1}{8} \\times E(0,1)+\\frac{7}{8} \\times E(1,6)} \\\\ {\\qquad \\begin{array}{l}{=0,543564-\\left(\\frac{1}{8} \\times 0\\right)+\\left(\\frac{7}{8} \\times 0,591673\\right)} \\\\ {=0,543564-0,517714=0,025851}\\end{array}}\\end{array} Information gain dari fitur shirt size \\begin{array}{l}{G(S, M)=0,543564-\\frac{2}{8} \\times E(0,1)+\\frac{3}{8} \\times E(0,3)+\\frac{2}{8} \\times E(1,1)+\\frac{1}{8} \\times E(0,1)} \\\\ {\\quad=0,543564-\\left(\\left(\\frac{2}{8} \\times 0\\right)+\\left(\\frac{3}{8} \\times 0\\right)+\\left(\\frac{2}{8} \\times 1\\right)+\\left(\\frac{1}{8} \\times 0\\right)\\right)} \\\\ {=0,543564-0,25=0,293564}\\end{array} \\begin{array}{l}{G(S, M)=0,543564-\\frac{2}{8} \\times E(0,1)+\\frac{3}{8} \\times E(0,3)+\\frac{2}{8} \\times E(1,1)+\\frac{1}{8} \\times E(0,1)} \\\\ {\\quad=0,543564-\\left(\\left(\\frac{2}{8} \\times 0\\right)+\\left(\\frac{3}{8} \\times 0\\right)+\\left(\\frac{2}{8} \\times 1\\right)+\\left(\\frac{1}{8} \\times 0\\right)\\right)} \\\\ {=0,543564-0,25=0,293564}\\end{array} Akhirnya diperoleh decision tree seperti berikut: Kelompokan data berdasarkan nilai atribut car tipe dan shirt size. kita dapat membuat tabel hasil pengelompokan seperti dibawah ini: Data yang difilter rule, car type yaitu family dan shirt size small. Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Small 1 1 0 Gender M 1 1 0 Data yang difilter rule, car type yaitu family dan shirt size adalah medium. Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Medium 1 0 1 Gender M 1 0 1 Data filter rule, car type yaitu family dan shirt = large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Large 1 0 1 Gender M 1 0 1 Data filter rule car type adalah fmaily dan shirt size = extra large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Family, Shirt Size=Extra Large 1 0 1 Gender M 1 0 1 Data filter rule car type car type = luxury dan shirt size = small Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Small 2 0 2 Gender F 2 0 2 Data filter rule car type = luxury dan shirt size = medium. Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Medium 3 0 3 Gender F 3 0 3 data filter rule car type = luxury, shirt size = large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Large 2 1 1 Gender F 2 1 1 Data filter rule, car type = luxury dan shirt size = extra large Node Attribut Value Jumlah Kasus (S) C0 C1 3 Car Type=Luxury, Shirt Size=Extra Large 1 0 1 Gender F 1 0 1 Dari hasil pengelompokan diatas maka menghasilkan decision tree seperti ini:","title":"Manual Klasifikasi Decison Tree"},{"location":"4tree/#penerapan-klasifikasi-decision-tree-menggunakan-python","text":"","title":"Penerapan Klasifikasi Decision Tree Menggunakan Python"},{"location":"4tree/#alat-dan-bahan","text":"Sebagai persiapan untuk melakukan implementasi klasifikasi decision tree, kita perlu data training dan library python yang akan memudahkan menyelesaikan kasus. Data training menggunakan data diatas. Persiapkan juga librarynya: pandas, manajemen dan analisis data scipy, kumpulan algoritma dan fungsi matemaika","title":"Alat dan Bahan"},{"location":"4tree/#proses","text":"Masukan library untuk melakukan klasifikasi, setelah itu panggil data yang akan dipakai data training. import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.tree import DecisionTreeClassifier , plot_tree data = pd . read_excel ( 'klasifikasi_data.xlsx' ) df = pd . DataFrame ( data ) df . style . hide_index () tampilah dataframe seperti berikut: Customer ID Gender Car Type Shirt Size Class 1 M Family Small C0 2 M Sports Medium C0 3 M Sports Medium C0 4 M Sports Large C0 5 M Sports Extra Large C0 6 M Sports Extra Large C0 7 F Sports Small C0 8 F Sports Small C0 9 F Sports Medium C0 10 F Luxury Large C0 11 M Family Large C1 12 M Family Extra Large C1 13 M Family Medium C1 14 M Luxury Extra Large C1 15 F Luxury Small C1 16 F Luxury Small C1 17 F Luxury Medium C1 18 F Luxury Medium C1 19 F Luxury Medium C1 20 F Luxury Large C1 Sklearn tidak bisa memproses data karakter, oleh karena itu rubah seluruh fitur dalam bentuk angka dengan modul sklearn yaitu LabelEncoder() la = LabelEncoder () df [ 'gender_n' ] = la . fit_transform ( df [ 'Gender' ]) df [ 'car_type_n' ] = la . fit_transform ( df [ 'Car Type' ]) df [ 'shirt_size_n' ] = la . fit_transform ( df [ 'Shirt Size' ]) df [ 'class_n' ] = la . fit_transform ( df [ 'Class' ]) df . style . hide_index () Munculah seperti berikut dalam bentuk dataframe: Shirt Size Class gender_n car_type_n shirt_size_n class_n 1 M Family Small C0 1 0 3 0 2 M Sports Medium C0 1 2 2 0 3 M Sports Medium C0 1 2 2 0 4 M Sports Large C0 1 2 1 0 5 M Sports Extra Large C0 1 2 0 0 6 M Sports Extra Large C0 1 2 0 0 7 F Sports Small C0 0 2 3 0 8 F Sports Small C0 0 2 3 0 9 F Sports Medium C0 0 2 2 0 10 F Luxury Large C0 0 1 1 0 11 M Family Large C1 1 0 1 1 12 M Family Extra Large C1 1 0 0 1 13 M Family Medium C1 1 0 2 1 14 M Luxury Extra Large C1 1 1 0 1 15 F Luxury Small C1 0 1 3 1 16 F Luxury Small C1 0 1 3 1 17 F Luxury Medium C1 0 1 2 1 18 F Luxury Medium C1 0 1 2 1 19 F Luxury Medium C1 0 1 2 1 20 F Luxury Large C1 0 1 1 1 Buang fitur string, gunakan fitur yg telah diubah dalam bentuk angka untuk data training. inputs = df . drop ([ 'Customer ID' , 'Gender' , 'Car Type' , 'Shirt Size' , 'Class' , 'class_n' ], axis = 'columns' ) target = df [ 'class_n' ] Data training yang telah diambil, lakukan klasifikasi dengan modul bawaan sklearn dengan kriteria berupa entropy seperti berikut. model = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 ) model . fit ( inputs , target ) Lalu kita dapat memvisualkan dalam decision tree dengan plot_tree dari sklearn. plot_tree ( model . fit ( inputs , target ), max_depth = None , feature_names = [ 'Customer ID' , 'Gender' , 'Car Type' , 'Shirt Size' ,], class_names = [ 'C0' , 'C1' ], label = 'all' , filled = True , impurity = True , node_ids = False , proportion = True , rotate = True , rounded = True , precision = 3 , ax = None , fontsize = None ) maka akan tampil seperti berikut proses encode diatas diperolehlah hasil sperti ini: Gender Car Type Shirt Size Class Value Encode Value Encode Value Encode Value Encode F 0 Family 1 Small 3 C0 0 M 1 Sport 2 Medium 2 C1 1 Luxury 0 Large 1 Extra Large 0 Jadi apabila ada data baru yang belum memiliki kelas, kita dapat memprediksi dia masuk kelas apa. Misal: Customer ID Gender Car Type Shirt Size Class 21 F Family Medium ? model . predict ([[ 0 , 1 , 2 ]]) Output : array ([ 1 ]) Dari prediksi di atas menghasilkan nilai [1] itu berarti data masuk pada kelas C1.","title":"Proses"},{"location":"4tree/#sumber-statistik-deskriptif","text":"https://id.wikipedia.org/wiki/Statistika_deskriptif https://achfaisol.github.io/Statistik%20Deskriptif/#source https://sfathoni.github.io/statistik_deskriptif/","title":"Sumber Statistik Deskriptif"},{"location":"5kmodes/","text":"Clustering \u00b6 Sebuah cara untuk mengelompokkan data sesuai dengan ukuran kemiripan data tersebut itulah yang disebut Clustering. Untuk mengelompokan data pada metode clustering tidaklah harus sama, namun kemiripan data yang diperoleh berdasar pada kedekatan suatu karakteristik sample. Pada dasarnya metode clustering mengoptimalkan pusat centroid untuk melakukan pengelompokan. Prinsip manfaat metode clustering dalam bidang keilmuan: Identify Object (Recognition) Metode clustering biasa digunakan pada bidang image processing, computer vision, robot vision, dan lainya. Decission Support System and Data Mining Hal ini metode cluster biasa digunakan dalam segmen pasar, manajemen marketing pemetaan wilayah, dan lain sebagainya. Salah satu algoritma clustering yang sering digunakan yaitu K-Means Clustering. Hal ini memungkinkan untuk mengelompokan data sesuai kesamaan yang ada diantara mereka dalam k-means cluster, ditambahkan ke input algoritma. K-Means Clustering \u00b6 K-Means yaitu metode pengelompokan data dengan proses permodelan yang unsupervised (yang tidak terwarisi). K-Means ini merupakan salah satu metode clustering dengan sistem partisi, dan dalam proses pengelompokannya terdapat 2 jenis data yang dipakai. Hierarchical Non-Hierarchical Metode K-Means clustering adalah salah satu metode clustering untuk pengelompokan data yang kedua, yaitu non-hierarchical. Metode ini akan mengelompokan data ke dalam beberapa cluster, dari tiap cluster memiliki karakteristik yang mirip antara data satu dengan yang lainnya dan berbeda pada cluster yang lain. Pada dasarnya, K-Means clustering melakukan pengelompokan dengan algoritma seperti di bawah ini: Tentukan jumlah k (k=cluster) Alokasikan data awal ke cluster secara acak Hitung centroidnya (rata-rata) dari masing-masing cluster yang telah ditentukan Hitung jarak masing-masing data dengan centroid, dan alokasikan data pada cluster terdekat Ulangi langkah 3 dan 4 hingga nilai centroid tidak lagi berubah K-Modes Clustering \u00b6 K-Modes yaitu turunan dari metode K-Means. Pada K-Means bekerja hanya pada data numerik tidak bekerja pada data yang nilainya kategorikal. K-Modes bekerja seperti K-Means, namun metode ini khusus untuk mengelompokan data yang bernilai kategorikal. K-Modes bekerja pakai metode basis frekuansi untuk mengupdate modus dalam proses pengelompokan dan mengkuantifikasi total antara dua objek. Semakin kecil jumlahnya maka semakin mirip kedua objek ini. K-Prototype Clustering \u00b6 K-Prototype adalah metode clustering gabungan dari K-Means dan K-Modes. Metode ini dipakai untuk mengelompokan data yang memiliki atribut numerik dan kategorikal. . . Implementasi dengan Python \u00b6 K-Means \u00b6 Pada implementasi K-Means, kita bisa menggunakan data glass yang bisa diunduh di link ini , data tersebut bisa ditampilkan dalam bentuk data frame. Yang harus dilakukan pertama adalah memuat data menggunakan library pandas. import pandas as pd data = pd . read_csv ( 'glass.csv' , delimiter = ';' , decimal = ',' ) df = pd . DataFrame ( data ) df . style . hide_index () dari data diatas akan menampilkan hasil seperti di bawah ini: ID refractive index Sodium Magnesium Aluminum Silicon Potassium Calcium Barium Iron 1 1.52101 13.64 4.49 1.1 71.78 0.06 8.75 0 0.001 2 1.51761 13.89 3.6 1.36 72.73 0.48 7.83 0 0 3 1.51618 13.53 3.55 1.54 72.99 0.39 7.78 0 0 4 1.51766 13.21 3.69 1.29 72.61 0.57 8.22 0 0 5 1.51742 13.27 3.62 1.24 73.08 0.55 8.07 0 0 6 1.51596 12.79 3.61 1.62 72.97 0.64 8.07 0 0.26 7 1.51743 13.3 3.6 1.14 73.09 0.58 8.17 0 0 8 1.51756 13.15 3.61 1.05 73.24 0.57 8.24 0 0 9 1.51918 14.04 3.58 1.37 72.08 0.56 8.3 0 0 10 1.51755 13 3.6 1.36 72.99 0.57 8.4 0 0.11 11 1.51571 12.72 3.46 1.56 73.2 0.67 8.09 0 0.24 12 1.51763 12.8 3.66 1.27 73.01 0.6 8.56 0 0 13 1.51589 12.88 3.43 1.4 73.28 0.69 8.05 0 0.24 14 1.51748 12.86 3.56 1.27 73.21 0.54 8.38 0 0.17 15 1.51763 12.61 3.59 1.31 73.29 0.58 8.5 0 0 16 1.51761 12.81 3.54 1.23 73.24 0.58 8.39 0 0 17 1.51784 12.68 3.67 1.16 73.11 0.61 8.7 0 0 18 1.52196 14.36 3.85 0.89 71.36 0.15 9.15 0 0 19 1.51911 13.9 3.73 1.18 72.12 0.06 8.89 0 0 20 1.51735 13.02 3.54 1.69 72.73 0.54 8.44 0 0.07 21 1.5175 12.82 3.55 1.49 72.75 0.54 8.52 0 0.19 22 1.51966 14.77 3.75 0.29 72.02 0.03 9 0 0 23 1.51736 12.78 3.62 1.29 72.79 0.59 8.7 0 0 24 1.51751 12.81 3.57 1.35 73.02 0.62 8.59 0 0 25 1.5172 13.38 3.5 1.15 72.85 0.5 8.43 0 0 26 1.51764 12.98 3.54 1.21 73 0.65 8.53 0 0 27 1.51793 13.21 3.48 1.41 72.64 0.59 8.43 0 0 28 1.51721 12.87 3.48 1.33 73.04 0.56 8.43 0 0 29 1.51768 12.56 3.52 1.43 73.15 0.57 8.54 0 0 30 1.51784 13.08 3.49 1.28 72.86 0.6 8.49 0 0 31 1.51768 12.65 3.56 1.3 73.08 0.61 8.69 0 0.14 32 1.51747 12.84 3.5 1.14 73.27 0.56 8.55 0 0 33 1.51775 12.85 3.48 1.23 72.97 0.61 8.56 0.09 0.22 34 1.51753 12.57 3.47 1.38 73.39 0.6 8.55 0 0.06 35 1.51783 12.69 3.54 1.34 72.95 0.57 8.75 0 0 36 1.51567 13.29 3.45 1.21 72.74 0.56 8.57 0 0 37 1.51909 13.89 3.53 1.32 71.81 0.51 8.78 0.11 0 38 1.51797 12.74 3.48 1.35 72.96 0.64 8.68 0 0 39 1.52213 14.21 3.82 0.47 71.77 0.11 9.57 0 0 40 1.52213 14.21 3.82 0.47 71.77 0.11 9.57 0 0 41 1.51793 12.79 3.5 1.12 73.03 0.64 8.77 0 0 42 1.51755 12.71 3.42 1.2 73.2 0.59 8.64 0 0 43 1.51779 13.21 3.39 1.33 72.76 0.59 8.59 0 0 44 1.5221 13.73 3.84 0.72 71.76 0.17 9.74 0 0 45 1.51786 12.73 3.43 1.19 72.95 0.62 8.76 0 0.3 46 1.519 13.49 3.48 1.35 71.95 0.55 9 0 0 47 1.51869 13.19 3.37 1.18 72.72 0.57 8.83 0 0.16 48 1.52667 13.99 3.7 0.71 71.57 0.02 9.82 0 0.1 49 1.52223 13.21 3.77 0.79 71.99 0.13 10.02 0 0 50 1.51898 13.58 3.35 1.23 72.08 0.59 8.91 0 0 51 1.5232 13.72 3.72 0.51 71.75 0.09 10.06 0 0.16 52 1.51926 13.2 3.33 1.28 72.36 0.6 9.14 0 0.11 53 1.51808 13.43 2.87 1.19 72.84 0.55 9.03 0 0 54 1.51837 13.14 2.84 1.28 72.85 0.55 9.07 0 0 55 1.51778 13.21 2.81 1.29 72.98 0.51 9.02 0 0.09 56 1.51769 12.45 2.71 1.29 73.7 0.56 9.06 0 0.24 57 1.51215 12.99 3.47 1.12 72.98 0.62 8.35 0 0.31 58 1.51824 12.87 3.48 1.29 72.95 0.6 8.43 0 0 59 1.51754 13.48 3.74 1.17 72.99 0.59 8.03 0 0 60 1.51754 13.39 3.66 1.19 72.79 0.57 8.27 0 0.11 61 1.51905 13.6 3.62 1.11 72.64 0.14 8.76 0 0 62 1.51977 13.81 3.58 1.32 71.72 0.12 8.67 0.69 0 63 1.52172 13.51 3.86 0.88 71.79 0.23 9.54 0 0.11 64 1.52227 14.17 3.81 0.78 71.35 0 9.69 0 0 65 1.52172 13.48 3.74 0.9 72.01 0.18 9.61 0 0.07 66 1.52099 13.69 3.59 1.12 71.96 0.09 9.4 0 0 67 1.52152 13.05 3.65 0.87 72.22 0.19 9.85 0 0.17 68 1.52152 13.05 3.65 0.87 72.32 0.19 9.85 0 0.17 69 1.52152 13.12 3.58 0.9 72.2 0.23 9.82 0 0.16 70 1.523 13.31 3.58 0.82 71.99 0.12 10.17 0 0.03 71 1.51574 14.86 3.67 1.74 71.87 0.16 7.36 0 0.12 72 1.51848 13.64 3.87 1.27 71.96 0.54 8.32 0 0.32 73 1.51593 13.09 3.59 1.52 73.1 0.67 7.83 0 0 74 1.51631 13.34 3.57 1.57 72.87 0.61 7.89 0 0 75 1.51596 13.02 3.56 1.54 73.11 0.72 7.9 0 0 76 1.5159 13.02 3.58 1.51 73.12 0.69 7.96 0 0 77 1.51645 13.44 3.61 1.54 72.39 0.66 8.03 0 0 78 1.51627 13 3.58 1.54 72.83 0.61 8.04 0 0 79 1.51613 13.92 3.52 1.25 72.88 0.37 7.94 0 0.14 80 1.5159 12.82 3.52 1.9 72.86 0.69 7.97 0 0 81 1.51592 12.86 3.52 2.12 72.66 0.69 7.97 0 0 82 1.51593 13.25 3.45 1.43 73.17 0.61 7.86 0 0 83 1.51646 13.41 3.55 1.25 72.81 0.68 8.1 0 0 84 1.51594 13.09 3.52 1.55 72.87 0.68 8.05 0 0.09 85 1.51409 14.25 3.09 2.08 72.28 1.1 7.08 0 0 86 1.51625 13.36 3.58 1.49 72.72 0.45 8.21 0 0 87 1.51569 13.24 3.49 1.47 73.25 0.38 8.03 0 0 88 1.51645 13.4 3.49 1.52 72.65 0.67 8.08 0 0.1 89 1.51618 13.01 3.5 1.48 72.89 0.6 8.12 0 0 90 1.5164 12.55 3.48 1.87 73.23 0.63 8.08 0 0.09 91 1.51841 12.93 3.74 1.11 72.28 0.64 8.96 0 0.22 92 1.51605 12.9 3.44 1.45 73.06 0.44 8.27 0 0 93 1.51588 13.12 3.41 1.58 73.26 0.07 8.39 0 0.19 94 1.5159 13.24 3.34 1.47 73.1 0.39 8.22 0 0 95 1.51629 12.71 3.33 1.49 73.28 0.67 8.24 0 0 96 1.5186 13.36 3.43 1.43 72.26 0.51 8.6 0 0 97 1.51841 13.02 3.62 1.06 72.34 0.64 9.13 0 0.15 98 1.51743 12.2 3.25 1.16 73.55 0.62 8.9 0 0.24 99 1.51689 12.67 2.88 1.71 73.21 0.73 8.54 0 0 100 1.51811 12.96 2.96 1.43 72.92 0.6 8.79 0.14 0 101 1.51655 12.75 2.85 1.44 73.27 0.57 8.79 0.11 0.22 102 1.5173 12.35 2.72 1.63 72.87 0.7 9.23 0 0 103 1.5182 12.62 2.76 0.83 73.81 0.35 9.42 0 0.2 104 1.52725 13.8 3.15 0.66 70.57 0.08 11.64 0 0 105 1.5241 13.83 2.9 1.17 71.15 0.08 10.79 0 0 106 1.52475 11.45 0 1.88 72.19 0.81 13.24 0 0.34 107 1.53125 10.73 0 2.1 69.81 0.58 13.3 3.15 0.28 108 1.53393 12.3 0 1 70.16 0.12 16.19 0 0.24 109 1.52222 14.43 0 1 72.67 0.1 11.52 0 0.08 110 1.51818 13.72 0 0.56 74.45 0 10.99 0 0 111 1.52664 11.23 0 0.77 73.21 0 14.68 0 0 112 1.52739 11.02 0 0.75 73.08 0 14.96 0 0 113 1.52777 12.64 0 0.67 72.02 0.06 14.4 0 0 114 1.51892 13.46 3.83 1.26 72.55 0.57 8.21 0 0.14 115 1.51847 13.1 3.97 1.19 72.44 0.6 8.43 0 0 116 1.51846 13.41 3.89 1.33 72.38 0.51 8.28 0 0 117 1.51829 13.24 3.9 1.41 72.33 0.55 8.31 0 0.1 118 1.51708 13.72 3.68 1.81 72.06 0.64 7.88 0 0 119 1.51673 13.3 3.64 1.53 72.53 0.65 8.03 0 0.29 120 1.51652 13.56 3.57 1.47 72.45 0.64 7.96 0 0 121 1.51844 13.25 3.76 1.32 72.4 0.58 8.42 0 0 122 1.51663 12.93 3.54 1.62 72.96 0.64 8.03 0 0.21 123 1.51687 13.23 3.54 1.48 72.84 0.56 8.1 0 0 124 1.51707 13.48 3.48 1.71 72.52 0.62 7.99 0 0 125 1.52177 13.2 3.68 1.15 72.75 0.54 8.52 0 0 126 1.51872 12.93 3.66 1.56 72.51 0.58 8.55 0 0.12 127 1.51667 12.94 3.61 1.26 72.75 0.56 8.6 0 0 128 1.52081 13.78 2.28 1.43 71.99 0.49 9.85 0 0.17 129 1.52068 13.55 2.09 1.67 72.18 0.53 9.57 0.27 0.17 130 1.5202 13.98 1.35 1.63 71.76 0.39 10.56 0 0.18 131 1.52177 13.75 1.01 1.36 72.19 0.33 11.14 0 0 132 1.52614 13.7 0 1.36 71.24 0.19 13.44 0 0.1 133 1.51813 13.43 3.98 1.18 72.49 0.58 8.15 0 0 134 1.518 13.71 3.93 1.54 71.81 0.54 8.21 0 0.15 135 1.51811 13.33 3.85 1.25 72.78 0.52 8.12 0 0 136 1.51789 13.19 3.9 1.3 72.33 0.55 8.44 0 0.28 137 1.51806 13 3.8 1.08 73.07 0.56 8.38 0 0.12 138 1.51711 12.89 3.62 1.57 72.96 0.61 8.11 0 0 139 1.51674 12.79 3.52 1.54 73.36 0.66 7.9 0 0 140 1.51674 12.87 3.56 1.64 73.14 0.65 7.99 0 0 141 1.5169 13.33 3.54 1.61 72.54 0.68 8.11 0 0 142 1.51851 13.2 3.63 1.07 72.83 0.57 8.41 0.09 0.17 143 1.51662 12.85 3.51 1.44 73.01 0.68 8.23 0.06 0.25 144 1.51709 13 3.47 1.79 72.72 0.66 8.18 0 0 145 1.5166 12.99 3.18 1.23 72.97 0.58 8.81 0 0.24 146 1.51839 12.85 3.67 1.24 72.57 0.62 8.68 0 0.35 147 1.51769 13.65 3.66 1.11 72.77 0.11 8.6 0 0 148 1.5161 13.33 3.53 1.34 72.67 0.56 8.33 0 0 149 1.5167 13.24 3.57 1.38 72.7 0.56 8.44 0 0.1 150 1.51643 12.16 3.52 1.35 72.89 0.57 8.53 0 0 151 1.51665 13.14 3.45 1.76 72.48 0.6 8.38 0 0.17 152 1.52127 14.32 3.9 0.83 71.5 0 9.49 0 0 153 1.51779 13.64 3.65 0.65 73 0.06 8.93 0 0 154 1.5161 13.42 3.4 1.22 72.69 0.59 8.32 0 0 155 1.51694 12.86 3.58 1.31 72.61 0.61 8.79 0 0 156 1.51646 13.04 3.4 1.26 73.01 0.52 8.58 0 0 157 1.51655 13.41 3.39 1.28 72.64 0.52 8.65 0 0 158 1.52121 14.03 3.76 0.58 71.79 0.11 9.65 0 0 159 1.51776 13.53 3.41 1.52 72.04 0.58 8.79 0 0 160 1.51796 13.5 3.36 1.63 71.94 0.57 8.81 0 0.09 161 1.51832 13.33 3.34 1.54 72.14 0.56 8.99 0 0 162 1.51934 13.64 3.54 0.75 72.65 0.16 8.89 0.15 0.24 163 1.52211 14.19 3.78 0.91 71.36 0.23 9.14 0 0.37 164 1.51514 14.01 2.68 3.5 69.89 1.68 5.87 2.2 0 165 1.51915 12.73 1.85 1.86 72.69 0.6 10.09 0 0 166 1.52171 11.56 1.88 1.56 72.86 0.47 11.41 0 0 167 1.52151 11.03 1.71 1.56 73.44 0.58 11.62 0 0 168 1.51969 12.64 0 1.65 73.75 0.38 11.53 0 0 169 1.51666 12.86 0 1.83 73.88 0.97 10.17 0 0 170 1.51994 13.27 0 1.76 73.03 0.47 11.32 0 0 171 1.52369 13.44 0 1.58 72.22 0.32 12.24 0 0 172 1.51316 13.02 0 3.04 70.48 6.21 6.96 0 0 173 1.51321 13 0 3.02 70.7 6.21 6.93 0 0 174 1.52043 13.38 0 1.4 72.25 0.33 12.5 0 0 175 1.52058 12.85 1.61 2.17 72.18 0.76 9.7 0.24 0.51 176 1.52119 12.97 0.33 1.51 73.39 0.13 11.27 0 0.28 177 1.51905 14 2.39 1.56 72.37 0 9.57 0 0 178 1.51937 13.79 2.41 1.19 72.76 0 9.77 0 0 179 1.51829 14.46 2.24 1.62 72.38 0 9.26 0 0 180 1.51852 14.09 2.19 1.66 72.67 0 9.32 0 0 181 1.51299 14.4 1.74 1.54 74.55 0 7.59 0 0 182 1.51888 14.99 0.78 1.74 72.5 0 9.95 0 0 183 1.51916 14.15 0 2.09 72.74 0 10.88 0 0 184 1.51969 14.56 0 0.56 73.48 0 11.22 0 0 185 1.51115 17.38 0 0.34 75.41 0 6.65 0 0 186 1.51131 13.69 3.2 1.81 72.81 1.76 5.43 1.19 0 187 1.51838 14.32 3.26 2.22 71.25 1.46 5.79 1.63 0 188 1.52315 13.44 3.34 1.23 72.38 0.6 8.83 0 0 189 1.52247 14.86 2.2 2.06 70.26 0.76 9.76 0 0 190 1.52365 15.79 1.83 1.31 70.43 0.31 8.61 1.68 0 191 1.51613 13.88 1.78 1.79 73.1 0 8.67 0.76 0 192 1.51602 14.85 0 2.38 73.28 0 8.76 0.64 0.09 193 1.51623 14.2 0 2.79 73.46 0.04 9.04 0.4 0.09 194 1.51719 14.75 0 2 73.02 0 8.53 1.59 0.08 195 1.51683 14.56 0 1.98 73.29 0 8.52 1.57 0.07 196 1.51545 14.14 0 2.68 73.39 0.08 9.07 0.61 0.05 197 1.51556 13.87 0 2.54 73.23 0.14 9.41 0.81 0.01 198 1.51727 14.7 0 2.34 73.28 0 8.95 0.66 0 199 1.51531 14.38 0 2.66 73.1 0.04 9.08 0.64 0 200 1.51609 15.01 0 2.51 73.05 0.05 8.83 0.53 0 201 1.51508 15.15 0 2.25 73.5 0 8.34 0.63 0 202 1.51653 11.95 0 1.19 75.18 2.7 8.93 0 0 203 1.51514 14.85 0 2.42 73.72 0 8.39 0.56 0 204 1.51658 14.8 0 1.99 73.11 0 8.28 1.71 0 205 1.51617 14.95 0 2.27 73.3 0 8.71 0.67 0 206 1.51732 14.95 0 1.8 72.99 0 8.61 1.55 0 207 1.51645 14.94 0 1.87 73.11 0 8.67 1.38 0 208 1.51831 14.39 0 1.82 72.86 1.41 6.47 2.88 0 209 1.5164 14.37 0 2.74 72.85 0 9.45 0.54 0 210 1.51623 14.14 0 2.88 72.61 0.08 9.18 1.06 0 211 1.51685 14.92 0 1.99 73.06 0 8.4 1.59 0 212 1.52065 14.36 0 2.02 73.42 0 8.44 1.64 0 213 1.51651 14.38 0 1.94 73.61 0 8.48 1.57 0 214 1.51711 14.23 0 2.08 73.36 0 8.62 1.67 0 Setelah itu kita dapat membuat fungsi untuk menampilkan masing-masing cluster dalam bentuk tabel. def show_cluster ( data , k ): cluster = {} for i in range ( k ): cluster [ 'Cluster ' + str ( i )] = data [ data [ \"Cluster\" ] . isin ([ i ])] . iloc [:, 0 ] . values dframe = pd . DataFrame . from_dict ( cluster , orient = 'index' ) dframe = dframe . transpose () dframe = dframe . fillna ( \"\" ) return dframe . style . hide_index () kita dapat menggunakan k-Means, yaitu library dari sklearn untuk melakukan clustering pada data numerik. Contoh ini digunakan k=5 untuk mengelompokan cluster menjadi 5 bagian. Hasil proses clustering yang dilakukan dapat digabungkan dengan data yang telah ada dengan menambahkan atribut cluster agar setiap baris data memiliki clusternya masing-masing. from sklearn.cluster import KMeans k = 5 data_set = df . iloc [:, 1 :] . values df_dummy = pd . get_dummy ( df ) data_set = df_dummy . reset_index () . values kmeans = KMeans ( n_clusters = k ) cluster = kmeans . fit ( data_set ) data [ 'Cluster' ] = cluster . labels_ show_cluster ( data , k ) kita dapat menggunakan fungsi show_cluster() yang telah dibuat sebelumnya dari fungsi cluster dan id anggota. Cluster 0 Cluster 1 Cluster 2 Cluster 3 Cluster 4 2 106 169 1 164 3 107 181 18 172 4 108 182 19 173 5 109 185 22 186 6 110 191 37 187 7 111 192 39 8 112 193 40 9 113 194 44 10 131 195 46 11 132 196 48 12 166 197 49 13 167 198 50 14 168 199 51 15 170 200 62 16 171 201 63 17 174 202 64 20 176 203 65 21 183 204 66 23 184 205 67 24 206 68 25 207 69 26 208 70 27 209 104 28 210 105 29 211 128 30 212 129 31 213 130 32 214 152 33 158 34 163 35 165 36 175 38 177 41 178 42 179 43 180 45 189 47 190 52 53 54 55 56 57 58 59 60 61 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 114 115 116 117 118 119 120 121 122 123 124 125 126 127 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 153 154 155 156 157 159 160 161 162 188 hasilnya dapat kita visualisasikan dalam bentuk plot dengan matplotlib import matplotlib.pyplot as plt from sklearn.decomposition import PCA pca = PCA ( 2 ) plot_columns = pca . fit_transform ( df_dummy . iloc [:, 0 : 10 ]) plt . title ( \"Hasil Klustering K-Means\" ) plt . scatter ( x = plot_columns [:, 1 ], y = plot_columns [:, 0 ], c = data [ \"Cluster\" ], s = 30 ) plt . show () maka akan tampil seperti ini: K-Modes \u00b6 Pada implementasi K-Modes ini kita menggunakan data yang dapat didownload disini . setelah itu dapat divisualisasikan dengan cara K-Prototype \u00b6 Yang terakhir adalah K-Prototype, pada kasus ini kita berlatih dengan menggunakan contoh yang bisa diunduh di link ini . import pandas as pd data = pd . read_csv ( 'tae_data.csv' , delimiter = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () setelah nge load maka data akan tampil seperti berikut ID Whether TA Course instructor Course Summer Class size Class attribute 1 english speaker 23 3 summer 19 high 2 non es 15 3 summer 17 high 3 english speaker 23 3 regular 49 high 4 english speaker 5 2 regular 33 high 5 non es 7 11 regular 55 high 6 non es 23 3 summer 20 high 7 non es 9 5 regular 19 high 8 non es 10 3 regular 27 high 9 english speaker 22 3 summer 58 high 10 non es 15 3 summer 20 high 11 non es 10 22 regular 9 high 12 non es 13 1 regular 30 high Kita menggunakan K-Protype merupakan library dari K-Modes untuk melakukan clustering data campuran dari numerik dan kategorikal. Pada contoh ini kita menggunakan k=5 untuk mengelompokkan menjadi 5 cluster. from kmodes.kprototypes import KPrototypes k = 5 df_dummy = pd . get_dummies ( df ) data_set = df_dummy . reset_index () . values kproto = KPrototypes ( n_clusters = k , init = 'Cao' , verbose = 2 ) cluster = kproto . fit ( data_set , categorical = [ 0 , 1 , 2 , 3 , 5 ]) data [ 'Cluster' ] = cluster . labels_ show_cluster ( data , k ) gunakan fungsi show_cluster() yang telah dibuat untuk menampilkan cluster-cluster dan ID dari anggota cluster. Cluster 0 Cluster 1 Cluster 2 Cluster 3 Cluster 4 4 1 3 14 11 8 2 5 15 17 12 6 9 16 19 13 7 18 20 26 21 10 31 25 34 23 22 38 28 50 30 24 42 29 56 33 27 44 35 58 43 32 48 36 65 47 37 57 39 73 51 40 70 53 81 52 41 77 54 83 60 45 85 55 99 62 46 117 59 105 69 49 126 64 107 72 61 149 67 108 93 63 150 68 118 94 66 74 121 95 71 75 123 98 76 78 129 106 79 82 137 110 80 89 141 112 84 101 143 113 86 104 145 114 87 109 148 115 88 125 119 90 128 127 91 132 139 92 138 142 96 144 147 97 151 100 102 103 111 116 120 122 124 130 131 133 134 135 136 140 146 Cluster tersebut dapat kita visualisasikan dalam bentuk plot dengan library matpotlib. import matplotlib.pyplot as plt from sklearn.decomposition import PCA pca = PCA ( 2 ) plot_columns = pca . fit_transform ( df_dummy . iloc [:, 1 :]) plt . title ( \"Hasil Klustering K-Prototype\" ) plt . scatter ( x = plot_columns [:, 1 ], y = plot_columns [:, 0 ], c = df_dummy [ \"Cluster\" ], s = 30 ) plt . show () Referensi \u00b6 https://achfaisol.github.io/fuzzy/#implementasi-pada-python","title":"5- Implementasi Metode K-Means, K-Modes, dan K-Prototype dengan Python"},{"location":"5kmodes/#clustering","text":"Sebuah cara untuk mengelompokkan data sesuai dengan ukuran kemiripan data tersebut itulah yang disebut Clustering. Untuk mengelompokan data pada metode clustering tidaklah harus sama, namun kemiripan data yang diperoleh berdasar pada kedekatan suatu karakteristik sample. Pada dasarnya metode clustering mengoptimalkan pusat centroid untuk melakukan pengelompokan. Prinsip manfaat metode clustering dalam bidang keilmuan: Identify Object (Recognition) Metode clustering biasa digunakan pada bidang image processing, computer vision, robot vision, dan lainya. Decission Support System and Data Mining Hal ini metode cluster biasa digunakan dalam segmen pasar, manajemen marketing pemetaan wilayah, dan lain sebagainya. Salah satu algoritma clustering yang sering digunakan yaitu K-Means Clustering. Hal ini memungkinkan untuk mengelompokan data sesuai kesamaan yang ada diantara mereka dalam k-means cluster, ditambahkan ke input algoritma.","title":"Clustering"},{"location":"5kmodes/#k-means-clustering","text":"K-Means yaitu metode pengelompokan data dengan proses permodelan yang unsupervised (yang tidak terwarisi). K-Means ini merupakan salah satu metode clustering dengan sistem partisi, dan dalam proses pengelompokannya terdapat 2 jenis data yang dipakai. Hierarchical Non-Hierarchical Metode K-Means clustering adalah salah satu metode clustering untuk pengelompokan data yang kedua, yaitu non-hierarchical. Metode ini akan mengelompokan data ke dalam beberapa cluster, dari tiap cluster memiliki karakteristik yang mirip antara data satu dengan yang lainnya dan berbeda pada cluster yang lain. Pada dasarnya, K-Means clustering melakukan pengelompokan dengan algoritma seperti di bawah ini: Tentukan jumlah k (k=cluster) Alokasikan data awal ke cluster secara acak Hitung centroidnya (rata-rata) dari masing-masing cluster yang telah ditentukan Hitung jarak masing-masing data dengan centroid, dan alokasikan data pada cluster terdekat Ulangi langkah 3 dan 4 hingga nilai centroid tidak lagi berubah","title":"K-Means Clustering"},{"location":"5kmodes/#k-modes-clustering","text":"K-Modes yaitu turunan dari metode K-Means. Pada K-Means bekerja hanya pada data numerik tidak bekerja pada data yang nilainya kategorikal. K-Modes bekerja seperti K-Means, namun metode ini khusus untuk mengelompokan data yang bernilai kategorikal. K-Modes bekerja pakai metode basis frekuansi untuk mengupdate modus dalam proses pengelompokan dan mengkuantifikasi total antara dua objek. Semakin kecil jumlahnya maka semakin mirip kedua objek ini.","title":"K-Modes Clustering"},{"location":"5kmodes/#k-prototype-clustering","text":"K-Prototype adalah metode clustering gabungan dari K-Means dan K-Modes. Metode ini dipakai untuk mengelompokan data yang memiliki atribut numerik dan kategorikal. . .","title":"K-Prototype Clustering"},{"location":"5kmodes/#implementasi-dengan-python","text":"","title":"Implementasi dengan Python"},{"location":"5kmodes/#k-means","text":"Pada implementasi K-Means, kita bisa menggunakan data glass yang bisa diunduh di link ini , data tersebut bisa ditampilkan dalam bentuk data frame. Yang harus dilakukan pertama adalah memuat data menggunakan library pandas. import pandas as pd data = pd . read_csv ( 'glass.csv' , delimiter = ';' , decimal = ',' ) df = pd . DataFrame ( data ) df . style . hide_index () dari data diatas akan menampilkan hasil seperti di bawah ini: ID refractive index Sodium Magnesium Aluminum Silicon Potassium Calcium Barium Iron 1 1.52101 13.64 4.49 1.1 71.78 0.06 8.75 0 0.001 2 1.51761 13.89 3.6 1.36 72.73 0.48 7.83 0 0 3 1.51618 13.53 3.55 1.54 72.99 0.39 7.78 0 0 4 1.51766 13.21 3.69 1.29 72.61 0.57 8.22 0 0 5 1.51742 13.27 3.62 1.24 73.08 0.55 8.07 0 0 6 1.51596 12.79 3.61 1.62 72.97 0.64 8.07 0 0.26 7 1.51743 13.3 3.6 1.14 73.09 0.58 8.17 0 0 8 1.51756 13.15 3.61 1.05 73.24 0.57 8.24 0 0 9 1.51918 14.04 3.58 1.37 72.08 0.56 8.3 0 0 10 1.51755 13 3.6 1.36 72.99 0.57 8.4 0 0.11 11 1.51571 12.72 3.46 1.56 73.2 0.67 8.09 0 0.24 12 1.51763 12.8 3.66 1.27 73.01 0.6 8.56 0 0 13 1.51589 12.88 3.43 1.4 73.28 0.69 8.05 0 0.24 14 1.51748 12.86 3.56 1.27 73.21 0.54 8.38 0 0.17 15 1.51763 12.61 3.59 1.31 73.29 0.58 8.5 0 0 16 1.51761 12.81 3.54 1.23 73.24 0.58 8.39 0 0 17 1.51784 12.68 3.67 1.16 73.11 0.61 8.7 0 0 18 1.52196 14.36 3.85 0.89 71.36 0.15 9.15 0 0 19 1.51911 13.9 3.73 1.18 72.12 0.06 8.89 0 0 20 1.51735 13.02 3.54 1.69 72.73 0.54 8.44 0 0.07 21 1.5175 12.82 3.55 1.49 72.75 0.54 8.52 0 0.19 22 1.51966 14.77 3.75 0.29 72.02 0.03 9 0 0 23 1.51736 12.78 3.62 1.29 72.79 0.59 8.7 0 0 24 1.51751 12.81 3.57 1.35 73.02 0.62 8.59 0 0 25 1.5172 13.38 3.5 1.15 72.85 0.5 8.43 0 0 26 1.51764 12.98 3.54 1.21 73 0.65 8.53 0 0 27 1.51793 13.21 3.48 1.41 72.64 0.59 8.43 0 0 28 1.51721 12.87 3.48 1.33 73.04 0.56 8.43 0 0 29 1.51768 12.56 3.52 1.43 73.15 0.57 8.54 0 0 30 1.51784 13.08 3.49 1.28 72.86 0.6 8.49 0 0 31 1.51768 12.65 3.56 1.3 73.08 0.61 8.69 0 0.14 32 1.51747 12.84 3.5 1.14 73.27 0.56 8.55 0 0 33 1.51775 12.85 3.48 1.23 72.97 0.61 8.56 0.09 0.22 34 1.51753 12.57 3.47 1.38 73.39 0.6 8.55 0 0.06 35 1.51783 12.69 3.54 1.34 72.95 0.57 8.75 0 0 36 1.51567 13.29 3.45 1.21 72.74 0.56 8.57 0 0 37 1.51909 13.89 3.53 1.32 71.81 0.51 8.78 0.11 0 38 1.51797 12.74 3.48 1.35 72.96 0.64 8.68 0 0 39 1.52213 14.21 3.82 0.47 71.77 0.11 9.57 0 0 40 1.52213 14.21 3.82 0.47 71.77 0.11 9.57 0 0 41 1.51793 12.79 3.5 1.12 73.03 0.64 8.77 0 0 42 1.51755 12.71 3.42 1.2 73.2 0.59 8.64 0 0 43 1.51779 13.21 3.39 1.33 72.76 0.59 8.59 0 0 44 1.5221 13.73 3.84 0.72 71.76 0.17 9.74 0 0 45 1.51786 12.73 3.43 1.19 72.95 0.62 8.76 0 0.3 46 1.519 13.49 3.48 1.35 71.95 0.55 9 0 0 47 1.51869 13.19 3.37 1.18 72.72 0.57 8.83 0 0.16 48 1.52667 13.99 3.7 0.71 71.57 0.02 9.82 0 0.1 49 1.52223 13.21 3.77 0.79 71.99 0.13 10.02 0 0 50 1.51898 13.58 3.35 1.23 72.08 0.59 8.91 0 0 51 1.5232 13.72 3.72 0.51 71.75 0.09 10.06 0 0.16 52 1.51926 13.2 3.33 1.28 72.36 0.6 9.14 0 0.11 53 1.51808 13.43 2.87 1.19 72.84 0.55 9.03 0 0 54 1.51837 13.14 2.84 1.28 72.85 0.55 9.07 0 0 55 1.51778 13.21 2.81 1.29 72.98 0.51 9.02 0 0.09 56 1.51769 12.45 2.71 1.29 73.7 0.56 9.06 0 0.24 57 1.51215 12.99 3.47 1.12 72.98 0.62 8.35 0 0.31 58 1.51824 12.87 3.48 1.29 72.95 0.6 8.43 0 0 59 1.51754 13.48 3.74 1.17 72.99 0.59 8.03 0 0 60 1.51754 13.39 3.66 1.19 72.79 0.57 8.27 0 0.11 61 1.51905 13.6 3.62 1.11 72.64 0.14 8.76 0 0 62 1.51977 13.81 3.58 1.32 71.72 0.12 8.67 0.69 0 63 1.52172 13.51 3.86 0.88 71.79 0.23 9.54 0 0.11 64 1.52227 14.17 3.81 0.78 71.35 0 9.69 0 0 65 1.52172 13.48 3.74 0.9 72.01 0.18 9.61 0 0.07 66 1.52099 13.69 3.59 1.12 71.96 0.09 9.4 0 0 67 1.52152 13.05 3.65 0.87 72.22 0.19 9.85 0 0.17 68 1.52152 13.05 3.65 0.87 72.32 0.19 9.85 0 0.17 69 1.52152 13.12 3.58 0.9 72.2 0.23 9.82 0 0.16 70 1.523 13.31 3.58 0.82 71.99 0.12 10.17 0 0.03 71 1.51574 14.86 3.67 1.74 71.87 0.16 7.36 0 0.12 72 1.51848 13.64 3.87 1.27 71.96 0.54 8.32 0 0.32 73 1.51593 13.09 3.59 1.52 73.1 0.67 7.83 0 0 74 1.51631 13.34 3.57 1.57 72.87 0.61 7.89 0 0 75 1.51596 13.02 3.56 1.54 73.11 0.72 7.9 0 0 76 1.5159 13.02 3.58 1.51 73.12 0.69 7.96 0 0 77 1.51645 13.44 3.61 1.54 72.39 0.66 8.03 0 0 78 1.51627 13 3.58 1.54 72.83 0.61 8.04 0 0 79 1.51613 13.92 3.52 1.25 72.88 0.37 7.94 0 0.14 80 1.5159 12.82 3.52 1.9 72.86 0.69 7.97 0 0 81 1.51592 12.86 3.52 2.12 72.66 0.69 7.97 0 0 82 1.51593 13.25 3.45 1.43 73.17 0.61 7.86 0 0 83 1.51646 13.41 3.55 1.25 72.81 0.68 8.1 0 0 84 1.51594 13.09 3.52 1.55 72.87 0.68 8.05 0 0.09 85 1.51409 14.25 3.09 2.08 72.28 1.1 7.08 0 0 86 1.51625 13.36 3.58 1.49 72.72 0.45 8.21 0 0 87 1.51569 13.24 3.49 1.47 73.25 0.38 8.03 0 0 88 1.51645 13.4 3.49 1.52 72.65 0.67 8.08 0 0.1 89 1.51618 13.01 3.5 1.48 72.89 0.6 8.12 0 0 90 1.5164 12.55 3.48 1.87 73.23 0.63 8.08 0 0.09 91 1.51841 12.93 3.74 1.11 72.28 0.64 8.96 0 0.22 92 1.51605 12.9 3.44 1.45 73.06 0.44 8.27 0 0 93 1.51588 13.12 3.41 1.58 73.26 0.07 8.39 0 0.19 94 1.5159 13.24 3.34 1.47 73.1 0.39 8.22 0 0 95 1.51629 12.71 3.33 1.49 73.28 0.67 8.24 0 0 96 1.5186 13.36 3.43 1.43 72.26 0.51 8.6 0 0 97 1.51841 13.02 3.62 1.06 72.34 0.64 9.13 0 0.15 98 1.51743 12.2 3.25 1.16 73.55 0.62 8.9 0 0.24 99 1.51689 12.67 2.88 1.71 73.21 0.73 8.54 0 0 100 1.51811 12.96 2.96 1.43 72.92 0.6 8.79 0.14 0 101 1.51655 12.75 2.85 1.44 73.27 0.57 8.79 0.11 0.22 102 1.5173 12.35 2.72 1.63 72.87 0.7 9.23 0 0 103 1.5182 12.62 2.76 0.83 73.81 0.35 9.42 0 0.2 104 1.52725 13.8 3.15 0.66 70.57 0.08 11.64 0 0 105 1.5241 13.83 2.9 1.17 71.15 0.08 10.79 0 0 106 1.52475 11.45 0 1.88 72.19 0.81 13.24 0 0.34 107 1.53125 10.73 0 2.1 69.81 0.58 13.3 3.15 0.28 108 1.53393 12.3 0 1 70.16 0.12 16.19 0 0.24 109 1.52222 14.43 0 1 72.67 0.1 11.52 0 0.08 110 1.51818 13.72 0 0.56 74.45 0 10.99 0 0 111 1.52664 11.23 0 0.77 73.21 0 14.68 0 0 112 1.52739 11.02 0 0.75 73.08 0 14.96 0 0 113 1.52777 12.64 0 0.67 72.02 0.06 14.4 0 0 114 1.51892 13.46 3.83 1.26 72.55 0.57 8.21 0 0.14 115 1.51847 13.1 3.97 1.19 72.44 0.6 8.43 0 0 116 1.51846 13.41 3.89 1.33 72.38 0.51 8.28 0 0 117 1.51829 13.24 3.9 1.41 72.33 0.55 8.31 0 0.1 118 1.51708 13.72 3.68 1.81 72.06 0.64 7.88 0 0 119 1.51673 13.3 3.64 1.53 72.53 0.65 8.03 0 0.29 120 1.51652 13.56 3.57 1.47 72.45 0.64 7.96 0 0 121 1.51844 13.25 3.76 1.32 72.4 0.58 8.42 0 0 122 1.51663 12.93 3.54 1.62 72.96 0.64 8.03 0 0.21 123 1.51687 13.23 3.54 1.48 72.84 0.56 8.1 0 0 124 1.51707 13.48 3.48 1.71 72.52 0.62 7.99 0 0 125 1.52177 13.2 3.68 1.15 72.75 0.54 8.52 0 0 126 1.51872 12.93 3.66 1.56 72.51 0.58 8.55 0 0.12 127 1.51667 12.94 3.61 1.26 72.75 0.56 8.6 0 0 128 1.52081 13.78 2.28 1.43 71.99 0.49 9.85 0 0.17 129 1.52068 13.55 2.09 1.67 72.18 0.53 9.57 0.27 0.17 130 1.5202 13.98 1.35 1.63 71.76 0.39 10.56 0 0.18 131 1.52177 13.75 1.01 1.36 72.19 0.33 11.14 0 0 132 1.52614 13.7 0 1.36 71.24 0.19 13.44 0 0.1 133 1.51813 13.43 3.98 1.18 72.49 0.58 8.15 0 0 134 1.518 13.71 3.93 1.54 71.81 0.54 8.21 0 0.15 135 1.51811 13.33 3.85 1.25 72.78 0.52 8.12 0 0 136 1.51789 13.19 3.9 1.3 72.33 0.55 8.44 0 0.28 137 1.51806 13 3.8 1.08 73.07 0.56 8.38 0 0.12 138 1.51711 12.89 3.62 1.57 72.96 0.61 8.11 0 0 139 1.51674 12.79 3.52 1.54 73.36 0.66 7.9 0 0 140 1.51674 12.87 3.56 1.64 73.14 0.65 7.99 0 0 141 1.5169 13.33 3.54 1.61 72.54 0.68 8.11 0 0 142 1.51851 13.2 3.63 1.07 72.83 0.57 8.41 0.09 0.17 143 1.51662 12.85 3.51 1.44 73.01 0.68 8.23 0.06 0.25 144 1.51709 13 3.47 1.79 72.72 0.66 8.18 0 0 145 1.5166 12.99 3.18 1.23 72.97 0.58 8.81 0 0.24 146 1.51839 12.85 3.67 1.24 72.57 0.62 8.68 0 0.35 147 1.51769 13.65 3.66 1.11 72.77 0.11 8.6 0 0 148 1.5161 13.33 3.53 1.34 72.67 0.56 8.33 0 0 149 1.5167 13.24 3.57 1.38 72.7 0.56 8.44 0 0.1 150 1.51643 12.16 3.52 1.35 72.89 0.57 8.53 0 0 151 1.51665 13.14 3.45 1.76 72.48 0.6 8.38 0 0.17 152 1.52127 14.32 3.9 0.83 71.5 0 9.49 0 0 153 1.51779 13.64 3.65 0.65 73 0.06 8.93 0 0 154 1.5161 13.42 3.4 1.22 72.69 0.59 8.32 0 0 155 1.51694 12.86 3.58 1.31 72.61 0.61 8.79 0 0 156 1.51646 13.04 3.4 1.26 73.01 0.52 8.58 0 0 157 1.51655 13.41 3.39 1.28 72.64 0.52 8.65 0 0 158 1.52121 14.03 3.76 0.58 71.79 0.11 9.65 0 0 159 1.51776 13.53 3.41 1.52 72.04 0.58 8.79 0 0 160 1.51796 13.5 3.36 1.63 71.94 0.57 8.81 0 0.09 161 1.51832 13.33 3.34 1.54 72.14 0.56 8.99 0 0 162 1.51934 13.64 3.54 0.75 72.65 0.16 8.89 0.15 0.24 163 1.52211 14.19 3.78 0.91 71.36 0.23 9.14 0 0.37 164 1.51514 14.01 2.68 3.5 69.89 1.68 5.87 2.2 0 165 1.51915 12.73 1.85 1.86 72.69 0.6 10.09 0 0 166 1.52171 11.56 1.88 1.56 72.86 0.47 11.41 0 0 167 1.52151 11.03 1.71 1.56 73.44 0.58 11.62 0 0 168 1.51969 12.64 0 1.65 73.75 0.38 11.53 0 0 169 1.51666 12.86 0 1.83 73.88 0.97 10.17 0 0 170 1.51994 13.27 0 1.76 73.03 0.47 11.32 0 0 171 1.52369 13.44 0 1.58 72.22 0.32 12.24 0 0 172 1.51316 13.02 0 3.04 70.48 6.21 6.96 0 0 173 1.51321 13 0 3.02 70.7 6.21 6.93 0 0 174 1.52043 13.38 0 1.4 72.25 0.33 12.5 0 0 175 1.52058 12.85 1.61 2.17 72.18 0.76 9.7 0.24 0.51 176 1.52119 12.97 0.33 1.51 73.39 0.13 11.27 0 0.28 177 1.51905 14 2.39 1.56 72.37 0 9.57 0 0 178 1.51937 13.79 2.41 1.19 72.76 0 9.77 0 0 179 1.51829 14.46 2.24 1.62 72.38 0 9.26 0 0 180 1.51852 14.09 2.19 1.66 72.67 0 9.32 0 0 181 1.51299 14.4 1.74 1.54 74.55 0 7.59 0 0 182 1.51888 14.99 0.78 1.74 72.5 0 9.95 0 0 183 1.51916 14.15 0 2.09 72.74 0 10.88 0 0 184 1.51969 14.56 0 0.56 73.48 0 11.22 0 0 185 1.51115 17.38 0 0.34 75.41 0 6.65 0 0 186 1.51131 13.69 3.2 1.81 72.81 1.76 5.43 1.19 0 187 1.51838 14.32 3.26 2.22 71.25 1.46 5.79 1.63 0 188 1.52315 13.44 3.34 1.23 72.38 0.6 8.83 0 0 189 1.52247 14.86 2.2 2.06 70.26 0.76 9.76 0 0 190 1.52365 15.79 1.83 1.31 70.43 0.31 8.61 1.68 0 191 1.51613 13.88 1.78 1.79 73.1 0 8.67 0.76 0 192 1.51602 14.85 0 2.38 73.28 0 8.76 0.64 0.09 193 1.51623 14.2 0 2.79 73.46 0.04 9.04 0.4 0.09 194 1.51719 14.75 0 2 73.02 0 8.53 1.59 0.08 195 1.51683 14.56 0 1.98 73.29 0 8.52 1.57 0.07 196 1.51545 14.14 0 2.68 73.39 0.08 9.07 0.61 0.05 197 1.51556 13.87 0 2.54 73.23 0.14 9.41 0.81 0.01 198 1.51727 14.7 0 2.34 73.28 0 8.95 0.66 0 199 1.51531 14.38 0 2.66 73.1 0.04 9.08 0.64 0 200 1.51609 15.01 0 2.51 73.05 0.05 8.83 0.53 0 201 1.51508 15.15 0 2.25 73.5 0 8.34 0.63 0 202 1.51653 11.95 0 1.19 75.18 2.7 8.93 0 0 203 1.51514 14.85 0 2.42 73.72 0 8.39 0.56 0 204 1.51658 14.8 0 1.99 73.11 0 8.28 1.71 0 205 1.51617 14.95 0 2.27 73.3 0 8.71 0.67 0 206 1.51732 14.95 0 1.8 72.99 0 8.61 1.55 0 207 1.51645 14.94 0 1.87 73.11 0 8.67 1.38 0 208 1.51831 14.39 0 1.82 72.86 1.41 6.47 2.88 0 209 1.5164 14.37 0 2.74 72.85 0 9.45 0.54 0 210 1.51623 14.14 0 2.88 72.61 0.08 9.18 1.06 0 211 1.51685 14.92 0 1.99 73.06 0 8.4 1.59 0 212 1.52065 14.36 0 2.02 73.42 0 8.44 1.64 0 213 1.51651 14.38 0 1.94 73.61 0 8.48 1.57 0 214 1.51711 14.23 0 2.08 73.36 0 8.62 1.67 0 Setelah itu kita dapat membuat fungsi untuk menampilkan masing-masing cluster dalam bentuk tabel. def show_cluster ( data , k ): cluster = {} for i in range ( k ): cluster [ 'Cluster ' + str ( i )] = data [ data [ \"Cluster\" ] . isin ([ i ])] . iloc [:, 0 ] . values dframe = pd . DataFrame . from_dict ( cluster , orient = 'index' ) dframe = dframe . transpose () dframe = dframe . fillna ( \"\" ) return dframe . style . hide_index () kita dapat menggunakan k-Means, yaitu library dari sklearn untuk melakukan clustering pada data numerik. Contoh ini digunakan k=5 untuk mengelompokan cluster menjadi 5 bagian. Hasil proses clustering yang dilakukan dapat digabungkan dengan data yang telah ada dengan menambahkan atribut cluster agar setiap baris data memiliki clusternya masing-masing. from sklearn.cluster import KMeans k = 5 data_set = df . iloc [:, 1 :] . values df_dummy = pd . get_dummy ( df ) data_set = df_dummy . reset_index () . values kmeans = KMeans ( n_clusters = k ) cluster = kmeans . fit ( data_set ) data [ 'Cluster' ] = cluster . labels_ show_cluster ( data , k ) kita dapat menggunakan fungsi show_cluster() yang telah dibuat sebelumnya dari fungsi cluster dan id anggota. Cluster 0 Cluster 1 Cluster 2 Cluster 3 Cluster 4 2 106 169 1 164 3 107 181 18 172 4 108 182 19 173 5 109 185 22 186 6 110 191 37 187 7 111 192 39 8 112 193 40 9 113 194 44 10 131 195 46 11 132 196 48 12 166 197 49 13 167 198 50 14 168 199 51 15 170 200 62 16 171 201 63 17 174 202 64 20 176 203 65 21 183 204 66 23 184 205 67 24 206 68 25 207 69 26 208 70 27 209 104 28 210 105 29 211 128 30 212 129 31 213 130 32 214 152 33 158 34 163 35 165 36 175 38 177 41 178 42 179 43 180 45 189 47 190 52 53 54 55 56 57 58 59 60 61 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 114 115 116 117 118 119 120 121 122 123 124 125 126 127 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 153 154 155 156 157 159 160 161 162 188 hasilnya dapat kita visualisasikan dalam bentuk plot dengan matplotlib import matplotlib.pyplot as plt from sklearn.decomposition import PCA pca = PCA ( 2 ) plot_columns = pca . fit_transform ( df_dummy . iloc [:, 0 : 10 ]) plt . title ( \"Hasil Klustering K-Means\" ) plt . scatter ( x = plot_columns [:, 1 ], y = plot_columns [:, 0 ], c = data [ \"Cluster\" ], s = 30 ) plt . show () maka akan tampil seperti ini:","title":"K-Means"},{"location":"5kmodes/#k-modes","text":"Pada implementasi K-Modes ini kita menggunakan data yang dapat didownload disini . setelah itu dapat divisualisasikan dengan cara","title":"K-Modes"},{"location":"5kmodes/#k-prototype","text":"Yang terakhir adalah K-Prototype, pada kasus ini kita berlatih dengan menggunakan contoh yang bisa diunduh di link ini . import pandas as pd data = pd . read_csv ( 'tae_data.csv' , delimiter = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () setelah nge load maka data akan tampil seperti berikut ID Whether TA Course instructor Course Summer Class size Class attribute 1 english speaker 23 3 summer 19 high 2 non es 15 3 summer 17 high 3 english speaker 23 3 regular 49 high 4 english speaker 5 2 regular 33 high 5 non es 7 11 regular 55 high 6 non es 23 3 summer 20 high 7 non es 9 5 regular 19 high 8 non es 10 3 regular 27 high 9 english speaker 22 3 summer 58 high 10 non es 15 3 summer 20 high 11 non es 10 22 regular 9 high 12 non es 13 1 regular 30 high Kita menggunakan K-Protype merupakan library dari K-Modes untuk melakukan clustering data campuran dari numerik dan kategorikal. Pada contoh ini kita menggunakan k=5 untuk mengelompokkan menjadi 5 cluster. from kmodes.kprototypes import KPrototypes k = 5 df_dummy = pd . get_dummies ( df ) data_set = df_dummy . reset_index () . values kproto = KPrototypes ( n_clusters = k , init = 'Cao' , verbose = 2 ) cluster = kproto . fit ( data_set , categorical = [ 0 , 1 , 2 , 3 , 5 ]) data [ 'Cluster' ] = cluster . labels_ show_cluster ( data , k ) gunakan fungsi show_cluster() yang telah dibuat untuk menampilkan cluster-cluster dan ID dari anggota cluster. Cluster 0 Cluster 1 Cluster 2 Cluster 3 Cluster 4 4 1 3 14 11 8 2 5 15 17 12 6 9 16 19 13 7 18 20 26 21 10 31 25 34 23 22 38 28 50 30 24 42 29 56 33 27 44 35 58 43 32 48 36 65 47 37 57 39 73 51 40 70 53 81 52 41 77 54 83 60 45 85 55 99 62 46 117 59 105 69 49 126 64 107 72 61 149 67 108 93 63 150 68 118 94 66 74 121 95 71 75 123 98 76 78 129 106 79 82 137 110 80 89 141 112 84 101 143 113 86 104 145 114 87 109 148 115 88 125 119 90 128 127 91 132 139 92 138 142 96 144 147 97 151 100 102 103 111 116 120 122 124 130 131 133 134 135 136 140 146 Cluster tersebut dapat kita visualisasikan dalam bentuk plot dengan library matpotlib. import matplotlib.pyplot as plt from sklearn.decomposition import PCA pca = PCA ( 2 ) plot_columns = pca . fit_transform ( df_dummy . iloc [:, 1 :]) plt . title ( \"Hasil Klustering K-Prototype\" ) plt . scatter ( x = plot_columns [:, 1 ], y = plot_columns [:, 0 ], c = df_dummy [ \"Cluster\" ], s = 30 ) plt . show ()","title":"K-Prototype"},{"location":"5kmodes/#referensi","text":"https://achfaisol.github.io/fuzzy/#implementasi-pada-python","title":"Referensi"},{"location":"6fuzzy/","text":"Pengertian Fuzzy C-Means \u00b6 Fuzzy C-Means atau biasa disingkat FCM adalah metode pengelompokkan data yang dimana tiap element data dalam suatu cluster dituntukan oleh derajat keanggotaannya. Metode ini dikembangkan oleh Dunn pada tahun 1973 dan diteruskan oleh Bezdek tahun 1981. Metode ini biasanya sering digunakan dalam pola minimalisasi fungsinya. J_{m}=\\sum_{i=1}^{N} \\sum_{j=1}^{C} x_{i j}^{m}\\left\\|x_{i}-c_{j}\\right\\|^{2}, m \\leq 1<\\infty J_{m}=\\sum_{i=1}^{N} \\sum_{j=1}^{C} x_{i j}^{m}\\left\\|x_{i}-c_{j}\\right\\|^{2}, m \\leq 1<\\infty dimana: m = bilangan real > 1 Uij = tingkat keanggotaan xi dalam cluster j Xi = jumlah data terukur d-dimensi cj = pusat dimensi d dari cluster | ... | = norma apapun yang mengungkapkan kesamaan antara data yang diukur dan pusat Partisi fuzzy dilakukan melalui optimasi berulang fungsi objektif yang ditunjukan diatas, dengan pembaruan keanggotan uji dan pusat cluster cj. Iterasi ini akan berhenti ketika |...| < E, dan iterasi telah mencapat batas yang telah diperoleh antara 0 dan 1, sedangkan k adalah langkah iterasi. Prosedur ini menyatu ke minimum lokal atau titik pelana Jm. Algoritma Fuzzy C-Means \u00b6 Algoritma perhitungan dapat dijalankan seperti berikut: Tentukan jumlah cluster (k) Inisialisasi nilai random derajat keanggotaan dari masing-masing cluster nilai antara 0 -1 Cari pusat cluster (centroid) sebanyak cluster Ubah nilai derajat keanggotaan tiap element cluster. Saat jarak kurang dari ambang batas atau iterasi sampai pada batas yang telah ditentukan maka iterasi dapat dihentikan. Selain itu kita dapat mengulang dari langkah ke-2. Implementasi pada Python \u00b6 Pertama yang dilakukan adalah dengan masukan data berbentuk .csv menggunakan library pandas. import pandas as pd import random import math as mt data = pd . read_csv ( 'cmeans.csv' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () Data akan menampilkan tiga fitur dan dua fiturnya sebagai perhitungan, yaitu Rumah dan Mobil. Nama Rumah Mobil A 1 3 B 3 3 C 4 3 D 5 3 E 1 2 F 4 2 G 1 1 H 2 1 kemudian buat fungsi yang digunakan membuat nilai derajat keanggotaan secara random sebanyak data dan jumlah cluster. def buat_mu ( X , cluster ): mu = [] for i in range ( len ( X )): temp_mu = [] for j in range ( cluster ): inp = random . randrange ( 1 , 10 ); temp_mu . append ( inp / 10 ) mu . append ( temp_mu ) return mu def normalisasi_mu ( mu ): n_mu = [] for i in range ( len ( mu )): temp_mu = [] for j in range ( len ( mu [ i ])): nilai2 = 0 for k in range ( len ( mu [ i ])): if k == j : nilai = mu [ i ][ k ] else : nilai2 += mu [ i ][ k ] temp_mu . append ( round ( nilai / ( nilai + nilai2 ), 4 )) n_mu . append ( temp_mu ) return n_mu def mu_kuadrat ( n_mu , m ): mu_pow = [] for i in range ( len ( n_mu )): temp = [] for j in range ( len ( n_mu [ i ])): temp . append ( round ( n_mu [ i ][ j ] ** m , 4 )) mu_pow . append ( temp ) return mu_pow def mu_kuad_X ( jumlah_cluster , X , mu_kuad ): mu_kuad_X = [] for i in range ( jumlah_cluster ): mu_x = [] for j in range ( len ( X )): temp = [] for k in range ( len ( X [ j ])): temp . append ( round ( X [ j ][ k ] * mu_kuad [ j ][ i ], 4 )) mu_x . append ( temp ) mu_kuad_X . append ( mu_x ) return mu_kuad_X def jumlah ( N ): jumlah = [] for i in range ( len ( N )): temp = [] for j in range ( len ( N [ i ])): if i == 0 : ij = N [ i ][ j ] else : ij = jumlah [ i - 1 ][ j ] + N [ i ][ j ] temp . append ( ij ) jumlah . append ( temp ) return jumlah [ - 1 ] def getCentroid ( U , mu_kuad_X ): SUM_C = jumlah ( U ) centroid = [] for i in range ( jumlah_cluster ): temp = [] SUM_U = jumlah ( mu_kuad_X [ i ]) for j in range ( len ( jumlah ( mu_kuad_X [ i ]))): temp . append ( SUM_U [ j ] / SUM_C [ i ]) centroid . append ( temp ) return centroid def manhatan_dist ( x = [], y = []): dist = 0 for i in range ( len ( x )): dist += round ( abs ( x [ i ] - y [ i ]), 4 ) return dist def euclidian_dist ( x = [], y = []): dist = 0 for i in range ( len ( x )): dist += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dist ), 2 ) def update_U ( n_mu , jumlah_cluster , X , centroid ): U = [] for i in range ( len ( n_mu )): temp = [] for j in range ( len ( n_mu [ i ])): penyebut = 0 for k in range ( jumlah_cluster ): pembilang = euclidian_dist ( X [ i ], centroid [ j ]) penyebut += euclidian_dist ( X [ i ], centroid [ k ]) hasil = pembilang / penyebut hasil = round ( 1 / ( hasil ** ( 2 / ( m - 1 ))), 4 ) temp . append ( hasil ) U . append ( temp ) return U def Fuzzy_CMeans ( X , jumlah_cluster , m , max_iter , threshold ): mu = buat_mu ( X , jumlah_cluster ) N_mu = normalisasi_mu ( mu ) iterasi = 0 stop = True while iterasi < max_iter and stop : N = N_mu print ( 'Iterasi' , iterasi ) centroid = getCentroid ( N_mu , mu_kuad_X ( jumlah_cluster , X , mu_kuadrat ( N_mu , m ))) U = update_U ( N_mu , jumlah_cluster , X , centroid ) N_mu = normalisasi_mu ( U ) iterasi += 1 if manhatan_dist ( jumlah ( N_mu ), jumlah ( N )) < threshold : print ( 'berhenti' ) stop = False print ( pd . DataFrame ( N_mu )) dan terakhir kita dapat memanggil fungsi Fuzzy_CMeans() diatas dengan menentukan jumlah cluster adalah 3, data yang akan dicluster yang berisi dua fitur yang disimpan pada variable X, memberikan nilai m=2, nilai ambang batas adalah 0.1 dan maksimal iterasi adalah 100 kali. data2 = data . drop ( \"Nama\" , axis = 1 ) X = data2 . values m = 2 threshold = 0.1 max_iter = 100 jumlah_cluster = 3 Fuzzy_CMeans ( X , jumlah_cluster , m , max_iter , threshold ) maka akan mengembalikan hasil seperti berikut: Iterasi 0 0 1 2 0 0.3006 0.4336 0.2658 1 0.3419 0.4051 0.2530 2 0.3500 0.3839 0.2661 3 0.3510 0.3711 0.2780 4 0.2595 0.5347 0.2058 5 0.3633 0.3782 0.2585 6 0.3198 0.5441 0.1360 7 0.4481 0.3969 0.1550 Iterasi 1 0 1 2 0 0.3275 0.3870 0.2855 1 0.3390 0.3752 0.2858 2 0.3414 0.3642 0.2944 3 0.3403 0.3571 0.3025 4 0.3185 0.4347 0.2468 5 0.3441 0.3638 0.2921 6 0.0713 0.9094 0.0193 7 0.3667 0.4039 0.2294 Iterasi 2 0 1 2 0 0.3297 0.3686 0.3017 1 0.3342 0.3625 0.3032 2 0.3349 0.3554 0.3097 3 0.3362 0.3498 0.3140 4 0.3241 0.3994 0.2765 5 0.3373 0.3544 0.3083 6 0.1465 0.7976 0.0559 7 0.3444 0.3869 0.2687 Iterasi 3 0 1 2 0 0.3287 0.3569 0.3144 1 0.3314 0.3519 0.3167 2 0.3313 0.3475 0.3211 3 0.3327 0.3444 0.3229 4 0.3239 0.3770 0.2991 5 0.3330 0.3475 0.3194 6 0.2066 0.6590 0.1344 7 0.3314 0.3703 0.2983 Iterasi 4 0 1 2 0 0.3292 0.3474 0.3234 1 0.3310 0.3443 0.3247 2 0.3303 0.3427 0.3269 3 0.3323 0.3396 0.3281 4 0.3255 0.3592 0.3153 5 0.3313 0.3414 0.3273 6 0.2711 0.4940 0.2350 7 0.3268 0.3575 0.3157 Iterasi 5 0 1 2 0 0.3303 0.3423 0.3274 1 0.3311 0.3399 0.3290 2 0.3322 0.3374 0.3304 3 0.3324 0.3367 0.3310 4 0.3296 0.3459 0.3245 5 0.3320 0.3380 0.3300 6 0.3138 0.3936 0.2925 7 0.3313 0.3431 0.3256 Iterasi 6 berhenti 0 1 2 0 0.3314 0.3373 0.3314 1 0.3319 0.3362 0.3319 2 0.3351 0.3333 0.3316 3 0.3333 0.3348 0.3319 4 0.3298 0.3405 0.3298 5 0.3320 0.3320 0.3360 6 0.3234 0.3015 0.3751 7 0.3392 0.3276 0.3333 cluster data dapat ditentukan dengan nilai derajat keanggotaan tertinggi pada masing-masing cluster. Referensi \u00b6 https://achfaisol.github.io/fuzzy/#implementasi-pada-python","title":"6- Implementasi Fuzzy C-Means Clustering dengan Python"},{"location":"6fuzzy/#pengertian-fuzzy-c-means","text":"Fuzzy C-Means atau biasa disingkat FCM adalah metode pengelompokkan data yang dimana tiap element data dalam suatu cluster dituntukan oleh derajat keanggotaannya. Metode ini dikembangkan oleh Dunn pada tahun 1973 dan diteruskan oleh Bezdek tahun 1981. Metode ini biasanya sering digunakan dalam pola minimalisasi fungsinya. J_{m}=\\sum_{i=1}^{N} \\sum_{j=1}^{C} x_{i j}^{m}\\left\\|x_{i}-c_{j}\\right\\|^{2}, m \\leq 1<\\infty J_{m}=\\sum_{i=1}^{N} \\sum_{j=1}^{C} x_{i j}^{m}\\left\\|x_{i}-c_{j}\\right\\|^{2}, m \\leq 1<\\infty dimana: m = bilangan real > 1 Uij = tingkat keanggotaan xi dalam cluster j Xi = jumlah data terukur d-dimensi cj = pusat dimensi d dari cluster | ... | = norma apapun yang mengungkapkan kesamaan antara data yang diukur dan pusat Partisi fuzzy dilakukan melalui optimasi berulang fungsi objektif yang ditunjukan diatas, dengan pembaruan keanggotan uji dan pusat cluster cj. Iterasi ini akan berhenti ketika |...| < E, dan iterasi telah mencapat batas yang telah diperoleh antara 0 dan 1, sedangkan k adalah langkah iterasi. Prosedur ini menyatu ke minimum lokal atau titik pelana Jm.","title":"Pengertian Fuzzy C-Means"},{"location":"6fuzzy/#algoritma-fuzzy-c-means","text":"Algoritma perhitungan dapat dijalankan seperti berikut: Tentukan jumlah cluster (k) Inisialisasi nilai random derajat keanggotaan dari masing-masing cluster nilai antara 0 -1 Cari pusat cluster (centroid) sebanyak cluster Ubah nilai derajat keanggotaan tiap element cluster. Saat jarak kurang dari ambang batas atau iterasi sampai pada batas yang telah ditentukan maka iterasi dapat dihentikan. Selain itu kita dapat mengulang dari langkah ke-2.","title":"Algoritma Fuzzy C-Means"},{"location":"6fuzzy/#implementasi-pada-python","text":"Pertama yang dilakukan adalah dengan masukan data berbentuk .csv menggunakan library pandas. import pandas as pd import random import math as mt data = pd . read_csv ( 'cmeans.csv' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () Data akan menampilkan tiga fitur dan dua fiturnya sebagai perhitungan, yaitu Rumah dan Mobil. Nama Rumah Mobil A 1 3 B 3 3 C 4 3 D 5 3 E 1 2 F 4 2 G 1 1 H 2 1 kemudian buat fungsi yang digunakan membuat nilai derajat keanggotaan secara random sebanyak data dan jumlah cluster. def buat_mu ( X , cluster ): mu = [] for i in range ( len ( X )): temp_mu = [] for j in range ( cluster ): inp = random . randrange ( 1 , 10 ); temp_mu . append ( inp / 10 ) mu . append ( temp_mu ) return mu def normalisasi_mu ( mu ): n_mu = [] for i in range ( len ( mu )): temp_mu = [] for j in range ( len ( mu [ i ])): nilai2 = 0 for k in range ( len ( mu [ i ])): if k == j : nilai = mu [ i ][ k ] else : nilai2 += mu [ i ][ k ] temp_mu . append ( round ( nilai / ( nilai + nilai2 ), 4 )) n_mu . append ( temp_mu ) return n_mu def mu_kuadrat ( n_mu , m ): mu_pow = [] for i in range ( len ( n_mu )): temp = [] for j in range ( len ( n_mu [ i ])): temp . append ( round ( n_mu [ i ][ j ] ** m , 4 )) mu_pow . append ( temp ) return mu_pow def mu_kuad_X ( jumlah_cluster , X , mu_kuad ): mu_kuad_X = [] for i in range ( jumlah_cluster ): mu_x = [] for j in range ( len ( X )): temp = [] for k in range ( len ( X [ j ])): temp . append ( round ( X [ j ][ k ] * mu_kuad [ j ][ i ], 4 )) mu_x . append ( temp ) mu_kuad_X . append ( mu_x ) return mu_kuad_X def jumlah ( N ): jumlah = [] for i in range ( len ( N )): temp = [] for j in range ( len ( N [ i ])): if i == 0 : ij = N [ i ][ j ] else : ij = jumlah [ i - 1 ][ j ] + N [ i ][ j ] temp . append ( ij ) jumlah . append ( temp ) return jumlah [ - 1 ] def getCentroid ( U , mu_kuad_X ): SUM_C = jumlah ( U ) centroid = [] for i in range ( jumlah_cluster ): temp = [] SUM_U = jumlah ( mu_kuad_X [ i ]) for j in range ( len ( jumlah ( mu_kuad_X [ i ]))): temp . append ( SUM_U [ j ] / SUM_C [ i ]) centroid . append ( temp ) return centroid def manhatan_dist ( x = [], y = []): dist = 0 for i in range ( len ( x )): dist += round ( abs ( x [ i ] - y [ i ]), 4 ) return dist def euclidian_dist ( x = [], y = []): dist = 0 for i in range ( len ( x )): dist += ( x [ i ] - y [ i ]) ** 2 return round ( mt . sqrt ( dist ), 2 ) def update_U ( n_mu , jumlah_cluster , X , centroid ): U = [] for i in range ( len ( n_mu )): temp = [] for j in range ( len ( n_mu [ i ])): penyebut = 0 for k in range ( jumlah_cluster ): pembilang = euclidian_dist ( X [ i ], centroid [ j ]) penyebut += euclidian_dist ( X [ i ], centroid [ k ]) hasil = pembilang / penyebut hasil = round ( 1 / ( hasil ** ( 2 / ( m - 1 ))), 4 ) temp . append ( hasil ) U . append ( temp ) return U def Fuzzy_CMeans ( X , jumlah_cluster , m , max_iter , threshold ): mu = buat_mu ( X , jumlah_cluster ) N_mu = normalisasi_mu ( mu ) iterasi = 0 stop = True while iterasi < max_iter and stop : N = N_mu print ( 'Iterasi' , iterasi ) centroid = getCentroid ( N_mu , mu_kuad_X ( jumlah_cluster , X , mu_kuadrat ( N_mu , m ))) U = update_U ( N_mu , jumlah_cluster , X , centroid ) N_mu = normalisasi_mu ( U ) iterasi += 1 if manhatan_dist ( jumlah ( N_mu ), jumlah ( N )) < threshold : print ( 'berhenti' ) stop = False print ( pd . DataFrame ( N_mu )) dan terakhir kita dapat memanggil fungsi Fuzzy_CMeans() diatas dengan menentukan jumlah cluster adalah 3, data yang akan dicluster yang berisi dua fitur yang disimpan pada variable X, memberikan nilai m=2, nilai ambang batas adalah 0.1 dan maksimal iterasi adalah 100 kali. data2 = data . drop ( \"Nama\" , axis = 1 ) X = data2 . values m = 2 threshold = 0.1 max_iter = 100 jumlah_cluster = 3 Fuzzy_CMeans ( X , jumlah_cluster , m , max_iter , threshold ) maka akan mengembalikan hasil seperti berikut: Iterasi 0 0 1 2 0 0.3006 0.4336 0.2658 1 0.3419 0.4051 0.2530 2 0.3500 0.3839 0.2661 3 0.3510 0.3711 0.2780 4 0.2595 0.5347 0.2058 5 0.3633 0.3782 0.2585 6 0.3198 0.5441 0.1360 7 0.4481 0.3969 0.1550 Iterasi 1 0 1 2 0 0.3275 0.3870 0.2855 1 0.3390 0.3752 0.2858 2 0.3414 0.3642 0.2944 3 0.3403 0.3571 0.3025 4 0.3185 0.4347 0.2468 5 0.3441 0.3638 0.2921 6 0.0713 0.9094 0.0193 7 0.3667 0.4039 0.2294 Iterasi 2 0 1 2 0 0.3297 0.3686 0.3017 1 0.3342 0.3625 0.3032 2 0.3349 0.3554 0.3097 3 0.3362 0.3498 0.3140 4 0.3241 0.3994 0.2765 5 0.3373 0.3544 0.3083 6 0.1465 0.7976 0.0559 7 0.3444 0.3869 0.2687 Iterasi 3 0 1 2 0 0.3287 0.3569 0.3144 1 0.3314 0.3519 0.3167 2 0.3313 0.3475 0.3211 3 0.3327 0.3444 0.3229 4 0.3239 0.3770 0.2991 5 0.3330 0.3475 0.3194 6 0.2066 0.6590 0.1344 7 0.3314 0.3703 0.2983 Iterasi 4 0 1 2 0 0.3292 0.3474 0.3234 1 0.3310 0.3443 0.3247 2 0.3303 0.3427 0.3269 3 0.3323 0.3396 0.3281 4 0.3255 0.3592 0.3153 5 0.3313 0.3414 0.3273 6 0.2711 0.4940 0.2350 7 0.3268 0.3575 0.3157 Iterasi 5 0 1 2 0 0.3303 0.3423 0.3274 1 0.3311 0.3399 0.3290 2 0.3322 0.3374 0.3304 3 0.3324 0.3367 0.3310 4 0.3296 0.3459 0.3245 5 0.3320 0.3380 0.3300 6 0.3138 0.3936 0.2925 7 0.3313 0.3431 0.3256 Iterasi 6 berhenti 0 1 2 0 0.3314 0.3373 0.3314 1 0.3319 0.3362 0.3319 2 0.3351 0.3333 0.3316 3 0.3333 0.3348 0.3319 4 0.3298 0.3405 0.3298 5 0.3320 0.3320 0.3360 6 0.3234 0.3015 0.3751 7 0.3392 0.3276 0.3333 cluster data dapat ditentukan dengan nilai derajat keanggotaan tertinggi pada masing-masing cluster.","title":"Implementasi pada Python"},{"location":"6fuzzy/#referensi","text":"https://achfaisol.github.io/fuzzy/#implementasi-pada-python","title":"Referensi"},{"location":"7regresi/","text":"Pengertian Regresi Linear \u00b6 Regresi linear berganda adalah salahsatu cara untuk memprediksi dengan memakai beberapa variabel bebas untuk mempredict hasil dari variabel. Tujuan dari regresi linera ini adalah untuk memodelkan relasi linear antara variabel bebas dan variabel terikat. Regresi linear ganda ini bisa dinyatakan dalam persamaan berikut: y=x_{1} b 1+x_{2} b_{2}+\\ldots+x_{n} b_{n}+a y=x_{1} b 1+x_{2} b_{2}+\\ldots+x_{n} b_{n}+a dimana: y = variabel terikat x = variabel bebas b= koefisien estimasi a = konstanta Perhitungan Regresi Linear \u00b6 Dalam kasus ini akan dilakukan perhitungan regresi linear dengan dua fitur variabel data seperti berikut: X1 X2 Y 2 3 10 4 2 12 5 3 16 7 1 16 maka persamaan ini dapat kita gunakan y=x1b1 + x2b2 +a untuk memprediksi inputan data terbaru. Sebelum menggunakan persamaan tersebut kita juga perlu mencari koefisien estimasi dari b1, b2, dan a. atau bisa juga menggunakan rumus di bawah ini: \\begin{aligned} b_{1} &=\\frac{\\left[\\left(\\sum x_{2}^{2} \\sum x_{1} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ b_{2} &=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ a &=\\frac{\\left(\\sum y\\right)-\\left(b_{1}-\\sum x_{1}\\right)-\\left(b_{2}-\\sum x_{2}\\right)}{n} \\end{aligned} \\begin{aligned} b_{1} &=\\frac{\\left[\\left(\\sum x_{2}^{2} \\sum x_{1} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ b_{2} &=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ a &=\\frac{\\left(\\sum y\\right)-\\left(b_{1}-\\sum x_{1}\\right)-\\left(b_{2}-\\sum x_{2}\\right)}{n} \\end{aligned} Sehingga kita bisa menghitung formula diatas untuk mendapatkan nilai koefisien estimator dan konstannya. Proses perhitungan seperti berikut dan baris terakhir adalah sigma dari seluruh baris pada kolom tersebut. No X1 X2 Y X1 ^ 2 X2 ^ 2 Y ^ 2 X1 * X2 X1 * Y X2 * Y 1 2 3 10 4 9 100 6 20 30 2 4 2 12 16 4 144 8 48 24 3 5 3 16 25 9 256 15 80 48 4 7 1 16 49 1 256 7 112 16 Total (\u2211\u2211) 18 9 54 94 23 756 36 260 118 dari hasil jumlah diatas kita harus normalisasi kembali. \\begin{array}{l}{\\sum x_{1}^{2}=\\sum x_{1}^{2}-\\frac{\\left(\\sum x_{1}\\right)^{2}}{n}=94-\\frac{324}{4}=13} \\\\ {\\sum x_{2}^{2}=\\sum x_{2}^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum y^{2}=\\sum y^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum x_{1} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma y\\right)}{n}=260-\\frac{972}{4}=17} \\\\ {\\sum x_{2} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma x_{1} x_{2}\\right)}{n}=260-\\frac{486}{[(12 * 75)-(-20.25)]}=2} \\\\ {b_{1}=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(2,75 * 75)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{\\left.\\left(5 x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(12 x-3,5)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{(54)-(2-18)-\\left(b_{2}-9\\right)}{4}=0}\\end{array} \\begin{array}{l}{\\sum x_{1}^{2}=\\sum x_{1}^{2}-\\frac{\\left(\\sum x_{1}\\right)^{2}}{n}=94-\\frac{324}{4}=13} \\\\ {\\sum x_{2}^{2}=\\sum x_{2}^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum y^{2}=\\sum y^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum x_{1} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma y\\right)}{n}=260-\\frac{972}{4}=17} \\\\ {\\sum x_{2} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma x_{1} x_{2}\\right)}{n}=260-\\frac{486}{[(12 * 75)-(-20.25)]}=2} \\\\ {b_{1}=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(2,75 * 75)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{\\left.\\left(5 x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(12 x-3,5)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{(54)-(2-18)-\\left(b_{2}-9\\right)}{4}=0}\\end{array} Sehingga dapatlah nilai koefisien estimator b1=2 , b2=2 dan a =0. Dari proses diatas maka didapatkan persamaan sebagai berikut y = x1 * 2 + x2 * 2 + 0 misal terdapat data baru dengan nilai x1 = 6 x2 = 2 maka bisa diperoleh sebagai berikut: y = 6 * 2 + 2 * 2 + 0 y = 16 Implementasi dengan Sklearn Python \u00b6 Pertama yang dilakukan adalah dengan masukan data berbentuk .csv menggunakan library pandas. import pandas as pd from sklearn.linear_model import LinearRegression data = pd . read_csv ( 'data.csv' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () Maka akan tampil seperti ini, dengan empat variabel bebas X1 X2 X3 X4 Y 2 3 6 2 169 5 2 4 2 169 3 4 2 3 144 5 7 3 3 324 5 6 7 8 676 Kemudian ambil data variabel bebas yang ditampung pada variabel x dan variabel y. Gunakan metode fit() untuk memprediksi regresi linear yang sudah tersedia di sklearn. X = df . iloc [ 0 :, 0 : 4 ] . values y = df . iloc [ 0 :, 4 ] . values reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) ketikan code untuk mengetahui nilai konstanta dengan atribut intercept_ a = reg . intercept_ a maka tampilah nilainya -276.51245551601414 untuk mendapatkan nilai coef dari seluruh fitur dengan atribut coef_ reg . coef_ maka akan tampil seperti berikut array ([ 31.5480427 , 27.83274021 , 33.40569395 , 49.24199288 ]) gunakan persamaan diatas untuk mengetahui prediksi nilai dari y b1 = reg . coef_ [ 0 ] b2 = reg . coef_ [ 1 ] b3 = reg . coef_ [ 2 ] b4 = reg . coef_ [ 3 ] x1 = 4 x2 = 5 x3 = 2 x4 = 1 y = b1 * x1 + b2 * x2 + b3 * x3 + b4 * x4 + a y inilah hasilnya 104.89679715302498 fungsi bawaan dari class LinearRegression() juga bisa kita gunakan untuk memprediksi data baru yang diberikan. reg . predict ( np . array ([[ 4 , 5 , 2 , 1 ]])) Sekian. Terima kasih :) Referensi \u00b6 https://achfaisol.github.io/fuzzy/#implementasi-pada-python","title":"7- Penerapan Regresi Linear"},{"location":"7regresi/#pengertian-regresi-linear","text":"Regresi linear berganda adalah salahsatu cara untuk memprediksi dengan memakai beberapa variabel bebas untuk mempredict hasil dari variabel. Tujuan dari regresi linera ini adalah untuk memodelkan relasi linear antara variabel bebas dan variabel terikat. Regresi linear ganda ini bisa dinyatakan dalam persamaan berikut: y=x_{1} b 1+x_{2} b_{2}+\\ldots+x_{n} b_{n}+a y=x_{1} b 1+x_{2} b_{2}+\\ldots+x_{n} b_{n}+a dimana: y = variabel terikat x = variabel bebas b= koefisien estimasi a = konstanta","title":"Pengertian Regresi Linear"},{"location":"7regresi/#perhitungan-regresi-linear","text":"Dalam kasus ini akan dilakukan perhitungan regresi linear dengan dua fitur variabel data seperti berikut: X1 X2 Y 2 3 10 4 2 12 5 3 16 7 1 16 maka persamaan ini dapat kita gunakan y=x1b1 + x2b2 +a untuk memprediksi inputan data terbaru. Sebelum menggunakan persamaan tersebut kita juga perlu mencari koefisien estimasi dari b1, b2, dan a. atau bisa juga menggunakan rumus di bawah ini: \\begin{aligned} b_{1} &=\\frac{\\left[\\left(\\sum x_{2}^{2} \\sum x_{1} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ b_{2} &=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ a &=\\frac{\\left(\\sum y\\right)-\\left(b_{1}-\\sum x_{1}\\right)-\\left(b_{2}-\\sum x_{2}\\right)}{n} \\end{aligned} \\begin{aligned} b_{1} &=\\frac{\\left[\\left(\\sum x_{2}^{2} \\sum x_{1} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ b_{2} &=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2}^{2}\\right)-\\left(\\sum x_{1} x_{2}\\right)^{2}\\right]} \\\\ a &=\\frac{\\left(\\sum y\\right)-\\left(b_{1}-\\sum x_{1}\\right)-\\left(b_{2}-\\sum x_{2}\\right)}{n} \\end{aligned} Sehingga kita bisa menghitung formula diatas untuk mendapatkan nilai koefisien estimator dan konstannya. Proses perhitungan seperti berikut dan baris terakhir adalah sigma dari seluruh baris pada kolom tersebut. No X1 X2 Y X1 ^ 2 X2 ^ 2 Y ^ 2 X1 * X2 X1 * Y X2 * Y 1 2 3 10 4 9 100 6 20 30 2 4 2 12 16 4 144 8 48 24 3 5 3 16 25 9 256 15 80 48 4 7 1 16 49 1 256 7 112 16 Total (\u2211\u2211) 18 9 54 94 23 756 36 260 118 dari hasil jumlah diatas kita harus normalisasi kembali. \\begin{array}{l}{\\sum x_{1}^{2}=\\sum x_{1}^{2}-\\frac{\\left(\\sum x_{1}\\right)^{2}}{n}=94-\\frac{324}{4}=13} \\\\ {\\sum x_{2}^{2}=\\sum x_{2}^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum y^{2}=\\sum y^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum x_{1} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma y\\right)}{n}=260-\\frac{972}{4}=17} \\\\ {\\sum x_{2} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma x_{1} x_{2}\\right)}{n}=260-\\frac{486}{[(12 * 75)-(-20.25)]}=2} \\\\ {b_{1}=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(2,75 * 75)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{\\left.\\left(5 x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(12 x-3,5)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{(54)-(2-18)-\\left(b_{2}-9\\right)}{4}=0}\\end{array} \\begin{array}{l}{\\sum x_{1}^{2}=\\sum x_{1}^{2}-\\frac{\\left(\\sum x_{1}\\right)^{2}}{n}=94-\\frac{324}{4}=13} \\\\ {\\sum x_{2}^{2}=\\sum x_{2}^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum y^{2}=\\sum y^{2}-\\frac{\\left(\\sum x_{2}\\right)^{2}}{n}=23-\\frac{81}{4}=2,75} \\\\ {\\sum x_{1} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma y\\right)}{n}=260-\\frac{972}{4}=17} \\\\ {\\sum x_{2} y=\\sum x_{1} y-\\frac{\\left(\\sum x_{1} \\Sigma x_{1} x_{2}\\right)}{n}=260-\\frac{486}{[(12 * 75)-(-20.25)]}=2} \\\\ {b_{1}=\\frac{\\left[\\left(\\sum x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(2,75 * 75)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{\\left.\\left(5 x_{1}^{2} \\sum x_{2} y\\right)-\\left(\\sum x_{2} y \\sum x_{1} x_{2}\\right)\\right]}{n}=\\frac{[(12 x-3,5)-(-3,5 x-4,5)]}{[(12 * 2,75)-(-20.25)]}=2} \\\\ {a=\\frac{(54)-(2-18)-\\left(b_{2}-9\\right)}{4}=0}\\end{array} Sehingga dapatlah nilai koefisien estimator b1=2 , b2=2 dan a =0. Dari proses diatas maka didapatkan persamaan sebagai berikut y = x1 * 2 + x2 * 2 + 0 misal terdapat data baru dengan nilai x1 = 6 x2 = 2 maka bisa diperoleh sebagai berikut: y = 6 * 2 + 2 * 2 + 0 y = 16","title":"Perhitungan Regresi Linear"},{"location":"7regresi/#implementasi-dengan-sklearn-python","text":"Pertama yang dilakukan adalah dengan masukan data berbentuk .csv menggunakan library pandas. import pandas as pd from sklearn.linear_model import LinearRegression data = pd . read_csv ( 'data.csv' , sep = ';' ) df = pd . DataFrame ( data ) df . style . hide_index () Maka akan tampil seperti ini, dengan empat variabel bebas X1 X2 X3 X4 Y 2 3 6 2 169 5 2 4 2 169 3 4 2 3 144 5 7 3 3 324 5 6 7 8 676 Kemudian ambil data variabel bebas yang ditampung pada variabel x dan variabel y. Gunakan metode fit() untuk memprediksi regresi linear yang sudah tersedia di sklearn. X = df . iloc [ 0 :, 0 : 4 ] . values y = df . iloc [ 0 :, 4 ] . values reg = LinearRegression () . fit ( X , y ) reg . score ( X , y ) ketikan code untuk mengetahui nilai konstanta dengan atribut intercept_ a = reg . intercept_ a maka tampilah nilainya -276.51245551601414 untuk mendapatkan nilai coef dari seluruh fitur dengan atribut coef_ reg . coef_ maka akan tampil seperti berikut array ([ 31.5480427 , 27.83274021 , 33.40569395 , 49.24199288 ]) gunakan persamaan diatas untuk mengetahui prediksi nilai dari y b1 = reg . coef_ [ 0 ] b2 = reg . coef_ [ 1 ] b3 = reg . coef_ [ 2 ] b4 = reg . coef_ [ 3 ] x1 = 4 x2 = 5 x3 = 2 x4 = 1 y = b1 * x1 + b2 * x2 + b3 * x3 + b4 * x4 + a y inilah hasilnya 104.89679715302498 fungsi bawaan dari class LinearRegression() juga bisa kita gunakan untuk memprediksi data baru yang diberikan. reg . predict ( np . array ([[ 4 , 5 , 2 , 1 ]])) Sekian. Terima kasih :)","title":"Implementasi dengan Sklearn Python"},{"location":"7regresi/#referensi","text":"https://achfaisol.github.io/fuzzy/#implementasi-pada-python","title":"Referensi"},{"location":"penambangandata/","text":"Welcome \u00b6 Hello World I'm Mochamad Salim Ubaidillah \u00b6 NIM 170411100097 bismillah saya ingin menyampaikan beberapa materi yang saya terima selama menempuh mata kuliah Penambangan Data Selamat Belajar Semoga Bermanfaat","title":"Hello"},{"location":"penambangandata/#welcome","text":"Hello World","title":"Welcome"},{"location":"penambangandata/#im-mochamad-salim-ubaidillah","text":"NIM 170411100097 bismillah saya ingin menyampaikan beberapa materi yang saya terima selama menempuh mata kuliah Penambangan Data Selamat Belajar Semoga Bermanfaat","title":"I'm Mochamad Salim Ubaidillah"}]}